---
title: "Translation Elicitation: Report"
date: "Last updated: `r format(Sys.Date(), '%d/%m/%Y')`"
output:
  html_document:
    theme: paper
    highlight: pygments
    toc: true
    toc_depth: 2
    toc_float:
        collapsed: false
    number_sections: true
    keep_md: true
bibliography: "references.bib"
csl: "apa7.csl"
---


```{r setup, include=FALSE}

knitr::opts_chunk$set(
    message = FALSE,
    warning = FALSE,
    cache.extra = knitr::rand_seed,
    dev.args = list(png = list(type = "cairo")),
    dpi = 300,
    include = TRUE,
    out.width = "80%",
    results = "asis",
    echo = TRUE
)

options(
    knitr.kable.NA = '-',
    htmltools.preserve.raw = FALSE,
    knitr.duplicate.label = "allow",
    ggplot2.discrete.fill = c("#1A85FF", "#ff2976", "#FFC20A"),
    ggplot2.discrete.colour = c("#1A85FF", "#ff2976", "#FFC20A"),
    ggplot2.continuous.fill = ggplot2::scale_fill_gradient2(
        low = "#04bf78", 
        mid = "white", 
        high = "#FFC20A", 
        na.value = "white", 
        limits = c(-0.4, 0.4)
    ),
    ggplot2.continuous.colour = ggplot2::scale_fill_gradient2(
        low = "#04bf78", 
        mid = "white",
        high = "#FFC20A", 
        na.value = "white",
        limits = c(-0.4, 0.4)
    )
)


```


```{r prepare, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# import data
tar_load_all()

theme_set(
    theme(
        title = element_text(size = 8, face = "bold"),
        
        legend.background = element_rect(fill = NA, colour = NA),
        legend.key = element_rect(fill = "white", colour = NA),
        legend.justification = "right",
        legend.direction = "horizontal",
        
        axis.line = element_line(colour = "black", size = 0.5),
        axis.ticks = element_line(colour = "black", size = 0.5),
        axis.text.x = element_text(colour = "black"),
        axis.text.y = element_text(colour = "black"),
        
        plot.caption.position = "plot",
        plot.title = element_text(hjust = 0.5),
        
        
        panel.grid = element_blank(),
        panel.background = element_rect(fill = "white", colour = NA)
    )
)



```


# Stimuli 


## Summary

Participants in the spa-ENG listened to `r nrow(stimuli[stimuli$group=="spa-ENG",])` Spanish words, and participants in the cat-ENG and the cat-SPA groups listened to `r nrow(stimuli[stimuli$group=="cat-ENG",])` trials. Five trials in each condition were practice trials and were excluded from analyses.


```{r stimuli-table, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

stimuli %>% 
    select(group, word_1, freq_2, freq_zipf_2, nd, global_similarity, lv) %>% 
    group_by(group) %>% 
    summarise_at(
        vars(freq_2:lv), 
        list(
            median = ~median(., na.rm = TRUE),  
            sd = ~sd(., na.rm = TRUE)
        )
    ) %>%
    gt(rowname_col = "group") %>% 
    tab_header(
        title = md("Table 1"),
        subtitle = md("Lexical frequency, phonological neigbourhood size, and levenshtein distance of the stimuli words.")
    ) %>% 
    tab_source_note(
        md("*Note*. Median and standard deviation (*SD*) of lexical frequencies of target words (expressed in counts per million and as Zipf scores), density of higher-frequency phonological neighbourhoods of target words, and normalised Levenshtein phonological similarity between the presented and the target words.")
    ) %>% 
    fmt_number(freq_2_median:nd_sd) %>%
    fmt_percent(matches("lv|global")) %>% 
    tab_spanner(
        md("**Freq. (million)**"),
        c("freq_2_median", "freq_2_sd", "freq_zipf_2_median", "freq_zipf_2_sd")
    ) %>% 
    tab_spanner(
        md("**Freq. (Zipf)**"), 
        matches("freq_zipf_2")
    ) %>% 
    tab_spanner(
        md("***PTHN* (presented)**"),
        matches("nd")
    ) %>% 
    tab_spanner(
        md("**Glob. similarity**"),
        matches("global")
    ) %>% 
    tab_spanner(
        md("**Levenshtein**"),
        matches("lv")
    ) %>% 
    cols_label(
        freq_2_median = md("Median"),
        freq_2_sd = md("*SD*"),
        freq_zipf_2_median = md("Median"),
        freq_zipf_2_sd = md("*SD*"),
        nd_median = md("Median"),
        nd_sd = md("*SD*"),
        global_similarity_median = md("*Mean*"),
        global_similarity_sd = md("*SD*"),
        lv_median = md("Median"),
        lv_sd = md("*SD*")
    ) %>% 
    summary_rows(
        columns = c(matches("median"), -matches("lv")),
        fns = list(
            Median = "median",
            SD = "sd"
        )
    ) %>% 
    summary_rows(
        columns = matches("lv"),
        fns = list(
            Median = "median",
            SD = "sd"
        ), 
        formatter = fmt_percent
    ) %>% 
    tab_style(
        style = cell_text(align = "left", weight = "bold", size = "medium"),
        locations = cells_title(groups = "title")
    ) %>% 
    tab_style(
        style = cell_text(align = "left", size = "medium", style = "italic"),
        locations = cells_title(groups = "subtitle")
    ) %>% 
    tab_style(
        style = cell_text(align = "left", size = "medium"),
        locations = cells_source_notes()
    ) %>% 
    tab_style(
        style = cell_text(align = "left", size = "medium"),
        locations = cells_stub_grand_summary()
    ) %>% 
    tab_style(
        style = cell_borders(sides = c("left", "right"), color = "white"),
        locations = list(
            cells_body(),
            cells_row_groups(),
            cells_column_spanners(), 
            cells_stub(),
            cells_stub_grand_summary(), 
            cells_source_notes(),
            cells_column_labels(),
            cells_stubhead()
        )
    ) %>% 
    tab_style(
        style = list(
            cell_text(align = "left", size = "medium"),
            cell_borders(sides = "all", color = "white")
        ),
        locations = cells_stub()
    ) %>% 
    tab_style(
        style = cell_text(
            align = "center", 
            size = "medium",
            weight = "normal",
            style = "normal"
        ),
        locations = list(
            cells_column_spanners(),
            cells_column_labels()
        )
    ) %>% 
    tab_style(
        style = cell_borders(sides = "all", color = "white"),
        locations = cells_body(columns = 1:11)
    ) %>% 
    tab_style(
        style = cell_borders(sides = "top", color = "white"),
        locations = cells_body(columns = 1:11)
    ) %>% 
    as_raw_html()

```

## Lexical frequency

```{r stimuli-frequency, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
stimuli %>% 
    distinct(
        group,
        word_1,
        .keep_all = TRUE
    ) %>% 
    left_join(count(., group, name = "n_total")) %>% 
    mutate(test_language = paste0(group,  " (N = ", n_total, ")")) %>% 
    ggplot(
        aes(
            freq_zipf_2, 
            fill = group
        )
    ) +
    facet_wrap(
        ~group, 
        ncol = 1
    ) +
    geom_histogram(
        bins = 25,
        colour = "white"
    ) +
    labs(
        x = "Lexical frequency (Zipf score)\nExtracted from CLEARPOND database", 
        y = "# trials", 
        fill = "Group"
    )  +
    scale_y_continuous(breaks = seq(0, 12, 2)) +
    theme(
        legend.position = "none",
        panel.grid.major.y = element_line(colour = "grey", linetype = "dotted")
    )

```


## PTHN

```{r stimuli-pthn, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
stimuli %>% 
    left_join(count(., group, name = "n_total")) %>% 
    mutate(translation = paste0(word_1, "/", word_2)) %>% 
    ggplot() +
    facet_wrap(~group, scales = "free_y") +
    aes(
        x = nd, 
        y = reorder(translation, nd),
        colour = group,
        fill = group,
        label = translation
    ) + 
    geom_text(
        size = 3,
        hjust = 0
    ) +
    labs(
        x = "Group", 
        y = "# phonological neighbours", 
    ) +
    scale_x_continuous(breaks = seq(0, 30, 5)) +
    theme(
        legend.position = "none",
        panel.grid.major.x = element_line(colour = "grey", linetype = "dotted"),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank()
    )

```


## Levenshtein distance

```{r stimuli-similarity, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
stimuli %>% 
    left_join(count(., group, name = "n_total")) %>% 
    mutate(group = paste0(group,  " (N = ", n_total, ")")) %>% 
    ggplot() +
    aes(
        x = lv,
        y = reorder_within(translation, lv, group),
        fill = group,
        colour = group,
        label = percent(lv, accuracy = 0.1)
    ) +
    facet_wrap(~group, scales = "free_y") +
    geom_text(
        size = 3,
        hjust = 0
    ) +
    labs(
        x = "Group", 
        y = "Phonological Leveshtein distance\n(corrected for string length)",
        fill = "Test language"
    ) +
    scale_x_continuous(
        labels = percent
    ) +
    scale_y_reordered() +
    theme(
        legend.position = "none",
        axis.title.y = element_blank(),
        axis.text.y = element_text(size = 7, hjust = 1),
        panel.grid.major.x = element_line(
            colour = "grey",
            linetype = "dotted"
        )
    ) 
ggsave(here("img", "lv.png"), height = 12)

```


## Global similarity

```{r global-similarity, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
stimuli %>% 
    ggplot() +
    aes(
        x = global_similarity, 
        y = reorder_within(translation, global_similarity, group),
        label = percent(global_similarity, accuracy = 0.1),
        fill = group,
        colour = group,
    ) + 
    facet_wrap(~group, scales = "free_y") + 
    geom_text(
        size = 2,
        hjust = 0
    ) +
    labs(
        x = "Global similarity (mean)",
        y = "Translation"
    ) +
    scale_y_reordered() +
    scale_x_continuous(
        limits = c(0, 0.2),
        labels = percent
    ) +
    theme(
        legend.position = "none",
        axis.text.y = element_text(size = 7, hjust = 1),
        axis.title.y = element_blank(),
        panel.grid.major.x = element_line(
            colour = "grey",
            linetype = "dotted"
        )
    )

ggsave(here("img", "global_similarity.png"), height = 12)
```

## Distributions

```{r stimuli-plot, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, out.width="100%", fig.width=7, fig.height=5.5, fig.cap="Distribution and summary statistics of stimuli translations, plotted in different panels for English translations of Catalan stimuli (cat-ENG), English translations of Spanish stimuli (spa-ENG), and Spanish translation of Catalan stimuli (cat-SPA). A\\) Lexical frequency of stimuli translations, expressed in counts per million and as Zipf scores. B\\) Number of higher-frequency phonological neighbourhoods (PTHN) of stimuli translations. C\\) Normalised Levenshtein phonological similarity between the presented stimuli and their translations, expressed in percentages. Lexical frequencies and PTHN scores of the stimuli (presented auditorily in Catalan or Spanish) are not described, as participants lack any lexical representation of such words."}

medians <- stimuli %>% 
    select(group, word_1, freq_2, freq_zipf_2, nd, global_similarity, lv) %>% 
    group_by(group) %>% 
    summarise_at(
        vars(freq_2:lv), 
        list(
            median = ~median(., na.rm = TRUE),  
            sd = ~sd(., na.rm = TRUE)
        )
    ) 


# frequency
stimuli %>% 
    ggplot() + 
    aes(
        x = freq_zipf_2,
        fill = group
    ) + 
    facet_wrap(~group, ncol = 1) + 
    geom_rect(
        data = medians, 
        aes(
            xmin = freq_zipf_2_median-freq_zipf_2_sd,
            xmax = freq_zipf_2_median+freq_zipf_2_sd, 
            ymin = -Inf,
            ymax = Inf, 
            fill = group
        ),
        inherit.aes = FALSE,
        alpha = 0.15
    ) +   
    geom_histogram(
        bins = 20,
        colour = "white"
    ) +
    geom_vline(
        data = medians, 
        aes(
            xintercept = freq_zipf_2_median,
            group = group
        ), 
        colour = "black",
        size = 0.5,
        linetype = "dashed"
    ) + 
    geom_text(
        data = medians, 
        aes(
            label = "Median ± SD", 
            x = freq_zipf_2_median+1.25,
            y = 13
        ), 
        size = 3
    ) +
    geom_curve(
        data = medians, 
        aes(
            x = freq_zipf_2_median+0.85, 
            xend = freq_zipf_2_median+0.1,
            y = 12, 
            yend = 11
        ),
        arrow = arrow(length = unit(0.2, "cm")),
        curvature = -0.25,
        colour = "grey30"
    ) +
    labs(
        x = "Lexical frequency (Zipf)", 
        y = "Number of trials", 
        title = "Frequency\n(stimuli translations)\n"
    ) +
    
    # PTHN 1
    stimuli %>% 
    count(group, nd) %>% 
    ggplot(
        aes(
            x = nd, 
            y = n,
            fill = group
        )
    ) + 
    facet_wrap(~group, ncol = 1) + 
    geom_rect(
        data = medians, 
        aes(
            xmin = 0, 
            xmax = nd_median+nd_sd, 
            ymin = -Inf, 
            ymax = Inf,
            fill = group
        ), 
        inherit.aes = FALSE, 
        alpha = 0.15
    ) +   
    geom_col(colour = "white") + 
    geom_vline(
        data = medians, 
        aes(
            xintercept = nd_median,
            group = group
        ), 
        colour = "black", 
        size = 0.5,
        linetype = "dashed"
    ) + 
    labs(
        x = "PTHN across languages", 
        y = "Number of trials", 
        title = "Higher-frequency neighbours\n(stimuli translations)\n"
    ) +
    
    # global similarity
    stimuli %>% 
    ggplot(
        aes(
            x = global_similarity, 
            fill = group
        )
    ) + 
    facet_wrap(~group, ncol = 1) + 
    geom_rect(
        data = medians, 
        aes(
            xmin = 0, 
            xmax = global_similarity_median+global_similarity_sd, 
            ymin = -Inf, 
            ymax = Inf,
            fill = group
        ), 
        inherit.aes = FALSE, 
        alpha = 0.15
    ) +   
    geom_histogram(colour = "white") + 
    geom_vline(
        data = medians, 
        aes(
            xintercept = global_similarity_median,
            group = group
        ), 
        colour = "black", 
        size = 0.5,
        linetype = "dashed"
    ) + 
    labs(
        x = "Global similarity across languages", 
        y = "Number of trials", 
        title = "Mean cross-language similarity\n(stimuli translations)\n"
    ) +
    
    
    # Levenshtein
    stimuli %>% 
    ggplot() +
    aes(lv, fill = group) +
    facet_wrap(~group, ncol = 1) + 
    geom_rect(
        data = medians, 
        aes(
            xmin = 0,
            xmax = lv_median+lv_sd, 
            ymin = -Inf,
            ymax = Inf, fill = group
        ),
        inherit.aes = FALSE,
        alpha = 0.15
    ) +   
    geom_histogram(
        bins = 20,
        colour = "white"
    ) +
    geom_vline(
        data = medians,
        aes(
            xintercept = lv_median, 
            group = group
        ), 
        colour = "black",
        size = 0.5,
        linetype = "dashed"
    ) + 
    scale_x_continuous(labels = percent) +
    labs(
        x = "Levenshtein",
        y = "Number of trials",
        title = "Phonological similarity\n(stimuli and stimuli translations)\n"
    ) +
    
    plot_layout(
        guides = "collect",
        nrow = 1
    ) &
    plot_annotation(tag_levels = "A") &
    scale_fill_manual(
        values = c("#1A85FF", "#ff2976", "#FFC20A"), 
        na.translate = FALSE
    ) &
    theme(legend.position = "none")

```


# Participants


```{r participants_coded, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
participants_coded <- participants %>% 
    mutate(valid_participant = ifelse(valid_participant, "Valid", "Not valid"))
```

We collected data from `r format(as_date(min(participants$date)), "%B %dth %Y")` to `r format(as_date(max(participants$date)), "%B %dth %Y")`. We tested `r length(unique(participants$participant_id))` participants. `r length(unique(participants$participant_id[participants$group=="spa-ENG"]))` were English natives tested in Spanish, `r length(unique(participants$participant_id[participants$group=="cat-ENG"]))` were English natives tested in Catalan, and `r length(unique(participants$participant_id[participants$group=="cat-SPA"]))` were Spanish natives tested in Catalan. `r sum(!participants$valid_participant)` failed to meet all the inclusion criteria and were excluded from further analyses. The final sample comprised data from `r sum(!participants$valid_participant)` participants, `r sum(participants$valid_participant[participants$group=="spa-ENG"])` in the spa-ENG group, `r sum(participants$valid_participant[participants$group=="cat-ENG"])` in the cat-ENG group, and `r sum(participants$valid_participant[participants$group=="cat-SPA"])` in the cat-SPA group.

Participants were included if^[We originally planned to exclude participants that reported any visual impairment that glasses would not correct. This item was phrased as Do you have normal or corrected-to-normal VISION? (Yes/No) in English, and as ¿Tienes problemas de VISIÓN que unas gafas o lentes de contacto NO corrijan? (Sí/No). Surprisingly, the proportion of Spanish participants that reported visual impairment was implausibly large (n = 6, 18.18%). This is possibly due to some participants using glasses daily and not having read the item until the end, where it is indicated that the use of glasses is considered as normal vision]:

* Aged 18 to 25 years
* Did not report being fluent in Catalan, Spanish, or Italian
* Provided at least 80% of the presented trials
* Did not report motor, auditory or visual (other than wearing glasses) problems

Trials were considered valid if:

* Participant did not leave a blank response
* Participant took longer than 10 seconds to respond


## Summary

```{r participants_summary, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

participants_coded %>% 
    count(
        group, 
        valid_participant
    ) %>% 
    mutate(valid_participant = factor(valid_participant)) %>% 
    ggplot() +
    aes(
        group,
        n, 
        fill = valid_participant,
        order = n
    ) + 
    geom_col() +
    labs(
        x = "Group", 
        y = "# participants", 
        fill = "Valid?"
    ) +
    theme(
        legend.title = element_blank(),
        legend.position = "top",
        axis.title.x = element_blank(),
        panel.grid.major.y = element_line(
            linetype = "dotted", 
            colour = "grey"
        )
    )

```

## Valid trials

```{r participants_stimuli, echo=FALSE, fig.height=4, fig.width=8, message=FALSE, warning=FALSE, paged.print=FALSE}

participants_coded %>% 
    filter(valid_participant=="Valid") %>% 
    select(
        participant_id, 
        group, 
        n,
        n_valid
    ) %>%
    mutate(valid_prop = n_valid/n) %>% 
    ggplot() +
    aes(
        x = valid_prop, 
        fill = group,
        colour = group
    ) + 
    facet_wrap(~group) +
    geom_histogram() +
    labs(
        x = "Valid trials (%)",
        y = "N participants",
        fill = "Group",
        colour = "Group"
    ) +
    scale_x_continuous(labels = percent) +
    theme(
        legend.title = element_blank(),
        legend.position = "none",
        axis.title.x = element_blank(),
        panel.grid.major.y = element_line(
            linetype = "dotted", 
            colour = "grey"
        )
    )
```

## Demographic information

```{r participants_demo, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
participants_valid <- filter(
    participants_coded,
    valid_participant=="Valid"
)

p_sex <- participants_valid %>% 
    left_join(count(participants_valid, group, name = "n_total")) %>% 
    mutate(group = paste0(group,  "\n(N = ", n_total, ")")) %>% 
    ggplot() +
    aes(
        x = group, 
        fill = sex
    ) +
    geom_bar(position = position_fill()) +
    labs(
        x = "Group", 
        y = "# participants", 
        fill = "Sex"
    ) +
    scale_y_continuous(labels = percent) +
    theme(
        legend.position = "top",
        axis.title.x = element_blank()
    )

p_age <- participants_valid %>% 
    left_join(count(participants_valid, group, name = "n_total")) %>% 
    mutate(group = paste0(group,  " (N = ", n_total, ")")) %>% 
    ggplot(
        aes(
            as.factor(age),
            fill = group,
            colour = group
        )
    ) +
    facet_wrap(~group, ncol = 1) +
    geom_bar(colour = "white") +
    labs(
        x = "Age (years)", 
        y = "% participants", 
        fill = "Group", 
        colour = "Group"
    ) +
    theme(
        legend.position = "none"
    )


(p_sex + p_age) &
    plot_layout() &
    plot_annotation(tag_levels = "A") &
    theme(
        legend.title = element_blank(),
        panel.grid.major.y = element_line(
            linetype = "dotted", 
            colour = "grey"
        )
    )

```



## Second language

```{r participants_l2, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

participants_valid %>% 
    count(group, l2, .drop = FALSE) %>% 
    mutate(
        l2 = case_when(
            l2=="Alemã\u0081n" ~ "German",
            l2=="Francã\u2030s" ~ "French",
            l2=="Cuã\u0081l" ~ "Missing",
            TRUE ~ l2
        ),
    ) %>% 
    left_join(
        count(
            participants_valid, 
            group, 
            name = "n_total"
        )
    ) %>% 
    mutate(group = paste0(group,  "\n(N = ", n_total, ")")) %>% 
    ggplot() +
    aes(
        group,
        n,
        fill = l2
    ) + 
    facet_wrap(
        ~group, 
        ncol = 1,
        scales = "free_y"
    ) +
    geom_col(position = position_fill()) +
    labs(
        x = "L2", 
        y = "# participants", 
        fill = "L2"
    ) +
    scale_y_continuous(labels = percent) +
    coord_flip() +
    theme(
        legend.position = "top",
        legend.title = element_blank(),
        strip.text = element_blank(),
        panel.grid = element_blank(),
        axis.text.x = element_text(),
        axis.title.y = element_blank()
    )

```


## Language profile proficiency

```{r participants_proficiency, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

participants_valid %>% 
    select(
        participant_id, 
        group,
        catalan_oral, 
        spanish_oral, 
        catalan_written,
        spanish_written
    ) %>% 
    mutate_at(
        vars(
            catalan_oral,
            spanish_oral,
            catalan_written,
            spanish_written
        ), 
        replace_na, 
        0
    ) %>% 
    pivot_longer(
        c(
            catalan_oral,
            spanish_oral, 
            catalan_written, 
            spanish_written
        ),
        names_to = "type",
        values_to = "score"
    ) %>% 
    separate(
        type, 
        c("language", "modality"), 
        sep = "_"
    ) %>% 
    mutate_at(
        vars(language, modality), 
        str_to_sentence
    ) %>%
    left_join(
        count(
            participants_valid,
            group, 
            name = "n_total"
        )
    ) %>% 
    mutate(group = paste0(group, " (N = ", n_total, ")")) %>% 
    ggplot(
        aes(
            as.integer(score), 
            fill = language)
    ) +
    facet_grid(modality~group) +
    geom_bar(
        position = position_dodge(),
        colour = "white"
    ) +
    labs(
        y = "# participants", 
        x = "Self-reported proficiency (0-5)",
        fill = "Language"
    ) +
    theme(
        legend.position = "top",
        panel.grid.major.y = element_line(
            colour = "grey",
            linetype = "dotted"
        )
    )

```


# Data analysis

We fitted a Bayesian multilevel regression model using the R package `brms` [@burkner2017brms], with correct responses (`correct`, 0 is incorrect response, 1 if correct response) as the response variable with a Bernoulli distribution and a logit link function. We modelled the probability of an average participant providing a correct response, conditional to a set of predictors (aka. fixed effects) and their interactions:

* PTHN (`pthn`, 0-Inf): number of phonological neighbours with higher frequency of the target word. Extracted from the CLEARPOND database. Standardised before entering the regression model.
* Phonological similarity (`lv`, 0-1): Inverse Levenshtein distance between the IPA transcription of the presented word and its correct translation in the target language, calculated using the `stringdist` function of the `stringdist` package [@loo2014stringdist]. Standardised before entering the regression model.
* Shared phonological onset (`onset`, Same/Different): Whether the presented and the target word share phonological onset, as judged by experimenters, and sum coded as $Different = -0.5$ and $Same = +0.5$. 

We added participants (`participants`) and as a grouping variable (aka., random effects), therefore adding participant-level adjustments to the model. We specified random intercepts slopes for all fixed effects within the `participant` grouping variable.

We performed multiple imputation via predictive mean matching using the `mice` function of the `mice` package [@van2011mice] to impute missing values in the response variable or the predictors. 

The model was implemented using the following formula (`fit_3`):

$$\begin{align}

&\textbf{Likelihood}  \\
y_{i} \sim& Bernoulli(p_{i}) && \text{[probability of correct translation]} \\ \\

&\textbf{Parameters}  \\

logit(p_{i}) = ~ &  \beta_{0[p,w]} ~ +  && \text{[linear model]}\\
& \beta_{1[p]} ~ Frequency_{i} ~ + \\
& \beta_{2[p]} ~ PTHN_i ~ + \\
& \beta_{3[p]} ~ Similarity_i ~ + \\
& \beta_{4[p]} ~ (PTHN_i \times Similarity_i) \\ \\

\beta_{0-6[p,w]} \sim& ~  \mathcal{N}(\mu_{\beta_{j}}, \sigma_{\beta_{j}}) \text{, for participant } p ~\text{in 1, ..., } P ~\text{and  word } w ~\text{in 1, ..., } W && \text{[participant- and word-level intercepts]} \\
\beta_{1-6[p]} \sim& ~  \mathcal{N}(\mu_{\beta_{j}}, \sigma_{\beta_{j}}) \text{, for participant } p ~\text{in 1, ..., } P
&& \text{[participant-level coefficients]} \\ \\

&\textbf{Prior}  \\

\mu_{\beta_{p,w}} ~ \sim& ~ \mathcal{N}(0, 0.1) && \text{[participant-level coefficients]} \\
\sigma_{\beta_{p}}, ~ \sigma_{\beta_{w}} \sim& ~ HalfCauchy(0, 0.1) && \text{[SD for population and participant]} \\
\rho_{p}, ~ \rho_{w} \sim& ~LKJ(8) && \text{[correlation between participant-level coefficients]} \\


\end{align}$$

The estimation of the model was performed using Bayesian inference via Hamiltonian Monte-Carlo in Stan [@carpenter2017stan], with 4 chains, 2,000 iterations each (1,000 warm-ups iterations). The output of this model is the approximated posterior distribution of all the parameters estimated, therefore indicating the probability of each combination of values of the parameters, conditional to the observed data. 

We compared this (extended model) to other models dropping one predictor at a time using Leave-one-out cross-validation (LOO-CV), using the `loo` and `loo_compare` functions of the `brms` package. We compared our extended model against the following models:

* fit_0: `~ 1 + (1 | participant) + (1 | word)`
* fit_1: `~ 1 + frequency_zipf + (1 + frequency_zipf | participant) + (1 | word)`
* fit_2: `~ 1 + frequency_zipf + pthn + (1 + frequency_zipf + pthn | participant) + (1 | word)`
* fit_3: `~ 1 + frequency_zipf + pthn + lv + (1 + frequency_zipf + pthn + lv | participant) + (1 | word)`
* fit_4: `~ 1 + frequency_zipf + pthn + lv + pthn:lv + (1 + frequency_zipf + pthn + pthn:lv + lv | participant) + (1 | word)`
* fit_5: `~ 1 + frequency_zipf + pthn + lv + pthn:lv + group + (1 + frequency_zipf + pthn + pthn:lv + lv | participant) + (1 | word)`
* fit_6: `~ 1 + frequency_zipf + pthn + lv + pthn:lv + group + group:lv + (1 + frequency_zipf + pthn + pthn:lv + lv | participant) + (1 | word)`



# Results


```{r responses_coded, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
responses_coded <- responses %>% 
    # typos are considered correct responses
    mutate_at(vars(group), as.factor) %>% 
    # center predictors
    mutate_at(
        vars(lv_std, nd_std, freq_zipf_2_std),
        function(x) scale(x, center = TRUE, scale = TRUE)[,1]) %>% 
    # impute missing data
    as_tibble() %>% 
    arrange(group, word)
```


## Model comparison

Leave-one-out cross-validation. The more negative is `elpd_diff` (and the larger its magnitude compared to its corresponding standard error), the better it first the data compared to the null model (`~ 1 + (1 | participant)`). The model that fits the data the best is the extended model.


```{r comparison, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

model_loos_gs %>% 
    as.data.frame() %>% 
    rownames_to_column("model") %>% 
    arrange(desc(elpd_loo)) %>% 
    select(
        model,
        matches("elpd_loo"),
        matches("looic"),
        matches("diff")
    ) %>%
    mutate(model = str_replace(model, "fit_", "Model ")) %>% 
    mutate_all(~ifelse(. == 0, NA, .)) %>% 
    gt(rowname_col = "model") %>% 
    tab_header(
        title = md("Table 2"),
        subtitle = md("Model performance and comparison using leave-one-out cross-validation")
    ) %>% 
    tab_source_note(
        md("*Note*. Outcomes of the leave-one-out cross-validation procedure. Each row indicates the values of the expected log-predicted density (*ELPD*), leave-one-out information creterion (*LOO-IC*), and the difference in *ELPD* (*LOO-diff*) between each model and the inmediately simpler model. Each value is accompanied by a standard error (*SE*) indicating the uncertainty of its estimate.")
    ) %>% 
    fmt_missing(everything(), missing_text = "-") %>% 
    fmt_number(2:7) %>% 
    cols_label(
        model = md("**Model**"),
        looic = md("***LOO<sub>IC</sub>***"),
        se_looic = md("***SE*<sub>IC</sub>**"),
        elpd_loo = md("***LOO<sub>ELPD</sub>***"),
        se_elpd_loo = md("***SE***"),
        elpd_diff = md("***LOO<sub>diff</sub>***"),
        se_diff = md("***SE<sub>diff</sub>***")
    ) %>% 
    tab_style(
        style = cell_text(align = "left", weight = "bold", size = "medium"),
        locations = cells_title(groups = "title")
    ) %>% 
    tab_style(
        style = cell_text(align = "left", size = "medium", style = "italic"),
        locations = cells_title(groups = "subtitle")
    ) %>% 
    tab_style(
        style = cell_text(align = "left", size = "medium"),
        locations = cells_source_notes()
    ) %>% 
    tab_style(
        style = list(
            cell_borders(
                sides = c("left", "right"), 
                color = "white"
            ),
            cell_text(
                align = "center"
            )
        ),
        locations = list(
            cells_body(), 
            cells_row_groups(), 
            cells_column_spanners(), 
            cells_stub(),
            cells_source_notes(),
            cells_column_labels(),
            cells_stubhead()
        )
    ) %>% 
    tab_style(
        style = list(
            cell_text(align = "left", size = "medium"), 
            cell_borders(sides = "all", color = "white")
        ),
        locations = list(
            cells_stub(), 
            cells_source_notes()
        )
    ) %>% 
    tab_style(
        style = cell_text(
            align = "center", 
            size = "medium",
            weight = "normal",
            style = "normal"
        ),
        locations = list(
            cells_column_spanners(),
            cells_column_labels()
        )
    ) %>% 
    tab_style(
        style = cell_borders(sides = "all", color = "white"),
        locations = cells_body(columns = 1:7)
    ) %>% 
    tab_style(
        style = cell_borders(sides = "top", color = "white"),
        locations = cells_body(columns = 1:7)
    ) %>% 
    as_raw_html()



```




### Fixed coefficients

```{r coefs_fixed_table, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

coefs <- fixef(fit_4_gs) %>% 
    as.data.frame() %>% 
    rownames_to_column("variable") %>% 
    clean_names() %>% 
    mutate_at(
        vars(estimate:q97_5), 
        ~ifelse(
            variable=="Intercept",
            inv_logit_scaled(.),
            ./4
        )
    ) 

str_repl <- c(
    "Intercept" = "Intercept",
    "freq_zipf_2_std" = "Frequency\n[+1 SD]",
    "global_similarity_std:lv_std" = "Glob. similarity \u00d7 Levenshtein", 
    "global_similarity_std" = "Glob. similarity \n[+1 SD]",
    "lv_std" = "Levenshtein\n[+1 SD]"
)

coefs %>% 
    mutate(
        variable = str_remove(variable, "b_") %>% 
            str_replace_all(str_repl) %>% 
            factor(
                levels = c(
                    "Intercept", 
                    "Frequency\n[+1 SD]", 
                    "Glob. similarity \n[+1 SD]", 
                    "Levenshtein\n[+1 SD]",
                    "Glob. similarity \u00d7 Levenshtein"
                )
            )
    ) %>% 
    select(-est_error) %>% 
    gt() %>% 
    fmt_percent(2:4) %>% 
    cols_merge(
        c("q2_5", "q97_5"), 
        pattern = "[{1}, {2}]"
    ) %>% 
    cols_label(
        variable = md("**Predictor**"),
        estimate = md("**Mean**"),
        q2_5 = md("**95\\% CrI**"),
    ) %>% 
    tab_source_note("Estimated posterior distributions of coefficients in Model 6. A\\) Population-level effects. Distributions indicate the estimated posterior likelihood density of regression coefficients of fixed effects. Credible intervals (*CrI*), represented with increasingly lighter segmentents in the distribution indicate the range of values that contain the true value with 95\\%, 80\\%, and 50\\% probability. Black dots represent the mean of each distribution.") %>% 
    cols_align(align = "left") %>% 
    as_raw_html()
```


```{r posteriorfix, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Estimated posterior distributions of coefficients in Model 4. A\\) Population-level effects. Distributions indicate the estimated posterior likelihood density of regression coefficients of fixed effects. Credible intervals (*CrI*), represented with increasingly lighter segmentents in the distribution indicate the range of values that contain the true value with 95\\%, 80\\%, and 50\\% probability. Black dots represent the mean of each distribution. B\\) Participant\\-level coefficient variability. The model estimated participant\\-level coefficients to account for the dependency between responses from the same participant. Distributions in this panel indicate the estimated variability across coefficients from different participants, expressed as standard deviations (*SD*). C\\) Correlation between participant\\-level effects. The model allowed participant-level coefficients to co\\-vary. This panel represents the Pearson correlations between each pair of coefficients, expressed as the mean of the posterior distribution of each  estimated correlation.", fig.height=7, fig.width=6.5}


coefs <- fixef(fit_4_gs) %>% 
    as.data.frame() %>% 
    rownames_to_column("variable") %>% 
    clean_names() %>% 
    mutate_at(
        vars(estimate:q97_5),
        ~ifelse(
            variable=="Intercept", 
            inv_logit_scaled(.), 
            ./4
        )
    ) %>% 
    group_split(variable) %>% 
    set_names(make_clean_names(map(., "variable"))) %>% 
    map(select, -variable) %>% 
    map(unlist)

str_repl <- c(
    "Intercept" = "Intercept",
    "freq_zipf_2_std" = "Frequency\n[+1 SD]",
    "global_similarity_std:lv_std" = "Glob. similarity \u00d7 Levenshtein", 
    "lv_std" = "Levenshtein\n[+1 SD]",
    "global_similarity_std" = "Glob. similarity \n[+1 SD]"
)

post_draws <-  gather_draws(
    fit_4_gs, 
    `b_.*`, 
    `sd_.*`, 
    regex = TRUE
) 

# fixed effects
post_fix <- post_draws %>% 
    filter(str_detect(.variable, "b_")) %>% 
    mutate(
        .variable_name = str_remove(.variable, "b_") %>% 
            str_replace_all(str_repl) %>%  
            factor(
                levels = c(
                    "Intercept", 
                    "Frequency\n[+1 SD]",
                    "Glob. similarity \n[+1 SD]", 
                    "Levenshtein\n[+1 SD]",
                    "Glob. similarity \u00d7 Levenshtein",
                    "Group 1\n[cat/spa-ENG vs. cat-SPA]",
                    "Group 2\n[cat-ENG vs. spa-ENG]",
                    "Levenshtein \u00d7 Group 1", 
                    "Levenshtein \u00d7 Group 2"
                )
            ),
        .value = ifelse(
            str_detect(.variable, "Intercept"),
            inv_logit_scaled(.value), 
            .value/4
        )
    ) %>% 
    arrange(.variable) %>% 
    ggplot() +
    aes(
        .value,
        fct_rev(.variable_name)
    ) +
    geom_vline(xintercept = 0) +
    stat_slab(
        aes(
            fill = stat(
                cut_cdf_qi(
                    cdf, 
                    .width = c(.5, .8, .95),
                    labels = percent_format()
                )
            )
        )
    ) +
    stat_pointinterval(
        .width = 0, 
        point_size = 1
    ) +
    scale_fill_manual(
        values = c("#1A85FF", "#9ccaff", "#d2e5fc"),
        na.translate = FALSE
    ) +
    scale_x_continuous(
        labels = function(x) percent(round(x, 1))
    ) +
    labs(
        x = "P(Correct)", 
        y = "Posterior probability density",
        fill = "CrI"
    ) +
    theme(
        legend.position = c(1, 0.25),
        legend.direction = "vertical",
        legend.background = element_rect(
            fill = "grey95"
        ),
        legend.key = element_rect(
            fill = "grey95", 
            colour = "grey95"
        ), 
        legend.key.height = unit(0.1, "cm"),
        axis.title.y = element_blank(),
        legend.text = element_text(
            size = 7
        ),
        axis.text.x = element_text(
            colour = "black"
        ),
        axis.text.y = element_text(
            colour = "black",
            hjust = 1
        ),
        axis.ticks.y = element_blank(),
        panel.grid.major.y = element_line(
            colour = "grey", 
            size = 0.5
        )
    ) 




# Pearson correlations between random effects
corr_mat <- VarCorr(fit_4_gs, summary = TRUE)$participant$cor 
corr_names <- rownames(VarCorr(fit_4_gs, summary = TRUE)$participant$cor)

corr_mat <- corr_mat %>% 
    as_tibble() %>% 
    select(matches("Estimate")) %>% 
    rename_all(str_remove, "Estimate.") %>% 
    mutate(term1 = corr_names)

corr_mat[lower.tri(corr_mat)] <- NA

corr_data <- corr_mat %>% 
    as.data.frame() %>% 
    pivot_longer(
        -term1, 
        names_to = "term2",
        values_to = ".value"
    ) %>% 
    drop_na(.value)

post_cors <- corr_data %>% 
    mutate(
        term1 = factor(
            term1, 
            levels = names(str_repl),
            ordered = TRUE
        ),
        term2 = factor(
            term2, 
            levels = names(str_repl),
            ordered = TRUE
        )
    ) %>% 
    mutate_at(
        vars(term1, term2),
        str_replace_all, 
        str_repl
    ) %>% 
    mutate(
        .value_label = case_when(
            term1==term2 ~ NA_character_, 
            TRUE ~ printnum(.value, gt1 = FALSE)
        ),
        .value = case_when(
            term1==term2 ~ NA_real_, 
            TRUE ~ .value
        )
    ) %>% 
    filter(
        term1 != "PTHN × Levenshtein", 
        term2 != "Intercept"
    ) %>% 
    ggplot() +
    aes(
        fct_inorder(term1),
        fct_rev(fct_inorder(term2)), 
        fill = .value
    ) +
    geom_tile(na.rm = TRUE) +
    geom_text(
        aes(label = .value_label), 
        size = 3, 
        na.rm = TRUE) +
    labs(
        x = "Term 1", 
        y = "Term 2",
        fill = "Posterior\ncorrelation"
    ) +
    scale_fill_gradient2(
        low = "#04bf78", 
        mid = "white",
        high = "#FFC20A", 
        na.value = "white", 
        limits = c(-0.1, 0.1)
    ) +
    scale_x_discrete(position = "top") +
    coord_equal() +
    theme(
        legend.position = c(1, 1),
        legend.direction = "horizontal",
        legend.key.height = unit(0.1, "cm"),
        legend.key.width = unit(0.3, "cm"),
        legend.justification = c(1, 1),
        legend.title = element_blank(),
        legend.text = element_text(size = 5),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        axis.ticks = element_blank(),
        axis.text.x = element_text(
            size = 6,
            angle = 90,
            hjust = 0, 
            vjust = 1),
        axis.text.y = element_text(
            size = 6,
            vjust = 1,
            hjust = 1
        )
    )

(post_fix | post_cors) +
    plot_layout(widths = c(0.6, 0.4)) +
    plot_annotation(tag_levels = "A")

```




```{r post_re_word, echo=FALSE, fig.height=9, fig.width=7, message=FALSE, warning=FALSE, paged.print=FALSE}
empirical_means <- responses %>% 
    group_by(group) %>% 
    summarise(
        successes = sum(correct, na.rm = TRUE),
        n = n(),
        .groups = "drop"
    ) %>% 
    mutate(
        prop = prop_adj(successes, n),
        prop_se = prop_adj_se(successes, n)
    )

empirical_accuracy <- responses %>% 
    mutate(
        translation = paste0(
            word_1, " /", ipa_flat_1, "/ - ", 
            word_2, " /", ipa_flat_2, "/ - ",
            percent(lv, accuracy = 1)
        )
    ) %>% 
    group_by(group, translation, lv) %>% 
    summarise(
        successes = sum(correct, na.rm = TRUE),
        n = n(),
        .groups = "drop"
    ) %>% 
    mutate(
        prop = successes/n,
        prop_se = sqrt((prop*(1-prop))/n)
    )


fixed_intercept <- fixef(fit_4_gs)[1 ,c(1, 3, 4)] %>% 
    inv_logit_scaled()

nd_re_words <- expand.grid(
    translation_id = unique(fit_4_gs$data$translation_id),
    global_similarity_std = c(-1, 1),
    freq_zipf_2_std = 0,
    lv_std = seq(
        min(fit_4_gs$data$lv_std, na.rm = TRUE), 
        max(fit_4_gs$data$lv_std, na.rm = TRUE
        ), by = 0.1
    ),
    group = NA
)

post_re <- add_epred_draws(
    nd_re_words, 
    fit_4_gs, 
    re_formula = ~ (1 | translation_id),
    ndraws = 50
) %>% 
    ungroup() %>% 
    select(-group) %>% 
    left_join(select(stimuli, group, translation, translation_id, lv)) %>% 
    mutate(translation = paste0(translation, " - ", percent(lv, accuracy = 1))) %>% 
    left_join(select(empirical_accuracy, translation, n, prop, prop_se))


post_re %>% 
    ggplot() +
    aes(
        x = .epred,
        y = reorder_within(translation, lv, group),
        colour = group
    ) +
    facet_wrap(~group, scales = "free_y") +
    # geom_vline(
    #     xintercept = fixed_intercept[1],
    #     colour = "#1A85FF", 
    #     size = 0.5,
    #     linetype = "dotted"
    # ) +
    geom_vline(
        data = empirical_means,
        aes(xintercept = prop),
        size = 0.75
    ) +
    # stat_interval(
    #     size = 1.5,
    #     .width = c(0.5, 0.67, 0.95),
    #     aes(y = reorder_within(translation, prop, group))
    # ) +
    geom_errorbar(
        data = empirical_accuracy,
        aes(
            x = prop,
            xmin = prop-prop_se,
            xmax = prop+prop_se,
            y = reorder_within(translation, prop, group)
        ),
        width = 0,
        size = 0.5
    ) +
    geom_point(
        data = empirical_accuracy,
        aes(
            x = prop,
            y = reorder_within(translation, prop, group)
        ),
        size = 1
    ) +
    labs(
        x = "Percentage of correct responses",
        y = "Item",
        title = "Observed translation accuracy",
        subtitle = "Dots indicate proportions of correct responses, error bars indicate +/-1 SE",
        colour = "Credible Interval (CrI)",
        caption = "Percentages next to items indicate the Levenshtein distance between both translations."
    ) +
    # scale_color_manual(
    #     values = c("#d2e5fc", "#9ccaff", "#1A85FF"),
    #     na.translate = FALSE, 
    #     labels = paste0(c(95, 67, 50), "%")
    # ) +
    scale_x_continuous(
        limits = c(0, 1), 
        labels = percent
    ) +
    scale_y_reordered() +
    theme(
        legend.position = "none",
        legend.key = element_rect(fill = NA, colour = NA),
        axis.text.x = element_text(size = 7),
        axis.text.y = element_text(size = 5, hjust = 1),
        axis.ticks = element_blank(),
        axis.title = element_text(size = 10),
        axis.title.y = element_blank(),
        panel.grid.major = element_line(
            size = 0.15,
            linetype = "dotted",
            colour = "grey"
        ),
        plot.caption.position = "plot",
        plot.caption = element_text(
            size = 9,
            hjust = 0, 
            face = "plain", 
            margin = margin(
                t = unit(3, "cm")
            )
        ),
        plot.title.position = "plot",
        plot.title = element_text(hjust = 0, size = 13),
        plot.subtitle = element_text(hjust = 0, face = "plain", size = 13)
    )

ggsave(here("img", "accuracy.png"), height = 9, width = 10, dpi = 1000)
```


## Marginal effects


### Global similarity

```{r marginaleffects, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.width=7, fig.height=3.5, fig.cap="Expected mean posterior predictions from Model 4. A) Population-level predictions: the X-axis and the Y-axis represent the PTHN score (in standard deviations from the mean) and the probability of correct translation, respectively. We simulated 300 observations from the posterior distribution of the model: 100 simulations for translations with no similarity (0% Levenshtein), 100 simulations for translations with mean accuracy (20.51% Levenshtein), and 100 simulations for translations with the maximum observed accuracy (83.33% Levenshtein). We did this across the range of values of the PTHN scores. For each simulation, we drew a single sample from the posterior distribution of each coefficient. Each simulation is depicted in the graph as a line: pink for minimum similarity translations, blue for hight similarity translations, and yellow for maximum similarity translations. Black, thick lines indicate the expected mean value of the posterior predictions of the model for each condition. The dispersion of the lines indicates the uncertainty of our predictions. We computed these posterior predictions for each group of participants, and plot tem in separate panels."}

nd <- expand.grid(
    lv_std = c(
        min(fit_6_gs$data$lv_std),
        0,
        max(fit_6_gs$data$lv_std)
    ),
    freq_zipf_2_std = 0,
    global_similarity_std = seq(
        min(fit_6_gs$data$global_similarity_std, na.rm = TRUE),
        max(fit_6_gs$data$global_similarity_std, na.rm = TRUE),
        by = 0.1
    ),
    group = unique(fit_6_gs$data$group)
)

m <- add_epred_draws(
    nd, 
    fit_6_gs, 
    # ndraws = 100, 
    re_formula = NA
) %>% 
    mutate(
        lv_std = factor(
            lv_std,
            levels = c(
                min(fit_6_gs$data$lv_std),
                0,
                max(fit_6_gs$data$lv_std)
            ),
            labels = c(
                paste0("-1 SD (", percent(min(responses$lv, na.rm = TRUE)), ")"),
                paste0("Mean (", percent(mean(responses$lv, na.rm = TRUE)), ")"),
                paste0("+1 SD (", percent(max(responses$lv, na.rm = TRUE)), ")")
            )
        ) 
    )


ggplot(m) +
    aes(
        x = global_similarity_std, 
        y = .epred,
        colour = group,
        fill = group
    ) +
    # facet_wrap(~group) +
    geom_hline(
        yintercept = 0.5, 
        colour = "grey",
        size = 1
    ) +
    # geom_line(
    #     aes(group = interaction(lv_std, .draw)), 
    #     size = 0.65,
    #     alpha = 0.5,
    #     show.legend = TRUE
    # # ) +
    # stat_lineribbon(
    #     alpha = 0.5,
    #     .width = 0.95,
    #     colour = NA
    # ) +
stat_summary(
    aes(linetype = lv_std), 
    fun = mean, 
    geom = "line",
    size = 0.75,
    show.legend = TRUE
) +
    annotate(
        "segment", 
        x = -0.5, 
        xend = -0.9, 
        y = 0.85, 
        yend = 0.85, 
        arrow = arrow(length = unit(0.1, "cm")), 
        size = .5
    ) +
    annotate(
        "segment", 
        x = 2.25, 
        xend = 2.65, 
        y = 0.85, 
        yend = 0.85, 
        arrow = arrow(length = unit(0.1, "cm")),
        size = .5
    ) +
    annotate(
        "text", 
        x = -0.5, 
        y = 0.95, 
        size = 2.75,
        label = "Less similar"
    ) +
    annotate(
        "text", 
        x = 2.15, 
        y = 0.95, 
        size = 2.75,
        label = "More similar"
    ) +
    labs(
        x = paste0("Neighbourhood density (+1 SD, ", round(sd(responses$nd), 1), " higher-frequency neighbours)"),
        y = "P(Correct)", 
        colour = "Levenshtein similarity",
        fill = "Levenshtein similarity",
        linetype = "Levenshtein similarity",
        title = "Population-level posterior predictions"
    ) +
    scale_colour_manual(
        values = c("#1A85FF", "#ff2976", "#FFC20A"),
        na.translate = FALSE
    ) +
    scale_y_continuous(
        labels = function(x) percent(round(x, 2)),
        limits = c(0, 1),
        breaks = seq(0, 1, 0.1)
    ) +
    theme(
        legend.position = "top",
        plot.title = element_blank()
    )


```

### Levenshtein distance

```{r marginaleffects-lv, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.width=7, fig.height=3.5, fig.cap="Expected mean posterior predictions from Model 4. A) Population-level predictions: the X-axis and the Y-axis represent the PTHN score (in standard deviations from the mean) and the probability of correct translation, respectively. We simulated 300 observations from the posterior distribution of the model: 100 simulations for translations with no similarity (0% Levenshtein), 100 simulations for translations with mean accuracy (20.51% Levenshtein), and 100 simulations for translations with the maximum observed accuracy (83.33% Levenshtein). We did this across the range of values of the PTHN scores. For each simulation, we drew a single sample from the posterior distribution of each coefficient. Each simulation is depicted in the graph as a line: pink for minimum similarity translations, blue for hight similarity translations, and yellow for maximum similarity translations. Black, thick lines indicate the expected mean value of the posterior predictions of the model for each condition. The dispersion of the lines indicates the uncertainty of our predictions. We computed these posterior predictions for each group of participants, and plot tem in separate panels."}

nd <- expand.grid(
    global_similarity_std = c(-1, 0, 1),
    freq_zipf_2_std = 0,
    lv_std = seq(
        min(fit_6_gs$data$lv_std, na.rm = TRUE),
        max(fit_6_gs$data$lv_std, na.rm = TRUE),
        by = 0.1
    ),
    group = unique(fit_6_gs$data$group)
)

m <- add_epred_draws(
    nd, 
    fit_6_gs, 
    # ndraws = 100, 
    re_formula = NA
) %>% 
    mutate(
        global_similarity_std = factor(
            global_similarity_std,
            levels = c(-1, 0, 1),
            labels = c("-1 SD", "Mean", "+1 SD")
        )
    ) 


ggplot(m) +
    aes(
        x = lv_std, 
        y = .epred,
        colour = group,
        fill = group
    ) +
    facet_wrap(~group) +
    geom_hline(
        yintercept = 0.5, 
        colour = "grey",
        size = 1
    ) +
    # geom_line(
    #     aes(group = interaction(lv_std, .draw)), 
    #     size = 0.65,
    #     alpha = 0.5,
    #     show.legend = TRUE
    # # ) +
    # stat_lineribbon(
    #     alpha = 0.5,
    #     .width = 0.95,
    #     colour = NA
    # ) +
stat_summary(
    aes(linetype = global_similarity_std), 
    fun = mean, 
    geom = "line",
    size = 0.75,
    show.legend = TRUE
) +
    annotate(
        "segment", 
        x = -0.5, 
        xend = -0.9, 
        y = 0.85, 
        yend = 0.85, 
        arrow = arrow(length = unit(0.1, "cm")), 
        size = .5
    ) +
    annotate(
        "segment", 
        x = 2.25, 
        xend = 2.65, 
        y = 0.85, 
        yend = 0.85, 
        arrow = arrow(length = unit(0.1, "cm")),
        size = .5
    ) +
    annotate(
        "text", 
        x = -0.5, 
        y = 0.95, 
        size = 2.75,
        label = "Less similar"
    ) +
    annotate(
        "text", 
        x = 2.15, 
        y = 0.95, 
        size = 2.75,
        label = "More similar"
    ) +
    labs(
        x = paste0("Neighbourhood density (+1 SD, ", round(sd(responses$nd), 1), " higher-frequency neighbours)"),
        y = "P(Correct)", 
        colour = "Levenshtein similarity",
        fill = "Levenshtein similarity",
        linetype = "Levenshtein similarity",
        title = "Population-level posterior predictions"
    ) +
    scale_colour_manual(
        values = c("#1A85FF", "#ff2976", "#FFC20A"),
        na.translate = FALSE
    ) +
    scale_y_continuous(
        labels = function(x) percent(round(x, 2)),
        limits = c(0, 1),
        breaks = seq(0, 1, 0.1)
    ) +
    theme(
        legend.position = "top",
        plot.title = element_blank()
    )


```

# Additional analyses

## Previous familiarity


```{r familiarity-frequency-distribution, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.height=10, fig.width=10}

familiarity_problematic <- here("data", "familiarity", "familiarity.xlsx") %>% read_xlsx() %>% 
    rename(
        word = presented,
        word_2 = translation,
        test_language = language,
        problem = note
    ) %>% 
    mutate(
        problem = ifelse(
            str_detect(problem, "check coding"), 
            "Check coding",
            str_to_sentence(problem)
        ),
        group = ifelse(test_language=="Catalan", "cat-ENG", "spa-ENG")
    ) %>% 
    select(word, group, problem)


familiarity <- stimuli %>% 
    filter(group != "cat-SPA") %>%
    mutate(
        stim = paste0(
            word_1, " /", ipa_flat_1, "/", " - ",
            word_2, " /", ipa_flat_2, "/", " - ",
            percent(lv, accuracy = 1)
        ),
        test_language = case_when(
            group=="cat-ENG" ~ "Catalan",
            group=="spa-ENG" ~ "Spanish",
            group=="cat-SPA" ~ "Catalan"
        ),
        word = replace_non_ascii(word_1)
    ) %>% 
    left_join(familiarity_problematic) %>% 
    select(
        group, 
        word = word,
        stim,
        problem
    ) %>% 
    left_join(filter(subtlex, test_language=="English")) %>% 
    replace_na(
        list(
            frequency_count = 1,
            frequency_zipf = 0.555,
            problem = "Normal"
        )
    ) %>% 
    left_join(
        select(
            word_accuracy, 
            group,
            stim,
            prop,
            prop_se
        )
    ) %>% 
    group_by(group) %>% 
    mutate(
        word_reorder = reorder_within(
            word, 
            frequency_zipf, 
            group
        )
    ) %>% 
    ungroup()

familiarity %>% 
    ggplot() +
    aes(
        x = frequency_zipf,
        y = word_reorder,
        fill = problem,
        colour = problem,
        label = word
    ) +
    facet_wrap(~group, scales = "free_y") +
    geom_text(size = 3) +
    labs(
        x = "Frequency (Zipf) of presented word in English",
        y = "Word",
        colour = "Problem"
    ) +
    scale_colour_manual(
        values = c("#1A85FF", "#ff2976", "#FFC20A", "black"),
        na.translate = FALSE
    ) +
    scale_y_reordered() +
    theme(
        legend.position = "top",
        panel.border = element_rect(fill = NA, colour = "grey"),
        panel.grid.major.x = element_line(
            colour = "grey", 
            linetype = "dotted"
        ),
        legend.title = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title.y = element_blank()
    )

ggsave(here("img", "problematic_frequency-distribution.png"), height = 10)

```

```{r previous-familiarity, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.height=3.5, fig.width=8}



familiarity %>% 
    filter(group != "cat-SPA") %>% 
    ggplot() +
    aes(
        x = frequency_zipf,
        y = prop,
        colour = problem,
        fill = problem,
        label = word
    ) +
    facet_wrap(~group) +
    geom_text(
        size = 3
    ) +
    # geom_point(
    #   shape = 1,
    #   size = 1,
    #   alpha = 0.5,
    #   stroke = 1
    # ) +
    geom_smooth(
        method = "lm",
        colour = "black",
        fill = "grey"
    ) +
    labs(
        x = "Frequency (Zipf) of presented word in English",
        y = "Accuracy",
        colour = "Group",
        fill = "Group"
    ) +
    scale_colour_manual(
        values = c("#1A85FF", "#ff2976", "#FFC20A", "black"),
        na.translate = FALSE
    ) +
    # scale_x_continuous(
    #   limits = c(2, 6)
    # ) +
    # scale_y_continuous(
    #   limits = c(0, 1),
    #   labels = percent
    # ) +
    theme(
        legend.position = "none"
    )

ggsave(
    here("img", "problematic_frequency-accuracy.png"), 
    height = 4, 
    width = 10
)


```

