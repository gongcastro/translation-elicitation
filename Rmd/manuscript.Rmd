---
title: "The role of cognateness in non-native spoken word recognition"

shorttitle: Cognateness and non-native word recognition

author:
- name: Gonzalo Garcia-Castro
  affiliation: '1'
  corresponding: yes
  address: Ramon Trial Fargas, 25-27, 08005 Barcelona
  email: gonzalo.garciadecastro@upf.edu
  role: 
    - Conceptualization
    - Data analysis
    - Writing - Original Draft Preparation
    - Writing - Review & Editing
- name: Serene Siow
  affiliation: '2'
  role:
    - Conceptualization
    - Writing - Review & Editing
- name: Nuria Sebastian-Galles
  affiliation: '1'
  role:
    - Conceptualization
    - Writing - Review & Editing
- name: Kim Plunkett
  affiliation: '2'
  role:
    - Conceptualization
    - Writing - Review & Editing
affiliation:
  - id            : "1"
    institution   : "Center for Brain and Cognition, Universitat Pompeu Fabra"
  - id            : "2"
    institution   : "Department of Experimental Psychology, University of Oxford"
    
authornote: |
  GG and SS contributed equally and share first authorship.
  
abstract: |
  There is compelling evidence that bilinguals access their lexicon in a language non-selective way. For example, bilinguals produce and translate cognates (words whose translation in the other language is form-similar) faster than non-cognates, suggesting that the phonology of both languages interact during word production and comprehension (Costa et al., 2000; Christoffels et al., 2006). Previous literature on this effect has often relied on measures of overall form-similarity to categorise words into cognates andnon-cognates, such as the Levenshtein distance. These measures partially ignore potential sources of variability during lexical access like vowel vs. consonant overlap, overlap in stressed syllables, onset overlap or the distance in features between both translations. In this study, we explored the impact of some of these variables on non-native word translation task: Spanish and English participants listened to non-native words (Catalan or Spanish) and were prompted to type their translation in their native language. Critically, participants where unfamiliar with the testing language, ensuring that they were not able to translate words based on previous knowledge on their meaning, leaving phonological information as the only cue participants were able to exploit to translate words to their native language correctly. We analysed the probability of correct translations, adjusting for the amount of vowel overlap, consonant overlap, overlap at stress position, overlap at onset, and distance in features between replaced phonemes, including the average frequency of phonological neighbours of the target words are a covariate.
  <!-- https://tinyurl.com/ybremelq -->
  
keywords: cognate, word recognition, translation, non-native, spoken word recognition
wordcount: X

bibliography:
  - r-references.bib
  - references.bib
floatsintext: yes
figurelist: no
tablelist: no
footnotelist: no
linenumbers: yes
mask: no
draft: no
csl: "apa7.csl"
documentclass: apa7
classoption: man
output:
  word_document:
    reference_docx: "manuscript_template.docx"
---


```{r setup, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}

set.seed(42)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  results = "asis", 
  dpi = 500
)

options(knitr.kable.NA = '-', knitr.duplicate.label = "allow")

```


```{r prepare, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}

# import objects
tar_load_globals()
tar_load(stimuli)
tar_load(participants)
tar_load(responses)
tar_load(model_fits)
tar_load(model_loos)
tar_load(posterior_draws_fixed)
tar_load(posterior_epreds_fixed)
tar_load(posterior_epreds_random)

# set custom ggplot2 theme
theme_set(
  theme(
    title = element_text(size = 8, face = "bold"),
    
    legend.background = element_rect(fill = NA, colour = NA),
    legend.key = element_rect(fill = "white", colour = NA),
    legend.justification = "right",
    legend.direction = "horizontal",
    
    axis.line = element_line(colour = "black", size = 0.5),
    axis.ticks = element_line(colour = "black", size = 0.5),
    axis.text.x = element_text(colour = "black"),
    axis.text.y = element_text(colour = "black"),
    
    plot.caption.position = "plot",
    plot.title = element_text(hjust = 0.5),
    
    
    panel.grid = element_blank(),
    panel.background = element_rect(fill = "white", colour = NA)
  )
)

```

# Introduction


Humans are able to recognise spoken words without much effort, even in adverse conditions such as compressed speech [@de1982relationship] or the loss of segmental information [@warren1970perceptual]. But even in the most ideal of the situations, the cognitive processes engaged in word recognition do not occur without uncertainty. The speech input activates multiple candidate lexical representations based on their similarity with the signal, and for word recognition to take place, one of the candidates must be selected. There is extensive literature about the influence on word recognition of the number of lexical candidates, their frequency, and their  phonological and semantic similarity with target words. Less is known about how all these factors affect the dynamics of word recognition in non-native languages.


Listening to speech in a non-native language is more costly than doing so in native languages, regardless of the level of proficiency [REF]. The source of this increased cognitive effort is likely to originate at multiple levels. For example, some sounds in the speech signal do not correspond to any phoneme in the native language, making the process of matching sounds to phonemes complicated. This is the case of a Spanish native listening to words containing the guttural /ʁ/ in French. Language learners, however, are rarely completely naïve to the language they are listening to. All languages share, to some extent, similarities, frequently due to their typological closeness. These similarities which can be exploited by non-native listeners to process the speech signal. In the present research we investigate the extent to which adult listeners rely on phonological similarity between cross-language synonyms during word translation.



Cognates are cross-language synonyms whose form (e.g., phonology, orthography, signature) is similar. The cause of such similarity is frequently attributed to a shared etymological origin. Romance languages such as Spanish and Catalan share many cognates [@schepens2012distributions], as in the case of *puerta* and *porta* (*door* in Spanish and Catalan, respectively)^[Some form-similar cross-language synonyms are technically not cognates. For example, *sun* and *sol* (in Spanish), share their phonological onset, but their etymology points to different origins. We will use the word *cognateness* to include all form-similar cross-language synonyms for simplicity. It is highly implausible that etymology plays a direct role on language perception if it is not via form-similarity, since it is not necessary for participants in psycholinguistic experiments to be aware of the etymology of the words they encounter in the tasks to be subject to the effect of form-similarity.].



Cognates play a major role in virtually all models of bilingual lexical processing because they provide evidence that bilinguals access the lexicon in a language non-selective way. During word production and comprehension, lexical representations of both languages are activated, and the form similarity between both impacts bilinguals' performance in naming or lexical decision tasks [e.g., @costa2000cognate; @thierry2007brain]. In a seminal study, @costa2000cognate asked Spanish-Catalan adult bilinguals to name pictures in Spanish. Unbeknownst to participants, half of the labels associated to pictures were cognates in Spanish and Catalan (e.g., *puerta*-*porta*) while the other half were non-cognates (e.g., *mesa*-*taula*, meaning *table* in Spanish and Catalan). Surprisingly, Spanish-Catalan bilinguals named cognate pictures faster than non-cognate pictures, while Spanish monolinguals did not show this effect. This suggests that bilinguals activate the picture's word representations in both languages, and the phonological overlap between both labels facilitated the naming process.


Additionally, cognates seem to be learnt more easily in L2 than non-cognates. @de2000hard presented Dutch natives pairs of Dutch words and pseudowords. Participants' performance learnt and retained better pairs with high orthographic similarity than pairs with low orthographic similarity [see also @lotto1998effects for Dutch natives learning Italian words].



Cognates are not only learnt faster than non-cognates, but translated faster too. Early accounts of bilingual lexical access suggest that low-proficiency learners first established links between newly acquired words in L2 and their meanings through their L1 translation equivalents [@potter1984lexical]. As learners become more proficient, the connection between L2 representations and their meaning grows stronger, and L2 word processing becomes less reliant on the mediation of L1 representations. The Revised Hierarchical Model [RHM; @kroll1994category] captured this assumption and predicted that translating words from L1 to L2 (forward translation) should take longer than translating from L2 to L1 (backward translation). The rationale behind this prediction is that backward translation relies more strongly on direct word-word links between L1 and L2 representations, while forward translation would rely more strongly on the mediation between the concept and the two word forms. One of the consequences of this prediction is that backward translation should be more sensitive to the form similarity between the L1 and the L2 representations: cognate words should be retrieved faster than non-cognate words during backward translation.


To test these predictions, @degroot1994forward and @de1992determinants asked Dutch natives with high (yet non-native) English proficiency to translate words from either Dutch to English (forward translation, L1 to L2) or from English to Dutch (backward translation, L2 to L1). Participants were presented with written words in English or Dutch, and asked to speak out loud their translation in the other language as fast as possible. Results who that translation times and accuracy were roughly equivalent in across both forward and backward conditions, and that semantic variables (e.g., concreteness), were positively associated with participants' performance, especially in the forward translation condition. Cognates were translated equally fast in both conditions, but non-cognates were translated faster during backward than forward translation, and translation of cognates was less affected  by semantic variables than non-cognates. 

These findings suggest that both conceptually mediated and direct translation routes are active during translation, although backward translation relies more strongly on direct links between L1 and L2 representations, which makes it more sensitive to cognateness (than forward translation). Interestingly, higher-proficiency bilinguals tested in the same task showed similar results. However, subsequent studies did not find differences in participant's performance during forward and backward translation, or even found better performances in forward translation [@christoffels2006memory; @christoffels2013language], contrary to the predictions of the RHM. Further models build upon this model, introducing a wider range of connections in their theorised lexica.


Since @kroll1994category proposed their RHM model, subsequent studies brought attention to the role of the network of connections that words establish with each other at the form-level (phonological or orthographic) or at the conceptual level. The Neighbourhood Activation Model [NAM; @luce1998recognizing] suggests that lexical selection is mediated not only by the lexical frequency of the target word (and the number of activated candidates), but also by the frequency of the candidates.


Subsequent behavioural experiments [e.g., @luce1998recognizing] show that English adult participants were tested in a lexical decision task in which words were presented across several conditions of noise-to-signal ratio. A computerised lexicon calculated number and average lexical frequency of phonological neighbours around each of the presented words. Participants answered more faster and more accurately to high frequency words, and words from high density neighbourhoods were also responded detected more accurately, but slowly. Therefore, average lexical frequency of  neighbourhoods was associated to a decrease in accuracy. Low frequency words were in fact detected more accurately in low density than in high density neighbours. These findings suggested that both word lexical frequency and neighbourhood density can facilitate spoken word recognition, but this effect is modulated by the frequency of the neighbourhood; when phonological neighbours are more frequent, lexical selection is hindered [@goldinger1989priming; @luce1990similarity].


The influence of phonological neighbourhoods and their structure on lexical processing poses rise important question in the field of bilingualism research. Do word representations in one language form part of the phonological or orthographic neighbours of the other language? The Bilingual Interactive Activation [BIA+; @van1998orthographic] proposes that lexical representations establish both excitatory and inhibitory connections with other representations from the same or the other language, forming an integrated lexicon for both languages [@mcclelland1981interactive]. Evidence supporting such integrated lexicon is provided by the fact that bilinguals' performance in word recognition tasks is sensitive to the orthographic neighbourhood density of the translation of the presented word. @van1998orthographic presented bilingual participants with words in only one language (target language) in a lexical decision task. When the translation of the presented word in the non-target language was part of a large neighbourhood, reaction times were slower. This provides evidence that the presented words activated orthographic neighbourhoods in the non-target language, which slowed down word recognition.



A more recent model of bilingual lexical processing is Multilink @dijkstra2019multilink, which integrates and formalises previous claims and predictions on how an interactive account of bilingual lexical access impacts word recognition, production, and more relevant to the aims of the present study, translation. This model assumes an integrated lexicon in which word representations from the two languages establish connections in a way that is similar to what words of a single language do. Such model also assumes that during backward translation, the presented word in L2 can activate form-similar words in L1 that compete with the target word in L1 and it proposes that translation equivalents are exclusively connected through their shared concept, contrary to the RHM model (which suggests that translation equivalents are connected by excitatory connections at the lexical level; e.g., translation can be done via word-word connections). In addition, this model suggests that the strength of the association between L2 representations and their concept is a function of language users' proficiency in L2. A remarkable correlation between Multilink simulations of @christoffels2006memory translation elicitation task and behavioural data with Dutch-English bilinguals has been reported, showing forward translation being faster than backward translation, contrary to the predictions made by the RHM model. Importantly, the model also generates data supporting the claim that cognates are translated faster than non-cognates, in line with previous literature on the cognate advantage during translation. Overall, results from Multilink from this model are compatible with an integrated bilingual lexicon and with the existence of a facilitatory effect of cross-language similarity during translation.  


The fact that Multilink fitted the experimental data from @christoffels2006memory with remarkable success, even in absence of word-word connections between translation equivalents might put into question the necessity of such connections during translation, in favour of a simpler model in which backward translation relies exclusively on the association between L2 and L1 word forms via their shared concept. This claim, however, should be taken. It should be taken into account, however, that @christoffels2006memory only tested fairly balanced bilinguals (i.e., bilinguals with high proficiency in L2). Additionally, data generated from Multilink model assumed similar vocabulary sizes across languages and, perhaps more critically in this regard, that word-concept connections were equally strong for L1 and L2 representations. These conclusions should therefore be taken with caution when extended to unbalanced bilinguals. It is low-proficiency bilinguals whom the RHM model predicts strong reliance on word-word connections during translation, especially during backward translation. This prediction remains untested my Multilink.


The role of this study was to investigate the plausibility of the lexical route as a mechanism exploited by monolingual adults during backward translation. Monolinguals can be considered a particular (extreme) case of unbalanced bilingualism, in which their vocabulary size in L2 is null, and word-concept connections are absent. In this case, monolinguals can only use the lexical route to suceeed in the translation task, and they can only do so by exploiting the cognateness of the words presented and their translation in the native language. If participants are able to translate words in L2, even with no knowledge of the language it belongs to, based on their form-similarity with the target word, this would suggest that the lexical route is in place for low-proficiency bilinguals. If participants are not able to translation words from L2 to L1 regardless of their form-similarity, this would suggest that low-proficiency bilinguals rely entirely on word-concept connections for backward translation.


# Methods

## Participants

Data collection took place from `r format(min(participants$date, na.rm = TRUE), "%B %dth, %Y")` to `r format(max(participants$date, na.rm = TRUE), "%B %dth, %Y")`. We collected data from `r length(unique(participants$participant))` participants (*Mean* = `r printnum(mean(participants$age, na.rm = TRUE))` years, *SD* = `r printnum(sd(participants$age, na.rm = TRUE))`, Range = `r printnum(min(participants$age, na.rm = TRUE))`-`r printnum(max(participants$age, na.rm = TRUE))`). `r length(unique(participants$participant[participants$group %in% c("ENG-CAT", "ENG-SPA")]))` participants were British English native speakers living in United Kingdom (`r length(unique(participants$participant[participants$group %in% c("ENG-CAT", "ENG-SPA") & participants$sex=="Female"]))` female), and `r length(unique(participants$participant[participants$group %in% c("SPA-CAT")]))` participants were Spanish native speakers living in Spain (`r length(unique(participants$participant[participants$group %in% c("SPA-CAT") & participants$sex=="Female"]))` female). Participants in UK were recruited via Prolific (5£ compensation) and SONA (compensation in academic credits). Participants in Spain were contacted via announcements in Faculties, and were compensated 5€ or an Amazon voucher for the same value. Participants gave informed consent before providing any data and the study was conducted in accordance with ethical standards of the Declaration of Helsinki and the protocol was approved by the local ethical committee (XXXXXXXXXXXX).  Participants were asked to complete the experiment using a laptop in a quiet place with good internet connection. We excluded data from participants that a) self-rated their oral and/or written skills in a second or third language as higher than 4 in a 5-point scale (*n* = `r participants %>% filter(l2written > 4 | l2oral > 4) %>% nrow()`), b) were diagnosed with a language (*n* = `r participants %>% filter(impairment) %>% nrow()`) , or c) did not contribute more than 80% of valid trials (*n* = `r participants %>% filter(invalid_participant_trials) %>% nrow()`).


## Procedure

The experiment was implemented online using Psychopy/Pavlovia [@peirce2019psychopy2]. Participants accessed the study from a link provided by Prolific or SONA and completed the experiment from an internet browser (Chrome or Mozilla). After giving their consent for participating, participants answered a series of questions about their demographic status, their language background, and the set up they were using for completing the study. Then, participants completed the experimental task. Participants were informed that they would listen to a series of pre-recorded words in Catalan or Spanish (English participants) or only Catalan (Spanish participants). They were instructed to listen to each word, guess its meaning in English (English participants) or Spanish (Spanish participants), and type their answer as soon as possible. English participants were randomly assigned to the list of Catalan or Spanish trials. Participants in the Catalan list were presented with `r nrow(stimuli[stimuli$group=="ENG-CAT",])` trials, and participants in the Spanish list were presented with `r nrow(stimuli[stimuli$group=="ENG-SPA",])` trials. Each trial started with a yellow fixation point presented during one second on the centre of the screen over a black background. After one second, the audio started playing while the dot remained being displayed until the audio ended. Upon the offset of the fixation point and audio, participants were prompted to write their answer by a ">" symbol. Typed letters were displayed in the screen in real time to provide visual feed-back to participants. Participants were allowed to correct their answer. Then, participants pressed the RETURN key to start and new trial. We excluded trials where participants did not type an existing word in the correspondent language, or did not type anything at all. Trials where the response was mistyped by only one character were counted as correct, as long as the respond did not correspond to a distinct word. Participants contributed a total of `r nrow(responses)` valid trials (`r nrow(responses[responses$group %in% c("ENG-CAT", "SPA-CAT"),])` in Catalan, `r nrow(responses[responses$group %in% c("ENG-SPA"),])` in Spanish). The task took approximately 15 minutes to be completed.



```{r procedurefigure, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, out.width="80%", fig.cap="Schematic representation of a trial in the experimental task."}

include_graphics(here("Figures", "design.png"))

```

## Stimuli

```{r stimulilengths, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
lengths <- stimuli %>% 
  select(trial_id, group, ipa1, ipa2, word1, word2) %>% 
  mutate_at(vars(ipa1:word2), nchar) %>% 
  group_by(group) %>% 
  summarise_at(vars(ipa1, ipa2, word1, word2), list(mean = mean, sd = sd, min = min, max = max)) %>%
  split(.$group) %>% 
  map(select, -group) %>% 
  set_names(make_clean_names(names(.))) %>% 
  map(unlist)
```

We arranged two lists of words: one in Catalan (to be presented to English and Spanish natives) and one in Spanish (to be presented to English natives only). Words in the Catalan list (listened to by participants) were `r printnum(lengths$eng_cat["ipa1_mean"])` phonemes long on average (*SD* = `r printnum(lengths$eng_cat["ipa1_sd"])`, Range = `r printnum(lengths$eng_cat["ipa1_min"], digits = 0)`-`r printnum(lengths$eng_cat["ipa1_max"], digits = 0)`). Their translations to English (typed by participants in the keyboard) were `r printnum(lengths$eng_cat["word2_mean"])` characters long on average (*SD* = `r printnum(lengths$eng_cat["word2_sd"])`, Range = `r printnum(lengths$eng_cat["word2_min"], digits = 0)`-`r printnum(lengths$eng_cat["word2_max"], digits = 0)`), and their translations to Spanish were `r printnum(lengths$spa_cat["word2_mean"])` characters long on average (*SD* = `r printnum(lengths$spa_cat["word2_sd"])`, Range = `r printnum(lengths$eng_cat["word2_min"], digits = 0)`-`r printnum(lengths$eng_cat["word2_max"], digits = 0)`). Words in the Spanish list were `r printnum(lengths$eng_spa["ipa1_mean"])` phonemes long on average (*SD* = `r printnum(lengths$eng_spa["ipa1_sd"])`, Range = `r printnum(lengths$eng_spa["ipa1_min"], digits = 0)`-`r printnum(lengths$eng_spa["ipa1_max"], digits = 0)`). Their translations to English were `r printnum(lengths$eng_spa["word2_mean"])` characters long on average (*SD* = `r printnum(lengths$eng_spa["word2_sd"])`, Range = `r printnum(lengths$eng_spa["word2_min"], digits = 0)`-`r printnum(lengths$eng_spa["word2_max"], digits = 0)`). 


We extracted the lexical frequencies from SUBTLEX-UK for English words [@van2014subtlex], SUBTLEX-ESP for Spanish words [@cuetos2011subtlex], and SUBTLEX-CAT for Catalan words [@boada2020subtlex]. We extracted or transformed scores as/into Zipf scores [@van2014subtlex] to so correct their logarithmic distribution, limiting their range roughly between 0 to 7, allowing an easier interpretation of further analyses [@van2014subtlex]. 

We retrieved PTHN scores from the CLEARPOND database [@marian2012clearpond]. PTHN scores indicate the number of phonological neighbours of the target word with higher lexical frequency, as indicated by its score in the corresponding SUBTLEX database. CLEARPOND defines a phonological neighbour as a word whose phonological transcription in International Phonetic Alphabet (IPA) format (generated from eSPEAK, [http://espeak.sourceforge.net/](http://espeak.sourceforge.net/)) differs from that of the target word in only one addition, deletion, or substitution. PTHN scores in CLEARPOND measure have been calculated using corpora of similar size across language, allowing reliable cross-language comparisons. 

We measured the phonological similarity between translation pairs by computing the Levenshtein similarity between their IPA translations using the `stringsim` function of the stringdist R package [@van2014stringdist]. This function computes the inverse of the Levenshtein distance between two character strings as a proportion. First, it computes the edit distance between two character strings (in this case, phoneme symbols) by counting the number of additions, deletions, and substitutions necessary to make both strings identical [@levenshtein1966binary]. This measure is then divided by the maximum distance (according to the length of the longest string) and then subtracted from 1. The result is a score that ranges from 0 to 1, where 0 indicates no similarity between the two strings and one indicates that both strings are identical. We computed this similarity measure (Levenshtein, from now on) for every translation pair in our stimuli lists. Table 1 summarises the lexical frequency, phonological neighbourhood density and phonological overlap of the words included in the Catalan and the Spanish lists.

Participants listened to one audio file in each trial. This audio file corresponded to a word in Catalan (for Spanish speakers, and for English participants allocated in the Catalan condition) or Spanish (for English speakers allocated in the Spanish condition). The audio files were the same ones used in child experiments conducted in the Laboratori de Recerca en Infància of Universitat Pompeu Fabra (Barcelona, Spain). These audio files were recorded by a proficient Catalan-Spanish female bilingual from the Metropolitan Area of Barcelona in a child-directed manner. Catalan and Spanish words were recorded at 44,100 Hz in separate files in the same session, and then de-noised using Audacity and normalised at peak intensity using Praat [@broersma2021praat]. The average duration of the audios was `r printnum(mean(stimuli$duration, na.rm = TRUE))` (*SD* = `r printnum(sd(stimuli$duration, na.rm = TRUE))`, Range = `r printnum(min(stimuli$duration, na.rm = TRUE))`-`r printnum(max(stimuli$duration, na.rm = TRUE))`). The average duration of the Catalan audios was `r printnum(mean(stimuli$duration[stimuli$group %in% c("ENG-CAT")], na.rm = TRUE))` seconds (*SD* = `r printnum(sd(stimuli$duration[stimuli$group %in% c("ENG-CAT")], na.rm = TRUE))`, Range = `r printnum(min(stimuli$duration[stimuli$group %in% c("ENG-CAT")], na.rm = TRUE))`-`r printnum(max(stimuli$duration[stimuli$group %in% c("ENG-CAT")], na.rm = TRUE))`), and the average duration of the Spanish audios was `r printnum(mean(stimuli$duration[stimuli$group %in% c("ENG-SPA")], na.rm = TRUE))` seconds (*SD* = `r printnum(sd(stimuli$duration[stimuli$group %in% c("ENG-SPA")], na.rm = TRUE))`, Range = `r printnum(min(stimuli$duration[stimuli$group %in% c("ENG-SPA")], na.rm = TRUE))`-`r printnum(max(stimuli$duration[stimuli$group %in% c("ENG-SPA")], na.rm = TRUE))`). 

```{r stimulitable, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Stimuli data. Mean and standard deviation (SD) of lexical frequencies od target words (expressed in counts per million and as Zipf scores), density of higher-frequency phonological neighbourhoods of target words, and normalised Levenshtein phonological similarity between the presented and the target words."}

stimuli_table <- stimuli %>% 
  select(group, trial_id, frequency, frequency_zipf, pthn, lv) %>% 
  group_by(group) %>% 
  summarise_at(
    vars(frequency:lv), 
    list(
      mean = ~mean(., na.rm = TRUE),  
      sd = ~sd(., na.rm = TRUE)
    )
  ) %>%
  gt(rowname_col = "group") %>% 
  fmt_number(frequency_mean:lv_sd) %>% 
  tab_spanner(md("**Freq./million**"), c("frequency_mean", "frequency_sd")) %>% 
  tab_spanner(md("**Freq. (Zipf)**"), matches("frequency_zipf")) %>% 
  tab_spanner(md("***PTHN***"), matches("pthn")) %>% 
  tab_spanner(md("**Levenshtein**"), matches("lv")) %>% 
  cols_label(
    frequency_mean = md("Mean"),
    frequency_sd = md("*SD*"),
    frequency_zipf_mean = md("Mean"),
    frequency_zipf_sd = md("*SD*"),
    pthn_mean = md("Mean"),
    pthn_sd = md("*SD*"),
    lv_mean = md("Mean"),
    lv_sd = md("*SD*")
  ) %>% 
  as_latex()

stimuli_table

```

## Data analysis

We modelled the probability of participants guessing the correct translation of each input word using a generalised multilevel Bayesian regression model with a Bernoulli logit link distribution. We first fitted a base model (Model 0) that only included the lexical frequency (`frequency`) of the target word as a fixed effect, with a random intercept per participant. Second, we extended the model to include `pthn` as a fixed effect, with a random slope by participant (Model 1). Third, we added the fixed effect `consonant_ratio` and the `pthn:consonant_ratio`, and random slopes for both effects by participant (Model 2). Third, we added the fixed effect `vowel_ratio` and the `pthn:vowel_ratio`, and random slopes for both effects by participant (Model 3). Finally, we fit a model that included the two-way interactions `pthn:consonant_ratio` and `pthn:vowel_ratio`, an their random slopes by participant (Model 4). All predictor variables were standardised (transformed in standard deviations from the mean) before entering the model. Model 4 can be formally expressed as:



To test and account for cross-group differences, we included a random intercept for each group. We compared models using leave-one-out cross-validation (*LOO*) [@vehtari2017practical]. More information about the models and model comparison can be found in Appendix 2. All analyses were performed in R environment [@rcore2019r]. We used the tidyverse family of R packages to process data and to generate figures, and the brms R package [@burkner2017brms] using the cmdstanr backend to the Stan probabilistic language [@carpenter2017stan] to estimate and compare the models (see Appendix 1 for mode details on the models).


# Results

All models showed good out-of-sample predictive validity, as suggested by the fact that the expected log-probability density was many times larger than its associated standard error. Model 4, which included all main effects and the two-way interactions between PTHN and vowel similarity, and PTHN and consonant similarity, showed the best performance (see Table 2). 




```{r modelvalues, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

coefs <- fixef(model_fits$fit_3) %>% 
  as.data.frame() %>% 
  rownames_to_column("variable") %>% 
  clean_names() %>% 
  mutate_at(vars(estimate:q97_5), ~ifelse(variable=="Intercept", inv_logit_scaled(.), ./4)) %>% 
  group_split(variable) %>% 
  set_names(make_clean_names(map(., "variable"))) %>% 
  map(select, -variable) %>% 
  map(unlist)

```




```{r comparison, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.width=4, fig.cap="Model comparison. Outcomes of the leave-one-out cross-validation procedure. Each row indicates the values of the expected log-predicted density (ELPD), leave-one-out information creterion (LOO-IC), and the difference in ELPD (LOO-diff) between each model and the inmediately simpler model. Each value is accompanied by a standard error (SE) indicating the uncertainty of its estimate."}

loos_table <- model_loos %>% 
  as.data.frame() %>% 
  rownames_to_column("model") %>% 
  arrange(desc(elpd_loo)) %>% 
  select(model, matches("elpd_loo"), matches("looic"), matches("diff")) %>%
  mutate(model = str_replace(model, "fit_", "Model ")) %>% 
  mutate_all(~ifelse(. == 0, NA, .)) %>% 
  gt(rowname_col = "model") %>% 
  fmt_missing(everything(), missing_text = "-") %>% 
  fmt_number(2:7) %>% 
  cols_label(
    model = md("**Model**"),
    looic = md("***LOO_{IC}***"),
    se_looic = md("***SE*<sub>IC</sub>**"),
    elpd_loo = md("***LOO<sub>ELPD</sub>***"),
    se_elpd_loo = md("***SE***"),
    elpd_diff = md("***LOO<sub>diff</sub>***"),
    se_diff = md("***SE<sub>diff</sub>***")
  ) %>% 
  as_latex()

loos_table

```

We now report the mean of the posterior distribution of each coefficient in Model 3, along with its associated measures of uncertainty. For interpretability, we transformed the estimates of the intercept using the inverse logit function so that the values are expressed in probability of correct response instead of log-odds, and we transformed the coefficients of the rest of predictors divided by four. Dividing a coefficient expressed in log-odds by four returns an approximate of the derivative of the logistic function indicating the maximum steepness of the logistic curve. This way, the coefficients are expressed as increases/decreases in probability of correct translation [@gelman2020regression].

```{r posteriorfix, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Estimated posterior distributions of coefficients in Model 3. A\\) Population-level effects. Distributions indicate the estimated posterior likelihood density of each coefficient. Credible intervals \\(CrI\\), represented with increasingly lighter segmentents in the distribution indicate the range of values that contain the true value with 95\\%, 80\\%, and 50\\% probability. Dots represent the mean of the distribution. B\\) Participant\\-level coefficient variability. Our model estimated participant\\-level coefficients to account for the dependency between responses from the same participant. Distributions in this panel indicate the estimated variability across coefficients from different participants, expressed as standard deviations \\(SD\\). C\\) Correlation between participant\\-level effects. Our model allowed participant-level coefficients to co\\-vary. This panel represents the Pearson correlations between each pair of coefficients, expressed as the mean of the posterior distribution of each correlation. Coefficients are represented in the X\\-axis and Y\\-axis in the same order as indicated in the Y\\-axis of panels A and C.", fig.height=5, fig.width=6}


str_repl <- c(
  "Intercept" = "Intercept",
  "frequency_zipf" = "Frequency (+1 SD)",
  "pthn:lv" = "PTHN \u00d7 Levenshtein", 
  "lv" = "Levenshtein (+1 SD)",
  "pthn" = "PTHN (+1 SD)"
)


# fixed effects
post_fix <- posterior_draws_fixed %>% 
  filter(str_detect(.variable, "b_")) %>% 
  mutate(
    .variable_name = str_remove(.variable, "b_") %>% 
      str_replace_all(str_repl) %>% 
      factor(levels = c("Intercept", "Frequency (+1 SD)", "PTHN (+1 SD)", "Levenshtein (+1 SD)", "PTHN \u00d7 Levenshtein")),
    .value = ifelse(str_detect(.variable, "Intercept"), inv_logit_scaled(.value), .value/4)
  ) %>% 
  arrange(.variable) %>% 
  ggplot(aes(.value, fct_rev(.variable_name))) +
  geom_vline(xintercept = 0) +
  stat_slab(aes(fill = stat(cut_cdf_qi(cdf, .width = c(.5, .8, .95), labels = percent_format())))) +
  stat_pointinterval(.width = 0, point_size = 1) +
  scale_fill_manual(values = c("#1A85FF", "#9ccaff", "#d2e5fc"), na.translate = FALSE) +
  scale_x_continuous(labels = function(x) percent(round(x, 1))) +
  labs(x = "P(Correct)", y = "Posterior probability density", fill = "CrI") +
  theme(
    legend.position = c(1, 0.1),
    axis.title.y = element_blank(),
    axis.text.x = element_text(colour = "black"),
    axis.text.y = element_text(colour = "black", vjust = 0),
    panel.grid.major.y = element_line(colour = "grey", size = 0.5)
  ) 

# SD of random effects
post_sd <- posterior_draws_fixed %>% 
  filter(str_detect(.variable, "sd_")) %>% 
  mutate(
    .variable_name = str_remove(.variable, "sd_participant__") %>% 
      str_replace_all(str_repl) %>% 
      factor(levels = c("Intercept", "Frequency (+1 SD)", "PTHN (+1 SD)", "Levenshtein (+1 SD)", "PTHN \u00d7 Levenshtein"))
  ) %>%   
  ggplot(aes(.value, fct_rev(.variable_name))) +
  stat_slab(aes(fill = stat(cut_cdf_qi(cdf, .width = c(.5, .8, .95), labels = percent_format())))) +
  stat_pointinterval(.width = 0, point_size = 1) +
  scale_fill_manual(values = c("#ff2976", "#f2a7c2", "#fcd9e6"), na.translate = FALSE) +
  scale_x_continuous(labels = percent) +
  labs(x = "SDs in P(Correct)", y = "Posterior probability density", fill = "CrI") +
  theme(
    legend.position = c(1, 0.1),
    axis.title.y = element_blank(),
    axis.text.x = element_text(colour = "black"),
    axis.text.y = element_text(colour = "black", vjust = 0),
    panel.grid.major.y = element_line(colour = "grey", size = 0.5)
  ) 


# Pearson correlations between random effects
corr_mat <- cov2cor(vcov(model_fits$fit_3)) 
corr_mat[lower.tri(corr_mat)] <- NA

post_cors <- corr_mat %>% 
  as.data.frame() %>% 
  rownames_to_column("term1") %>% 
  pivot_longer(-term1, names_to = "term2", values_to = ".value") %>% 
  drop_na(.value) %>% 
  mutate(term1 = factor(term1, levels = names(str_repl), ordered = TRUE),
         term2 = factor(term2, levels = names(str_repl), ordered = TRUE)) %>% 
  mutate_at(vars(term1, term2), str_replace_all, str_repl) %>% 
  mutate(
    .value_label = case_when(
      term1==term2 ~ NA_character_,
      TRUE ~ printnum(.value, gt1 = FALSE)
    ),
    .value = case_when(
      term1==term2 ~ NA_real_,
      TRUE ~ .value
    )
  ) %>% 
  ggplot(aes(fct_inorder(term1), fct_rev(fct_inorder(term2)), fill = .value)) +
  geom_tile(na.rm = TRUE) +
  geom_text(aes(label = .value_label), size = 3, na.rm = TRUE) +
  labs(x = "Term 1", y = "Term 2", fill = "Posterior correlation") +
  scale_fill_gradient(low = "#FFC20A", high = "white", na.value = "white") +
  coord_equal() +
  theme(
    legend.position = "none",
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_blank()
  )

post_fix / (post_sd + post_cors + plot_layout(widths = c(0.6, 0.4))) +
  plot_layout(guides = "keep") & 
  plot_annotation(tag_levels = "A")



```

Overall, participants were `r percent(coefs$intercept["estimate"], accuracy = 0.01)`, (*SE* = `r percent(coefs$intercept["est_error"], accuracy = 0.01)`, 95% *CrI* = [`r percent(coefs$intercept["q2_5"], accuracy = 0.01)`, `r percent(coefs$intercept["q97_5"], accuracy = 0.01)`]) likely to produce correct translations. Every standard deviation increment in the translation's lexical frequency (*SD* = `r printnum(sd(stimuli$frequency_zipf, na.rm = TRUE))`) increased the probability of a correct responses in `r percent(coefs$frequency_zipf["estimate"], accuracy = 0.01)` (*SE* = `r percent(coefs$frequency_zipf["est_error"], accuracy = 0.01)`, 95% *CrI* = [`r percent(coefs$frequency_zipf["q2_5"], accuracy = 0.01)`, `r percent(coefs$frequency_zipf["q97_5"], accuracy = 0.01)`]). The number of the translation's more frequent phonological neighbours, on the other hand, decreased the probability of a correct responses in `r percent(coefs$pthn["estimate"], accuracy = 0.01)` (*SE* = `r percent(coefs$pthn["est_error"], accuracy = 0.01)`, 95% *CrI* = [`r percent(coefs$pthn["q2_5"], accuracy = 0.01)`, `r percent(coefs$pthn["q97_5"], accuracy = 0.01)`]) for every increase in 1 *SD* (`r printnum(sd(stimuli$pthn, na.rm = TRUE))`). The effect of phonological similarity was conditional to the phonological density of the translation. Phonological similarity barely increased the probability of a correct responses (`r percent(coefs$lv["estimate"], accuracy = 0.01)`, *SE* = `r percent(coefs$lv["est_error"])`, 95% *CrI* = [`r percent(coefs$lv["q2_5"], accuracy = 0.01)`, `r percent(coefs$lv["q97_5"])`]) by itself. However, for every *SD* increase in PTHN, phonological similarity increased in `r percent(coefs$pthn_lv["estimate"])`, (*SE* = `r percent(coefs$pthn_lv["est_error"], accuracy = 0.01)`, 95% *CrI* = [`r percent(coefs$pthn_lv["q2_5"], accuracy = 0.01)`, `r percent(coefs$pthn_lv["q97_5"], accuracy = 0.01)`]) such probability.


```{r marginaleffects, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.width=6, fig.height=3, fig.cap="Expected mean posterior predictions. A) Population-level expected mean posterior predictions. The X-axis and the Y-axis represent the Levenshtein distance (in standard deviations from the mean) and the probability of correct translation, respectively. We simulated 200 observations from our model: 100 simulations for words with low PTHN (-1 SD) and 100 simulations for high PTHN (+1 SD) words. We did this across the range of values of the Levenshtein scores. For each simulation, we drew a single sample from the posterior distribution of each coefficient. Each simulation is depicted in the graph as a line: pink in the case of low PTHN words, and blue in the case of high PTHN words. We also plotted the mean of the high PTHN (black solid line) and low PTHN (black dashed line) simulations to indicate the expected mean value of the posterior predictions of the model. The dispersion of the lines indicates the uncertainty of our predictions. B) Participant-level expected mean posterior predictions. We conducted the same procedure for each individual participant, simulating 100 observations for high-PTHN and 100 for low-PTHN from our model acrosss the range of Levenshtein scores. We averaged the resulting predictions for each participant and then plotted each participant's predictions in separate panels to show how the effect of PTHN and phonological similarity changed at the individual level."}

# by vowel_ratio
m <- posterior_epreds_fixed %>% 
  mutate_at(vars(pthn, frequency_zipf), function(x) paste0(x, " SD")) %>% 
  mutate(
    pthn = case_when(
      pthn=="-1 SD" ~ "Low PTHN",
      pthn=="0 SD" ~ "Mean PTHN",
      pthn=="1 SD" ~ "High PTHN"
    ),
    frequency_zipf = case_when(
      frequency_zipf=="-1 SD" ~ "Low frequency",
      frequency_zipf=="0 SD" ~ "Mean frequency",
      frequency_zipf=="1 SD" ~ "High frequency"
    )
  ) 

m_re <- posterior_epreds_random %>% 
  mutate_at(vars(pthn, frequency_zipf), function(x) paste0(x, " SD")) %>% 
  mutate(
    pthn = case_when(
      pthn=="-1 SD" ~ "Low PTHN",
      pthn=="0 SD" ~ "Mean PTHN",
      pthn=="1 SD" ~ "High PTHN"
    ),
    frequency_zipf = case_when(
      frequency_zipf=="-1 SD" ~ "Low frequency",
      frequency_zipf=="0 SD" ~ "Mean frequency",
      frequency_zipf=="1 SD" ~ "High frequency"
    )
  ) %>% 
  group_by(participant, lv, pthn) %>% 
  summarise(.epred = mean(.epred, na.rm = TRUE), .groups = "drop")

plot_m <- ggplot(m, aes(lv, .epred, colour = pthn, fill = pthn)) +
  # geom_jitter(data = filter(nd_obs, correct), aes(y = 1), alpha = 0.1,
  #             size = 0.25, height = 0.05, width = 0.1, show.legend = FALSE) +
  # geom_jitter(data = filter(nd_obs, !correct), aes(y = 0), alpha = 0.25, 
  #             size = 0.25, height = 0.05, width = 0.1, show.legend = FALSE) +
  geom_hline(yintercept = 0.5, colour = "grey") +
  geom_line(aes(group = interaction(pthn, .draw)), alpha = 0.25, size = 0.25, show.legend = FALSE) +
  stat_summary(aes(linetype = pthn), fun = mean, geom = "line", colour = "black", size = 0.5, show.legend = FALSE) +
  labs(x = "Levenshtein (phonological similarity)", y = "P(Correct)", 
       colour = "PTHN", fill = "PTHN", linetype = "PTHN",
       title = "Population-level predictions") +
  scale_color_manual(values = c("#1E88E5", "#ff2976")) +
  scale_y_continuous(labels = function(x) percent(round(x, 2)), limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  guides(colour = guide_legend(ncol = 2)) 

plot_m_re <- ggplot(m_re, aes(lv, .epred, colour = pthn, fill = pthn)) +
  facet_wrap(~participant) +
  geom_line(aes(group = interaction(pthn, participant)), size = 0.35) +
  labs(x = "Levenshtein (phonological similarity)", y = "P(Correct)", 
       colour = "PTHN", fill = "PTHN", linetype = "PTHN",
       title = "Participant-level predictions") +
  scale_color_manual(values = c("#1E88E5", "#ff2976")) +
  scale_y_continuous(labels = function(x) percent(round(x, 2)), limits = c(0, 1), breaks = seq(0, 1, 0.1)) +
  guides(colour = guide_legend(ncol = 2)) +
  theme(
    legend.position = "none",
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    axis.ticks = element_blank(),
    strip.text = element_blank(),
    strip.background = element_blank(),
    panel.spacing = unit(0.1, "lines")
  ) 

plot_m + plot_m_re +
  plot_layout(guides = "collect") &
  plot_annotation(tag_levels = "A") &
  theme(
    legend.position = "top",
    legend.title = element_blank(),
  )

```




# Discussion


\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup

\newpage

# Appendix


```{r accuracy, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Proportion of correct translation by item. Words presented to participants in the English\\-Spanish or in the English\\-Catalan and Spanish\\-Catalan are listed in the Y\\-axis, ordered from higher to lower average translation accuracy, depicted in the X\\-axis. Dots and whiskers represent the average accuracy and 95\\% confidence interval of each word. Accuracy is plotted separately for each group.", fig.height=10, fig.width=9}

responses_item <- responses %>%
  group_by(trial_id, group, word) %>%
  summarise(
    correct_sum = sum(correct, na.rm = TRUE),
    correct_n = sum(!is.na(correct), na.rm = TRUE),
    .groups = "drop"
  ) %>% 
  left_join(select(stimuli, group, trial_id, word2, ipa1, ipa2)) %>% 
  mutate(
    word = paste0(word, " (", ipa1, ") / ", word2, " (", ipa2, ")"),
    correct_prop = prop_adj(correct_sum, correct_n),
    correct_se = prop_adj_se(correct_sum, correct_n),
    word_label = reorder_within(word, by = correct_prop, within = group)
  ) 


ggplot(responses_item, aes(reorder(word_label, correct_prop), correct_prop, ymin = correct_prop-correct_se, ymax = correct_prop+correct_se, colour = group)) +
  facet_wrap(~group, scales = "free_y") +
  geom_hline(yintercept = 0.5, colour = "black") +
  geom_errorbar(width = 0) +
  geom_point(size = 0.5) +
  labs(y = "Proportion of correct responses", x = "Word", colour = "Group") +
  coord_flip() +
  scale_color_manual(values = c("#1E88E5", "#ff2976", "#FFC20A")) +
  scale_y_continuous(limits = c(0, 1), labels = scales::percent) +
  scale_x_discrete(labels = function(x) str_remove(x, "___ENG-CAT|___ENG-SPA|___SPA-CAT")) +
  theme(
    legend.position = "none",
    axis.text.y = element_text(size = 5, colour = "black", hjust = 1),
    axis.ticks = element_blank(),
    axis.text.x = element_text(size = 7, colour = "black"),
  )

```



