---
title: "Translation Elicitation"
output:
  md_document:
    variant: markdown_github
  html_document:
    df_print: paged
    toc: true
    toc_depth: 4
    toc_float: true
bibliography: "references.bib"
---

```{r, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = TRUE,
  warning = TRUE, 
  cache.extra = knitr::rand_seed,
  out.width = "100%",
  results = "asis",
  dev.args = list(png = list(type = "cairo"))
)
options(
  knitr.kable.NA = '-',
  knitr.duplicate.label = "allow",
  ggplot2.discrete.fill = wesanderson::wes_palettes$Darjeeling1,
  ggplot2.discrete.colour = wesanderson::wes_palettes$Darjeeling1,
  ggplot2.continuous.fill = wesanderson::wes_palettes$Darjeeling1,
  ggplot2.continuous.colour = wesanderson::wes_palettes$Darjeeling1
)

```


```{r prepare, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}

# load packages
library(tidyverse)
library(gt)
library(lubridate)
library(wesanderson)
library(ggdist)
library(readxl)
library(tidytext)
library(brms)
library(tidybayes)
library(patchwork)
library(janitor)
library(mice)
library(here)

# import data
tar_load_globals()
tar_load(stimuli)
tar_load(participants)
tar_load(responses)
tar_load(model_fits)
tar_load(model_loos)

theme_set(theme_github())

```


# Experimental task

## Methods

### Participants {.tabset .tab-pills}

```{r participants_coded}
participants_coded <- participants %>% 
  mutate(valid_participant = ifelse(valid_participant, "Valid", "Not valid"))
```

We collected data from `r format(as_date(min(participants$date)), "%B %dth %Y")` to `r format(as_date(max(participants$date)), "%B %dth %Y")`. We tested `r length(unique(participants$participant))` participants. `r length(unique(participants$participant[participants$group=="ENG-SPA"]))` were English natives tested in Spanish, `r length(unique(participants$participant[participants$group=="ENG-CAT"]))` were English natives tested in Catalan, and `r length(unique(participants$participant[participants$group=="SPA-CAT"]))` were Spanish natives tested in Catalan. `r sum(!participants$valid_participant)` failed to meet all the inclusion criteria and were excluded from further analyses. The final sample comprised data from `r sum(!participants$valid_participant)` participants, `r sum(participants$valid_participant[participants$group=="ENG-SPA"])` in the ENG-SPA group, `r sum(participants$valid_participant[participants$group=="ENG-CAT"])` in the ENG-CAT group, and `r sum(participants$valid_participant[participants$group=="SPA-CAT"])` in the SPA-CAT group.

Participants were included if^[We originally planned to exclude participants that reported any visual impairment that glasses would not correct. This item was phrased as Do you have normal or corrected-to-normal VISION? (Yes/No) in English, and as ¿Tienes problemas de VISIÓN que unas gafas o lentes de contacto NO corrijan? (Sí/No). Surprisingly, the proportion of Spanish participants that reported visual impairment was implausibly large (n = 6, 18.18%). This is possibly due to some participants using glasses daily and not having read the item until the end, where it is indicated that the use of glasses is considered as normal vision]:

* Aged 18 to 25 years
* Did not report being fluent in Catalan, Spanish, or Italian
* Provided at least 80% of the presented trials
* Did not report motor, auditory or visual (other than wearing glasses) problems

Trials were considered valid if:

* Participant did not leave a blank response
* Participant took longer than 10 seconds to respond

#### Summary

```{r participants_summary, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

ggplot(participants_coded, aes(group, fill = valid_participant)) +
  geom_bar() +
  labs(x = "Group", y = "# participants", fill = "Valid?") +
  theme(
    legend.title = element_blank(),
    legend.position = "top",
    axis.title.x = element_blank(),
    panel.grid.major.y = element_line(linetype = "dotted", colour = "grey")
  )

```

#### Valid trials

```{r participants_stimuli, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

participants_coded %>% 
  filter(valid_participant=="Valid") %>% 
  select(participant, group, n, n_valid) %>%
  mutate(valid_prop = n_valid/n) %>% 
  ggplot(aes(group, valid_prop, fill = group, colour = group)) +
  geom_violin() +
  geom_point(shape = 1, stroke = 1, alpha = 0.5, position = position_jitter(width = 0.1),
             colour = "black") +
  geom_boxplot(colour = "black", fill = "white", width = 0.05, outlier.color = NA) +
  labs(x = "Group", y = "Valid trials", fill = "Group", colour = "Group") +
  scale_y_continuous(limits = c(0.8, 1), labels = scales::percent) +
  theme(
    legend.title = element_blank(),
    legend.position = "top",
    axis.title.x = element_blank(),
    panel.grid.major.y = element_line(linetype = "dotted", colour = "grey")
  )
```

#### Demographic information

```{r participants_demo, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
participants_valid <- filter(participants_coded, valid_participant=="Valid")
p_sex <- ggplot(participants_valid, aes(group, fill = sex)) +
  geom_bar() +
  labs(x = "Group", y = "# participants", fill = "Sex") +
  theme(axis.title.x = element_blank())

p_age <- ggplot(participants_valid, aes(as.factor(age), fill = group, colour = group)) +
  geom_bar() +
  labs(x = "Age (years)", y = "# participants", fill = "Group", colour = "Group")

(p_sex + p_age) &
  plot_layout() &
  plot_annotation() &
  theme(
    legend.position = "top",
    legend.title = element_blank(),
    panel.grid.major.y = element_line(linetype = "dotted", colour = "grey")
  )

```

#### Second language

```{r participants_l2, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

participants_valid %>% 
  count(group, l2) %>% 
  mutate(l2 = reorder_within(l2, n, within = group)) %>% 
  ggplot(aes(x = reorder(l2, -n), n,  fill = l2)) +
  facet_wrap(~group, scales = "free_x") +
  geom_col() +
  labs(x = "L2", y = "# participants", fill = "L2") +
  scale_x_reordered() +
  theme(
    legend.position = "none",
    panel.grid.major.y = element_line(colour = "grey", linetype = "dotted"),
    axis.text.x = element_text(angle = 90)
  )

```


#### Language profile proficiency

```{r participants_proficiency, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

participants_valid %>% 
  select(participant, group, catalan_oral, spanish_oral, catalan_written, spanish_written) %>% 
  mutate_at(vars(catalan_oral, spanish_oral, catalan_written, spanish_written), replace_na, 0) %>% 
  pivot_longer(c(catalan_oral, spanish_oral, catalan_written, spanish_written),
               names_to = "type", values_to = "score") %>% 
  separate(type, c("language", "modality"), sep = "_") %>% 
  mutate_at(vars(language, modality), str_to_sentence) %>%
  ggplot(aes(as.integer(score), fill = language)) +
  facet_grid(modality~group) +
  geom_bar() +
  labs(y = "# participants", x = "Self-reportd proficiency (0-5)", fill = "Language") +
  scale_x_continuous(limits = c(0, 5)) +
  coord_flip() +
  theme(
    panel.grid.major.y = element_line(colour = "grey", linetype = "dotted")
  )

```

#### {-}


### Stimuli {.tabset .tab-pill}

Participants in the ENG-SPA listened to `r nrow(stimuli[stimuli$group=="ENG-SPA",])` Spanish words, and participants in the ENG-CAT and the SPA-CAT groups listened to `r nrow(stimuli[stimuli$group=="ENG-CAT",])` trials. Five trials in each condition were practice trials and were excluded from analyses.

```{r stimuli_coded, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE,  include=FALSE}
stimuli_coded <- stimuli %>% 
  mutate(
    test_language = ifelse(group=="ENG-SPA", "Spanish", "Catalan"),
    zipf = log10(frequency)+3,
    pthn_log = log(pthn),
    lv_log = log(lv)
  )
```

#### Lexical frequency

```{r stimuli_frequency, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
stimuli_coded %>% 
  ggplot(aes(zipf, fill = test_language)) +
  geom_histogram(bins = 25) +
  labs(x = "Lexical frequency (Zipf score)\nExtracted from CLEARPOND database", y = "# trials", fill = "Test language") +
  theme(
    panel.grid.major.y = element_line(colour = "grey", linetype = "dotted")
  )

```

#### PTHN

```{r stimuli_pthn, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
stimuli_coded %>% 
  count(test_language, pthn) %>% 
  ggplot(aes(reorder(as.factor(pthn), -n), n, fill = test_language)) +
  geom_col(position = position_dodge()) +
  labs(x = "# Phonological neighbours with higher lexical frequency\nExtracted from CLEARPOND database", y = "# trials", fill = "Test language") +
  theme(
    legend.position = "top",
    panel.grid.major.y = element_line(colour = "grey", linetype = "dotted")
  )

```

#### Phonological similarity

```{r stimuli_similarity, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
stimuli_coded %>% 
  ggplot(aes(group, lv, fill = group, colour = group)) +
  geom_violin() +
  geom_point(shape = 1, stroke = 1, position = position_jitter(width = 0.1), colour = "black", alpha = 0.25) +
  geom_boxplot(colour = "black", fill = "white", width = 0.1, size = 0.75) +
  labs(x = "Group", y = "Phonological Leveshtein distance\n(corrected for string length)", fill = "Test language") +
  scale_y_continuous(limits = c(0, 1), labels = scales::percent) +
  theme(
    legend.position = "none",
    axis.title.x = element_blank(),
    panel.grid.major.y = element_line(colour = "grey", linetype = "dotted")
  ) +
  
  stimuli_coded %>% 
  select(trial_id, group, ends_with("_ratio")) %>% 
  pivot_longer(ends_with("_ratio"), names_to = "type", values_to = "ratio") %>% 
  mutate(type = str_to_sentence(str_remove(type, "_ratio"))) %>% 
  ggplot(aes(group, ratio, fill = group, colour = group)) +
  facet_wrap(~type, ncol = 1) +
  geom_violin() +
  geom_point(shape = 1, stroke = 1, position = position_jitter(width = 0.1), colour = "black", alpha = 0.25) +
  geom_boxplot(colour = "black", fill = "white", width = 0.1, size = 0.75) +
  labs(x = "Grouop", y = "Similarity", fill = "Test language") +
  scale_y_continuous(limits = c(0, 1), labels = scales::percent) +
  theme(
    legend.position = "none",
    axis.title.y = element_blank(),
    axis.title.x = element_blank(),
    panel.grid.major.y = element_line(colour = "grey", linetype = "dotted")
  ) +
  
  plot_layout(ncol = 2)

```

#### Phonological onset

```{r stimuli_onset, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
stimuli_coded %>% 
  select(trial_id, group, onset) %>% 
  ggplot(aes(group, fill = onset, colour = onset)) +
  geom_bar(position = position_dodge(width = 0.9)) +
  labs(x = "Group", y = "# trials", fill = "Phonological onset", colour = "Phonological onset") +
  theme(
    legend.position = "top",
    panel.grid.major.y = element_line(colour = "grey", linetype = "dotted")
  )
```

#### Phonological similarity and onset

```{r stimuli_similarity_onset, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
stimuli_coded %>% 
  select(trial_id, group, lv, onset) %>% 
  ggplot(aes(lv, fill = onset, colour = onset)) +
  facet_wrap(~group, ncol = 2) +
  geom_histogram() +
  labs(x = "Phonological Leveshtein distance\n(corrected for string length)", y = "# trials",
       fill = "Phonological onset", colour = "Phonological onset") +
  theme(
    legend.position = c(0.75, 0.25),
    panel.grid.major.y = element_line(colour = "grey", linetype = "dotted")
  )
```

#### {-}

### Design

### Data analysis {.tabset .tab-pills}

#### Model details

We fitted a Bayesian multilevel regression model using the R package `brms` [@burkner2017brms], with correct responses (`correct`, 0 is incorrect response, 1 if correct response) as the response variable with a Bernoulli distribution and a logit link function. We modelled the probability of an average participant providing a correct response, conditional to a set of predictors (aka. fixed effects) and their interactions:

* PTHN (`pthn`, 0-Inf): number of phonological neighbours with higher frequency of the target word. Extracted from the CLEARPOND database. Standardised before entering the regression model.
* Phonological similarity (`lv`, 0-1): Inverse Levenshtein distance between the IPA transcription of the presented word and its correct translation in the target language, calculated using the `stringdist` function of the `stringdist` package [@loo2014stringdist]. Standardised before entering the regression model.
* Shared phonological onset (`onset`, Same/Different): Whether the presented and the target word share phonological onset, as judged by experimenters, and sum coded as $Different = -0.5$ and $Same = +0.5$. 

We added participants (`participants`) and as a grouping variable (aka., random effects), therefore adding participant-level adjustments to the model. We specified random intercepts slopes for all fixed effects within the `participant` grouping variable.

We performed multiple imputation via predictive mean matching using the `mice` function of the `mice` package [@van2011mice] to impute missing values in the response variable or the predictors. 

The model was implemented using the following formula (`fit_3`):

```
~ pthn*lv*onset + (1 + pthn*lv*onset | participant)
```

The estimation of the model was performed using Bayesian inference via Hamiltonian Monte-Carlo in Stan [@carpenter2017stan], with 4 chains, 2,000 iterations each (1,000 warmups). The output of this model is the approximated posterior distribution of all the parameters estimated, therefore indicating the probability of each combination of values of the parameters, conditional to the observed data. 

We compared this (extended model) to other models dropping one predictor at a time using Leave-one-out cross-validation (LOO-CV), using the `loo` and `loo_compare` functions of the `brms` package. We compared our extended model against the following models:

* fit_0: `~ 1 + frequency + (1  | participant)`
* fit_1: `~ 1 + frequency + pthn + (1 + pthn | participant)`
* fit_2: `~ 1 + frequency + pthn*lv + (1 + pthn*lv | participant)`

#### Stan code

Stan code generated by `brms`:

```{stan stan_model, echo=TRUE, output.var="model"}
// generated with brms 2.15.0
functions {
/* compute correlated group-level effects
* Args: 
*   z: matrix of unscaled group-level effects
*   SD: vector of standard deviation parameters
*   L: cholesky factor correlation matrix
* Returns: 
*   matrix of scaled group-level effects
*/ 
matrix scale_r_cor(matrix z, vector SD, matrix L) {
// r is stored in another dimension order than z
return transpose(diag_pre_multiply(SD, L) * z);
}
}
data {
int<lower=1> N;  // total number of observations
int Y[N];  // response variable
int<lower=1> K;  // number of population-level effects
matrix[N, K] X;  // population-level design matrix
// data for group-level effects of ID 1
int<lower=1> N_1;  // number of grouping levels
int<lower=1> M_1;  // number of coefficients per level
int<lower=1> J_1[N];  // grouping indicator per observation
// group-level predictor values
vector[N] Z_1_1;
vector[N] Z_1_2;
vector[N] Z_1_3;
vector[N] Z_1_4;
int<lower=1> NC_1;  // number of group-level correlations
int prior_only;  // should the likelihood be ignored?
}
transformed data {
int Kc = K - 1;
matrix[N, Kc] Xc;  // centered version of X without an intercept
vector[Kc] means_X;  // column means of X before centering
for (i in 2:K) {
means_X[i - 1] = mean(X[, i]);
Xc[, i - 1] = X[, i] - means_X[i - 1];
}
}
parameters {
vector[Kc] b;  // population-level effects
real Intercept;  // temporary intercept for centered predictors
vector<lower=0>[M_1] sd_1;  // group-level standard deviations
matrix[M_1, N_1] z_1;  // standardized group-level effects
cholesky_factor_corr[M_1] L_1;  // cholesky factor of correlation matrix
}
transformed parameters {
matrix[N_1, M_1] r_1;  // actual group-level effects
// using vectors speeds up indexing in loops
vector[N_1] r_1_1;
vector[N_1] r_1_2;
vector[N_1] r_1_3;
vector[N_1] r_1_4;
// compute actual group-level effects
r_1 = scale_r_cor(z_1, sd_1, L_1);
r_1_1 = r_1[, 1];
r_1_2 = r_1[, 2];
r_1_3 = r_1[, 3];
r_1_4 = r_1[, 4];
}
model {
// likelihood including constants
if (!prior_only) {
// initialize linear predictor term
vector[N] mu = Intercept + rep_vector(0.0, N);
for (n in 1:N) {
// add more terms to the linear predictor
mu[n] += r_1_1[J_1[n]] * Z_1_1[n] + r_1_2[J_1[n]] * Z_1_2[n] + r_1_3[J_1[n]] * Z_1_3[n] + r_1_4[J_1[n]] * Z_1_4[n];
}
target += bernoulli_logit_glm_lpmf(Y | Xc, mu, b);
}
// priors including constants
target += normal_lpdf(b | 0, 3);
target += normal_lpdf(Intercept | 0, 3);
target += cauchy_lpdf(sd_1 | 0, 3)
- 4 * cauchy_lccdf(0 | 0, 3);
target += std_normal_lpdf(to_vector(z_1));
target += lkj_corr_cholesky_lpdf(L_1 | 5);
}
generated quantities {
// actual population-level intercept
real b_Intercept = Intercept - dot_product(means_X, b);
// compute group-level correlations
corr_matrix[M_1] Cor_1 = multiply_lower_tri_self_transpose(L_1);
vector<lower=-1,upper=1>[NC_1] cor_1;
// extract upper diagonal of correlation matrix
for (k in 1:M_1) {
for (j in 1:(k - 1)) {
cor_1[choose(k - 1, 2) + j] = Cor_1[j, k];
}
}
}
```


#### {-}

## Results {.tabset .tab-pill}

### Model outputs {.tabset .tab-pill}

```{r responses_coded, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
responses_coded <- responses %>% 
  # typos are considered correct responses
  mutate_at(vars(onset, overlap_stress, group), as.factor) %>% 
  # center predictors
  mutate_at(
    vars(lv, consonant_ratio, vowel_ratio, pthn, frequency),
    function(x) scale(x, center = TRUE, scale = TRUE)[,1]) %>% 
  # impute missing data
  mice(m = 5, print = FALSE, method = "pmm") %>% 
  complete() %>% 
  as_tibble() %>% 
  arrange(trial_id, group)
```

#### Fixed coefficients

```{r coefs_fixed_table, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
summary(model_fits$fit_4)$fixed %>% 
  as.data.frame() %>%
  rownames_to_column("term") %>% 
  mutate(
    Estimate = ifelse(term=="Intercept", inv_logit_scaled(Estimate), Estimate/4),
    `l-95% CI` = ifelse(term=="Intercept", inv_logit_scaled(`l-95% CI`), `l-95% CI`/4),
    `u-95% CI` = ifelse(term=="Intercept", inv_logit_scaled(`u-95% CI`), `u-95% CI`/4),
    Est.Error = ifelse(term=="Intercep)", inv_logit_scaled(Est.Error), Est.Error/4)
  ) %>% 
  gt() %>% 
  fmt_percent(2:5) %>% 
  fmt_number(6:6, decimals = 2) %>% 
  fmt_number(7:8, decimals = 0) %>% 
  cols_merge(vars("l-95% CI", "u-95% CI"), pattern = "[{1}, {2}]") %>% 
  cols_label(
    term = md("**Predictor**"),
    Estimate = md("**Mean**"),
    Est.Error = md("**SEM**"),
    "l-95% CI" = md("**95\\% CrI**"),
    Rhat = md("**Rhat**"),
    Bulk_ESS = md("**Bulk ESS**"),
    Tail_ESS = md("**Tail ESS**")
  ) %>% 
  tab_footnote(
    footnote = "Transformed using the inverse logit to get the average probability of correct response",
    locations = cells_body(columns = "term", rows = term=="Intercept")
  ) %>% 
  tab_footnote(
    footnote = "Transformed using the divide-by-four- rule to get the maximum change in probability of correct response, associated with a unit increase in this variable.",
    locations = cells_body(columns = "term", rows = term %in% c("pthn", "lv", "onset1", "pthn:lv", "pthn:onset1", "lv:onset1", "pthn:lv:onset1"))
  ) %>% 
  tab_footnote(
    footnote = "ESS: Effective sample size",
    locations = cells_column_labels(columns = c("Bulk_ESS", "Tail_ESS"))
  ) %>% 
  cols_align(
    align = c("center"),
    columns = 2:4
  ) %>% 
  as_raw_html()
```


```{r post_fix, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Points and error bars in indicate the posterior means, and 50% and 95% CrI. The intercept has been transformed using the inverse logit to get the average probability of correct response. The resto of the coefficients has been transformed using the divide-by-four- rule to get the maximum change in probability of correct response, associated with a unit increase in this variable."}
post <- gather_draws(model_fits$fit_4, `b_.*`, regex = TRUE) %>% 
  mutate(.value = ifelse(.variable=="b_Intercept", inv_logit_scaled(.value), .value/4))

ggplot(post, aes(.value, .variable, fill = .variable)) +
  geom_vline(xintercept = 0) +
  stat_halfeye() +
  labs(x = "Value", y = "Posterior probability density", fill = "Variable") +
  theme(
    legend.position = "none",
    plot.caption.position = "plot",
    plot.caption = element_text(hjust = 0),
    # axis.text = element_text(size = 7),
    panel.grid.major.x = element_line(colour = "grey", linetype = "dotted"),
    panel.grid.major.y = element_line(colour = "grey", linetype = "dotted")
  )
```


#### Model comparison

Leave-one-out cross-validation. The more negative is `elpd_diff` (and the larger its magnitude compared to its corresponding standard error), the better it first the data compared to the null model (`~ 1 + (1 | participant)`). The model that fits the data the best is the extended model.

```{r model_comparison, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
model_loos %>% 
  as.data.frame() %>% 
  rownames_to_column("model") %>% 
  arrange(desc(model)) %>% 
  select(model, matches("elpd_loo"), matches("diff")) %>%
  mutate(model = str_replace(model, "fit_", "Model ")) %>% 
  mutate_all(~ifelse(. == 0, NA, .)) %>% 
  gt(rowname_col = "model") %>% 
  fmt_missing(everything(), missing_text = "-") %>% 
  fmt_number(2:5) %>% 
  cols_label(
    model = md("**Model**"),
    elpd_loo = md("**ELPD**"),
    se_elpd_loo = md("***SE***"),
    elpd_diff = md("**diff**"),
    se_diff = md("***SE* diff**")
  ) %>% 
  as_raw_html()

```

#### Random effects

```{r post_re, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
post_re <- gather_draws(model_fits$fit_4, r_participant[participant, .param]) %>% 
  mean_qi() %>% 
  ungroup() %>% 
  left_join(select(participants, participant, group)) %>% 
  arrange(group, participant, .param)

ggplot(post_re, aes(.value, sort(as.factor(participant)), xmin = .lower, xmax = .upper, colour = group)) +
  facet_grid(group~.param, scales = "free") +
  geom_errorbarh(height = 0.5, alpha = 0.75) +
  geom_point(size = 0.1, shape = 4) +
  geom_vline(xintercept = 0) +
  labs(x = "Participant-level adjustment", y = "Participant", colour = "Group") +
  theme(
    legend.position = "top",
    legend.title = element_blank(),
    panel.grid.major.x = element_line(colour = "grey", linetype = "dotted"),
    axis.text.x = element_text(size = 9),
    axis.text.y = element_blank(),
    axis.ticks = element_blank()
  )
```

#### Marginal effects


```{r emmeans_pthn, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.width=6, fig.height=6}

# by pthn
nd <- expand_grid(
  pthn = seq(min(responses_coded$pthn, na.rm = TRUE),
             max(responses_coded$pthn, na.rm = TRUE), by = 0.1),
  frequency = 0,
  consonant_ratio = c(-1, 1),
  vowel_ratio = c(-1, 1)
)

m <- add_fitted_draws(nd, model_fits$fit_4, n = 50, re_formula = NA) %>% 
  mutate_at(vars(consonant_ratio, vowel_ratio), function(x) paste0(x, " SD"))

ggplot(m, aes(pthn, .value, colour = interaction(consonant_ratio, vowel_ratio, sep = "/"), fill = interaction(consonant_ratio, vowel_ratio, sep = "/"))) +
  stat_lineribbon(.width = 0.95, alpha = 0.25, size = 0) +
  stat_summary(fun = mean, geom = "line", size = 1) +
  labs(x = "Phon. neigh. dens. (SD)", y = "P(Correct)",
       colour = "Consonant/Vowel similarity", fill = "Consonant/Vowel similarity") +
  scale_y_continuous(labels = scales::percent, limits= c(0, 1)) +
  guides(colour = guide_legend(ncol = 2)) +
  theme(
    legend.position = "top",
    panel.grid = element_line(colour = "grey", linetype = "dotted")
  ) 


```


```{r emmeans_consonant, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.width=6, fig.height=6}
# by consonant_ratio
nd <- expand_grid(
  pthn = c(-1, 1),
  frequency = 0,
  consonant_ratio = seq(
    min(responses_coded$consonant_ratio, na.rm = TRUE),
    max(responses_coded$consonant_ratio, na.rm = TRUE),
    by = 0.1
  ),
  vowel_ratio = c(-1, 1)
)

m <- add_fitted_draws(nd, model_fits$fit_4, n = 50, re_formula = NA) %>% 
  mutate_at(vars(pthn, vowel_ratio), function(x) paste0(x, " SD"))

ggplot(m, aes(consonant_ratio, .value, colour = interaction(vowel_ratio, pthn, sep = "/"), fill = interaction(vowel_ratio, pthn, sep = "/"))) +
  stat_lineribbon(.width = 0.95, alpha = 0.25, size = 0) +
  stat_summary(fun = mean, geom = "line", size = 1) +
  labs(x = "Consonant similarity", y = "P(Correct)", colour = "PTHN/Vowel similarity",
       fill = "PTHN/Vowel similarity") +
  scale_y_continuous(labels = scales::percent, limits= c(0, 1)) +
  guides(colour = guide_legend(ncol = 2)) +
  theme(
    legend.position = "top",
    panel.grid = element_line(colour = "grey", linetype = "dotted")
  )

```

```{r emmeans_vowel, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.width=6, fig.height=6}
# by vowel_ratio
nd <- expand_grid(
  pthn = c(-1, 1),
  frequency = 0,
  vowel_ratio = seq(
    min(responses_coded$vowel_ratio, na.rm = TRUE),
    max(responses_coded$vowel_ratio, na.rm = TRUE),
    by = 0.1
  ),
  consonant_ratio = c(-1, 1)
)

m <- add_fitted_draws(nd, model_fits$fit_4, n = 50, re_formula = NA) %>% 
  mutate_at(vars(pthn, consonant_ratio), function(x) paste0(x, " SD"))

ggplot(m, aes(vowel_ratio, .value, colour = interaction(consonant_ratio, pthn, sep = "/"), fill = interaction(consonant_ratio, pthn, sep = "/"))) +
  stat_lineribbon(.width = 0.95, alpha = 0.25, size = 0) +
  stat_summary(fun = mean, geom = "line", size = 1) +
  labs(x = "Vowel similarity", y = "P(Correct)", colour = "PTHN/Consonant similarity",
       fill = "PTHN/Consonant similarity") +
  scale_y_continuous(labels = scales::percent, limits= c(0, 1)) +
  guides(colour = guide_legend(ncol = 2)) +
  theme(
    legend.position = "top",
    panel.grid = element_line(colour = "grey", linetype = "dotted")
  )

```

#### {-}

### Accuracy by item

```{r accuracy_item, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
responses_item <- responses %>%
  group_by(group, word) %>%
  summarise(
    correct_sum = sum(correct, na.rm = TRUE),
    correct_n = sum(!is.na(correct), na.rm = TRUE),
    .groups = "drop"
  ) %>% 
  mutate(
    correct_prop = prop_adj(correct_sum, correct_n),
    correct_se = prop_adj_se(correct_sum, correct_n),
    word_label = reorder_within(word, by = correct_prop, within = group)
  )

ggplot(responses_item, aes(reorder(word_label, correct_prop), correct_prop, ymin = correct_prop-correct_se, ymax = correct_prop+correct_se, colour = group)) +
  facet_wrap(~group, scales = "free_y") +
  geom_hline(yintercept = 0.5, colour = "black") +
  geom_errorbar() +
  geom_point(size = 0.5) +
  labs(y = "Correct responses", x = "Word", colour = "Group") +
  coord_flip() +
  scale_y_continuous(limits = c(0, 1), labels = scales::percent) +
  theme(
    legend.position = "none",
    axis.text.y = element_blank(),
    axis.ticks = element_blank(),
    axis.text.x = element_text(size = 9),
    panel.grid.major.x = element_line(colour = "grey", linetype = "dotted")
  )

```

### Accuracy by participant

```{r responses_participant, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
responses_participant <- responses %>%
  group_by(group, participant) %>%
  summarise(
    correct_sum = sum(correct, na.rm = TRUE),
    correct_n = sum(!is.na(correct), na.rm = TRUE),
    .groups = "drop"
  ) %>% 
  mutate(
    correct_prop = prop_adj(correct_sum, correct_n),
    correct_se = prop_adj_se(correct_sum, correct_n),
    participant_label = reorder_within(participant, by = correct_prop, within = group)
  )

ggplot(responses_participant, aes(reorder(participant_label, correct_prop), correct_prop, ymin = correct_prop-correct_se, ymax = correct_prop+correct_se, colour = group)) +
  facet_wrap(~group, scales = "free_y") +
  geom_hline(yintercept = 0.5) +
  geom_errorbar(width = 0.5) +
  geom_point(size = 1) +
  labs(y = "Correct responses", x = "Participant", colour = "Group") +
  coord_flip() +
  scale_y_continuous(limits = c(0, 1), labels = scales::percent) +
  theme(
    legend.position = "none",
    axis.text.y = element_blank(),
    axis.ticks = element_blank(),
    axis.text.x = element_text(size = 9),
    panel.grid.major.x = element_line(colour = "grey", linetype = "dotted")
  )

```


### {-}

## {-}

# {-}

# References