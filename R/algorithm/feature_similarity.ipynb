{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study of word similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'TranElicit_word_lists.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d24439bd1792>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# read the word list from the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mwordlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfnameW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwordsheet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m#print(wordlist)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, io, engine)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstringify_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Install xlrd >= 1.0.0 for Excel support\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"xlrd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36mload_workbook\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/xlrd/__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# a ZIP file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'TranElicit_word_lists.xlsx'"
     ]
    }
   ],
   "source": [
    "import pandas as pd # pandas package for importing files (e.g. from Excel)\n",
    "import numpy as np\n",
    "import difflib # package for SequenceMatcher\n",
    "from IPython.display import Markdown, display # Use Markdown\n",
    "\n",
    "############################\n",
    "# WORD LIST\n",
    "############################\n",
    "# Requires columns:\n",
    "#Index - (important to be unique - if the index is repeated, the code gives an error)\n",
    "#Word1 - English word (or other reference language)\n",
    "#Phon1 - phonetic transcription of English word / reference language (can be IPA or SAMPA)\n",
    "#Word2 - AL word\n",
    "#Phon2 - phonetic transcription of AL word\n",
    "\n",
    "#fnameW      = 'word_lists.xlsx'\n",
    "#fnameW      = 'OCDI100_word_lists.xlsx'\n",
    "#wordsheet  = 'English-French'\n",
    "#wordsheet  = 'English-Dutch'\n",
    "#wordsheet  = 'English-German'\n",
    "#wordsheet  = 'English-Italian'\n",
    "#wordsheet  = 'English-Spanish'\n",
    "#wordsheet  = 'English-Welsh'\n",
    "#wordsheet  = 'Test'\n",
    "\n",
    "fnameW      = 'TranElicit_word_lists.xlsx'\n",
    "wordsheet  = 'English-Spanish.Task'\n",
    "#wordsheet  = 'English-Catalan.Task'\n",
    "#wordsheet  = 'Spanish-Catalan.Task'\n",
    "#wordsheet  = 'FalseFriends'\n",
    "\n",
    "#fnameW      = 'OCDI_word_lists.xlsx'\n",
    "#wordsheet  = 'OCDI'\n",
    "\n",
    "# read the word list from the file\n",
    "wordlist = pd.read_excel(fnameW, sheet_name=wordsheet, index_col='Index')\n",
    "\n",
    "#print(wordlist)\n",
    "\n",
    "\n",
    "############################\n",
    "# PHONEME LIST\n",
    "############################\n",
    "# Requires columns:\n",
    "# IPA (and/or) SAMPA - List of IPA / SAMPA phonetic transcriptions\n",
    "# Code - Lists of associated phoneme codes\n",
    "\n",
    "# 3 digit code for each phoneme, corresponding to manner/place/voicing or height/backness/roundedness\n",
    "# Consonants have codes starting 1 or 2, vowels have codes starting 7,8,9\n",
    "# consonants [0] - voicing, [1] - place, [2] - manner\n",
    "# vowels [0] - height, [1] - backness, [2] - roundedness\n",
    "\n",
    "fnameP      = 'Phon_list.xlsx'\n",
    "#phonesheet = 'PhoneCoding'\n",
    "phonesheet = 'PhoneCoding.broad' # collapses phoneme features into broader categories\n",
    "\n",
    "# read as appropriate\n",
    "phncodes = pd.read_excel(fnameP, sheet_name=phonesheet, index_col='IPA')\n",
    "#phncodes = pd.read_excel(fnameP, sheet_name=phonesheet, index_col='SAMPA')\n",
    "\n",
    "\n",
    "# Updates 26/06/2020\n",
    "# - Now takes SAMPA as input (big achievement!)\n",
    "# - automatically formats the input (removing separators, diacritics, etc), reducing need for manual editing\n",
    "# - calculates phoneme count, so there is no need to calculate phoneme count in source file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:112: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:112: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:112: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:118: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:118: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:118: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:126: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<ipython-input-4-9eb9df02d3d1>:112: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if tag is 'equal' and i1 is 0 and j1 is 0:\n",
      "<ipython-input-4-9eb9df02d3d1>:112: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if tag is 'equal' and i1 is 0 and j1 is 0:\n",
      "<ipython-input-4-9eb9df02d3d1>:112: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if tag is 'equal' and i1 is 0 and j1 is 0:\n",
      "<ipython-input-4-9eb9df02d3d1>:118: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  elif tag is not 'equal' and i1 is 0 and j1 is 0:\n",
      "<ipython-input-4-9eb9df02d3d1>:118: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif tag is not 'equal' and i1 is 0 and j1 is 0:\n",
      "<ipython-input-4-9eb9df02d3d1>:118: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif tag is not 'equal' and i1 is 0 and j1 is 0:\n",
      "<ipython-input-4-9eb9df02d3d1>:126: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if tag is 'replace':\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'wordlist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9eb9df02d3d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# https://docs.python.org/3.5/library/difflib.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mwrd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwordlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mwrd1\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mwordlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwrd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Phon1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# IPA or SAMPA transcription of L1 word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mwrd2\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mwordlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwrd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Phon2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wordlist' is not defined"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# SIMILARITY ALGORITHM\n",
    "########################################################\n",
    "\n",
    "# This code just gets more and more efficient :D\n",
    "# But I still don't know how to export as a file lol\n",
    "\n",
    "### based off difflib.SequenceMatcher\n",
    "# https://docs.python.org/3.5/library/difflib.html\n",
    "    \n",
    "for wrd in wordlist.index:\n",
    "    wrd1  = wordlist.loc[wrd]['Phon1'] # IPA or SAMPA transcription of L1 word\n",
    "    wrd2  = wordlist.loc[wrd]['Phon2']\n",
    "    ort1  = wordlist.loc[wrd]['Word1'] # orthographic form of L1 word\n",
    "    ort2  = wordlist.loc[wrd]['Word2']  \n",
    "    \n",
    "    \n",
    "    # Removes non-informative elements (spaces, long vowel marker, phoneme separator, stress markers)\n",
    "    for r in ((\" \", \"\"), (\":\", \"\"), (\".\", \"\"), (\"'\", \"\"), ('\"', \"\"), ('ˈ', \"\")):\n",
    "        wrd1 = wrd1.replace(*r)\n",
    "        wrd2 = wrd2.replace(*r)\n",
    "    \n",
    "    # Remove diacritics from phonemes (centralised, pharyngealized, nasalised, dental, aspirated)\n",
    "    # (not of interest, and the algorithm treats them as separate units, which is problematic)\n",
    "    for d in ((\"_0\", \"\"), (\"_?\", \"\"),(\"~\", \"\"), ('̃',\"\"), (\"_d\",\"\"), ('̪', \"\"), (\"_h\",\"\"), (\"ʰ\", \"\")):\n",
    "        wrd1 = wrd1.replace(*d)\n",
    "        wrd2 = wrd2.replace(*d)\n",
    "    \n",
    "    #print(wrd1+', '+wrd2)     \n",
    "\n",
    "    # identify the phoneme count of the longer word\n",
    "    len1 = len(wrd1)\n",
    "    len2 = len(wrd2)\n",
    "    if len1 >= len2:\n",
    "        length = len1\n",
    "    else:\n",
    "        length = len2 \n",
    "\n",
    "    phnlist1 = list(wrd1) # convert word into list of individual phonemes\n",
    "    phnlist2 = list(wrd2)\n",
    "    \n",
    "    #If there are separators\n",
    "    #phnlist1 = wrd1.split(\".\") # this would be useful if we want to treat diphtongs as a single unit\n",
    "    #phnlist2 = wrd2.split(\".\") # if we go for this route, need to add specific codes for diphtongs to phonlist\n",
    "\n",
    "    #print(type(phnlist1[i]))\n",
    "        \n",
    "    # phnlist1 output is list of strings\n",
    "    # If phoneme code is a number, convert to integer format, otherwise keep as string\n",
    "    \n",
    "    # For some reason .loc doesn't accept numbers as strings\n",
    "        #print(phncodes.loc[9]['Code']) # works\n",
    "        #print(phncodes.loc['9']['Code']) # doesn't work\n",
    "        #print(phncodes.loc['a']['Code']) # works\n",
    "\n",
    "    # convert phoneme list into phoneme CODE list\n",
    "    for i in range(len(phnlist1)):\n",
    "        if phnlist1[i].isdigit():\n",
    "            phnlist1[i] = int(phnlist1[i])\n",
    "        phnlist1[i] = phncodes.loc[phnlist1[i]]['Code']\n",
    "                    \n",
    "    for i in range(len(phnlist2)):\n",
    "        if phnlist2[i].isdigit():\n",
    "            phnlist2[i] = int(phnlist2[i])\n",
    "        phnlist2[i] = phncodes.loc[phnlist2[i]]['Code']\n",
    "       \n",
    "    #print(phnlist1, phnlist2)\n",
    "\n",
    "    # Note: By applying the algorithm on phoneme codes instead of phonemes, \n",
    "    # we allow the algorithm to match up functionally identical phonemes,\n",
    "    # which otherwise which would have been treated as different items\n",
    "    # e.g. [l] and [ɫ] (Voiced Alveolar Approximant, code 213)\n",
    "    # The sensitivity of phoneme differentiation can therefore be fine-tuned in the code specificiation\n",
    "    \n",
    "    ############################################\n",
    "    # STEP 1: Apply SequenceMatcher Base Algorithm\n",
    "    ############################################\n",
    "    \n",
    "    s = difflib.SequenceMatcher(None, phnlist1, phnlist2)\n",
    "    # Use SequenceMatcher to identify similiarity between phoneme code lists \n",
    "    # (base algorithm only matches identical elements)\n",
    "        \n",
    "    simEdits = s.get_opcodes() # show edit operation: equal/replace/delete/insert\n",
    "    #print(simEdits)\n",
    "    \n",
    "    matchblock = s.get_matching_blocks() # Get details of each matching block\n",
    "    #print(matchblock)\n",
    "    matchcount = 0\n",
    "    for match in matchblock:\n",
    "        matchcount += int(match.size) # Count total number of identical elements\n",
    "    #print(matchcount)\n",
    "    \n",
    "    similarity = s.ratio() # proportion of overlapping elements\n",
    "    #print(similarity)\n",
    "    \n",
    "    longmatch = s.find_longest_match(0, len(phnlist1), 0, len(phnlist2)) # longest CONTINUOUS string of identical elements\n",
    "    #print(longmatch.size)\n",
    "\n",
    "    ############################################\n",
    "    # STEP 2: Refine calculation\n",
    "    ############################################\n",
    "    \n",
    "    nsim = 0\n",
    "    for tag, i1, i2, j1, j2 in simEdits:\n",
    "        \n",
    "        #################################\n",
    "        # Common onset facilitation effect\n",
    "        # boost similarity score if the first phoneme is the same\n",
    "        #################################\n",
    "        if tag is 'equal' and i1 is 0 and j1 is 0:\n",
    "            onset = 'same'\n",
    "            onsetlong = i2-i1 # how many consecutive matched phonemes are there at onset\n",
    "            nsim += 0.5*onsetlong # onset boost is multiplied by the length of the onset match        \n",
    "            # cohort effect, where a large number of consecutive matched elements at onset reduces the possible referents\n",
    "            \n",
    "        elif tag is not 'equal' and i1 is 0 and j1 is 0:\n",
    "            onset = 'different'\n",
    "            onsetlong = 0\n",
    "                        \n",
    "            \n",
    "        #################################\n",
    "        # Identify non-identical but very similar phonemes and add weighted score\n",
    "        #################################\n",
    "        if tag is 'replace': \n",
    "            # Identify which phonemes were replaced with which\n",
    "            # SequenceMatcher output shows chunks that were replaced in between matched elements\n",
    "            phn1 = phnlist1[i1:i2]\n",
    "            phn2 = phnlist2[j1:j2]\n",
    "            #print(tag, phn1, phn2)\n",
    "            \n",
    "            # convert feature code to a list of digits for detailed comparison\n",
    "            # Output - List of lists\n",
    "                # https://stackoverflow.com/questions/12293208/how-to-create-a-list-of-lists\n",
    "            lst1 = []\n",
    "            for i in range(len(phn1)):\n",
    "                line  = [int(x) for x in str(phn1[i])]\n",
    "                lst1.append(line)\n",
    "            \n",
    "            lst2 = []\n",
    "            for i in range(len(phn2)):\n",
    "                line  = [int(x) for x in str(phn2[i])]\n",
    "                lst2.append(line)\n",
    "\n",
    "            #print(lst1)\n",
    "            #print(lst2)\n",
    "\n",
    "            codesim1 = 0\n",
    "            \n",
    "            # identify length of shorter list, to specify number of phonemes to compare across words\n",
    "            if len(lst1) <= len(lst2):\n",
    "                strlen = len(lst1)\n",
    "            elif len(lst1) >= len(lst2):\n",
    "                strlen = len(lst2)\n",
    "\n",
    "            # Compare first digit in English code to first digit in Spanish code, the second to the second, third to the third\n",
    "            # outer for-loop\n",
    "                # runs through the list of phoneme codes \"lst1\"\n",
    "            # inner for-loop\n",
    "                # uses the index to work up from the first to the last number in the code (in this case 3 numbers, with index being 0, 1, 2)\n",
    "            for i in range(strlen):\n",
    "                # For vowels (code 777-999)\n",
    "                # [0] - height, [1] - backness, [2] - roundedness\n",
    "                if lst1[i][0] >= 7 and lst2[i][0] >= 7: # if it's a vowel replaceement\n",
    "                    # Weight option 1: Each feature change is weighted equal, and add score as long as there is at least one common feature\n",
    "                    for dgt, idgt in zip(lst1[i], range(len(lst1[i]))):\n",
    "                        if dgt == lst2[i][idgt]:\n",
    "                            codesim1 += 1/3 \n",
    "                        # If compared digits (feature) are the same, add 1/3 to the similarity score\n",
    "\n",
    "                # For consonants (code 144-244)\n",
    "                # [0] - voicing, [1] - place, [2] - manner\n",
    "                if lst1[i][0] <= 6 and lst2[i][0] <= 6: # consonant replacement\n",
    "                    # Weight option 2: Different feature changes weighted differently\n",
    "                    # Only accept close feature changes\n",
    "                    if lst1[i][1] == lst2[i][1] and lst1[i][2] == lst2[i][2]: # voicing change\n",
    "                        codesim1 += 0.6 # allows [k] to [g]\n",
    "                    elif lst1[i][0] == lst2[i][0] and lst1[i][2] == lst2[i][2]: # place change\n",
    "                        if abs(int(lst1[i][1]) - int(lst2[i][1])) == 1: \n",
    "                            codesim1 += 0.4 # allows [n] to [ŋ]\n",
    "                    elif lst1[i][0] == lst2[i][0] and lst1[i][1] == lst2[i][1]: # manner change\n",
    "                        if abs(int(lst1[i][0]) - int(lst2[i][0])) == 1: \n",
    "                            codesim1 += 0.2 # allows [b] to [β]\n",
    "\n",
    "            #print(codesim1)\n",
    "            \n",
    "            ##############################\n",
    "            #IMPORTANT\n",
    "            # Temporary measure, don't add score for close phonemes \n",
    "            ##############################\n",
    "            #nsim += codesim1 # total 'refinement' score\n",
    "    \n",
    "    # calculate standardised refinement score to be added to initial similarity score\n",
    "    replace_ratio = nsim/length        \n",
    "\n",
    "    #print(nsim)\n",
    "    #print(replace_ratio)\n",
    "    \n",
    "    ############################################\n",
    "    # STEP 3: Add the base score and refinement score together\n",
    "    ############################################\n",
    "\n",
    "    similarity_phoneme = similarity + replace_ratio\n",
    "\n",
    "    # Note: because of the onset phoneme boost, final score can go above 1 for very similar words\n",
    "    # Round back to max value of 1 if that happens\n",
    "    similarity_phoneme = min(1, similarity_phoneme) #...why is the function to set maximum value min()\n",
    "    #print(similarity_phoneme) # Final similarity score\n",
    "    \n",
    "    # Lang1_ortho, Lang2_ortho, Lang1_phon, Lang2_phon, final_similarity_score, total_identical_matches, shared_onset, number of consecutive phonemes shared at onset, longest_continuous_match\n",
    "    print(ort1+','+ort2+','+wrd1+','+wrd2+','+str(similarity_phoneme)+','+str(matchcount)+','+onset+','+str(onsetlong)+','+str(longmatch.size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# POINTS TO BE IMPROVED\n",
    "############################################\n",
    "        # Next step 27/06/2020\n",
    "        # check translation elicitation task if phon neighbour density is a significant predictor\n",
    "            \n",
    "        \n",
    "        # additional (less strong) multiplier on non-onset longest match (at rhyme)\n",
    "\n",
    "# - Currently short words are more strongly affected by the score multiplier, which doesn't match up to real life\n",
    "#   In some cases, a long word with half overlapping phonemes is more perceptually recognisable as a cognate \n",
    "#   than a 3 phoneme word with 2 overlapping phonemes, because of cohort effects (the long word has less cohorts to select from)\n",
    "\n",
    "# Can look into possible other ways to manipulate weights:\n",
    "    # Distance of feature change (i.e. front vs mid vowel is more similar than front vs back)\n",
    "    # A consonant match adds less score than a vowel match\n",
    "\n",
    "# - certain types of phoneme changes are likely more salient than others\n",
    "#   and this goes beyond simple differences between manner/place/voicing\n",
    "#   More likely it is language-dependent: If a language does not have a minimal pair between a particular pair of phonemes,\n",
    "#   the salience of that change would likely be perceptually negligible to the speaker\n",
    "#   Example: Japanese doesn't differentiate between /r/ and /l/\n",
    "#   Example2: English typically pronounce R as approximant [ɹ]\n",
    "#   so are less likely to recognise difference between approximant [ɹ], tap [ɾ], trill [r]\n",
    "#   Frequency of the minimal pair also likely makes a difference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
