```{r}
#| label: setup
library(targets)
library(dplyr)
library(tidyr)
library(ggplot2)
library(patchwork)
library(gt)
library(gtExtras)
library(brms)
library(tidybayes)
library(ggExtra)
library(tinytable)
library(stringdist)

set.seed(1234)

# import objects
tar_config_set(
  store = here::here("_targets"),
  script = here::here("_targets.R")
)
tar_load_globals()
tar_load(participants)
tar_load(exp_participants)
tar_load(exp_responses)
tar_load(quest_responses)
tar_load(quest_participants)
tar_load(stimuli)
tar_load(epreds)
tar_load(dataset_1)
tar_load(dataset_2)
tar_load(dataset_3)
tar_load(exp_1_m0)
tar_load(exp_2_m0)
tar_load(exp_3_m1)


theme_set(theme_ggdist() +
  theme(panel.grid.major.y = element_line(
    colour = "grey",
    linetype = "dotted"
  )))

```

# Introduction


::: {.content-visible when-format="pdf"}

When some German speakers listened to the song *The Power* by SNAP! (*Coyote Ugly*, 2000), many of them misheard the line "I've got the power" as *Agatha Bauer!* When Michael Jackson vocalised "ma-ma-coo-sah" in his song *Wanna Be Startin' Somethin'* (*Thriller*, 1982), some Dutch speakers understood *mama appelsap* [mama applejuice]. When Spanish speakers listen to the line "Circumvent your thick ego" from the song *Pictures* by System of a Down (*Steal this Album!*, 2002), they tend to hear *Sácame de aquí, King-Kong* [take me out of here, King-Kong]. When Japanese speakers listen to the line "I want to hold your hand" in the homonym song by The Beatles (*Meet The Beatles*, 1964), they often mishear *Aho na hōnyōhan* [stupid urinator]. Outrageous as these examples might sound to speakers of other languages, many readers may know a few cases in their own native language. This auditory illusion can feel quite real, and often inevitable after the first time it happens. In Japanese, this phenomenon takes the name *Soramimi* (lit. "empty ear"), after the *Soramimi Hour*---a section of the program *Tamori Club* hosted by comedian Morita Kazuyoshi (Tamori)---in which instances of this illusion were presented with comedy purposes. Soramimi is a particular case of *homophonic translation*: words or phrases in one language are translated into similar-sounding words and phrases in a different language, without necessarily preserving the meaning [@gasparov2006semen]. This phenomenon extends beyond song lyrics, and may occur anytime when listening to non-native speech. For instance, when listening to "You know me?" (\textipa{/ju: n@U mi:}/) in English, Japanese-native listeners tend to understand *yunomi* (/\textipa{jUnomi}/), which is a type of Japanese teacup. Overall, the occurrence of Soramimi---and more generally, homophonic translations---suggests that speech in an unfamiliar language has the potential to activate lexical representations in the native language.

Although homophonic translations do not necessarily take place with the preservation of meaning---which often results in outrageous mistranslations like the ones aforementioned---homophonic translations sometimes may lead to correct translations. For example, imagine a Spanish native with no previous familiarity with any other language listening to French for the first time. This listener may encounter the word /\textipa{"pOKt}/ (*porte*), translation of door in French. When comparing the phonemic transcription of the French to that of its Spanish translation /\textipa{"pweR.ta}/ (*puerta*), one may be led to infer that these two words do not have much overlap. But one thing to note is that the voiced uvular fricative consonant /\textipa{K}/ and the open-mid back rounded vowel /\textipa{O}/, which do not exist in Spanish as phonemic categories, can be perceived as allophonic variations the native (Spanish) phonemes /\textipa{R}/ and /o/, as described by assimilation models of phoneme perception [@best2001discrimination; @best1988examination]. Although this phonetic mismatch between the non-native word and its translation can have a noticeable toll on comprehension @cutler2004patterns, the Spanish listener may still be able to activate the phonological representation of /\textipa{"pweR.ta}/ (*puerta*) from hearing the word-form /\textipa{"pOKt}/ (*porte*), ultimately allowing them to access the correct semantic representation. Given enough phonological similarity between the non-native word and its translation in the native language, a naïve listener may succeed at word comprehension when listening to an unfamiliar language. Such phonological similarity between translation equivalents---known as *cognateness*---is common across many languages, and is often due to typological closeness and/or socio-historical events involving the speakers of these languages (e.g., migration, social contact). For example, Romance languages such as Spanish and Catalan share many form-similar translation equivalents  [@schepens2012distributions], as in the case of *puerta* and *porta* (*door* in Spanish and Catalan, respectively). Given no prior knowledge of either language, a Spanish native speaker is likely to be much more successful at correctly translating Catalan *porta* than English *door*, due to the phonological and orthographic similarity of the former to the equivalent in their native language.

While Soramimi (and more generally homophonic translations) are popular and frequent phenomena, the psycholinguistic literature on the topic is relatively scarce, and has mostly focused on the phonological variables involved in the occurence of Soramimi. For instance, @otake2007interlingual analysed 194 instances of Soramimi, broadcasted between 1992 and 2007 by the TV show Soramimi hour. These instances consisted of homophonic translations of English song lyrics to Japanese words and phrases. The vast majority of the reported instances were phrases (96%) and a few, to single words (4%). The phonetic features of the presented English lyrics were preserved with relatively high variability, with some Japanese resulting words or phrases sharing high overlap with the original English strings of sounds, and some sharing very little overlap. When analysing the few instances of homophonic translations of single words, the author identified three phonological processes that might explain how Japanese listeners reconstructed the English input to generate the Japanese words: insertion (e.g., *cry* /\textipa{kraI}/ to *kurai*, /\textipa{kuRai}/ [dark]), deletion (e.g., *go*, /\textipa{goU}/ to *go*, /go/ [go], and alternation (e.g., *low*, /\textipa{loU}/ to *rou*, /\textipa{Roo}/ [wax]). This suggests that the reported homophonic translations can be explained, to some extent, as the result of the Japanese listeners accommodating the strings of English sounds to the Japanese phoneme inventory and Japanese phonotactics [@peperkamp2008perceptual; @dupoux1999epenthetic]. Native prosody also constrains the occurence of homophonic translations. For instance, @kentner2015rhythmic presented in the auditory modality French-native and German-native participants with English songs with lyrics. Homophonic translations followed language-specific segmentation strategies. Most homophonic translations reported by German-native participants resulted from inserting phrase and word boundaries *before* stressed syllables (following the more frequent trochaic patter in German). On the contrary, most homophonic translations reported by French-native participants resulted from inserting phrase and word boundaries *after* stressed syllables, following the more frequent iambic pattern in French.

These studies have addressed the phonological aspects of homophonic translation, but little attention has been paid to the dynamics of lexical activation and competition that underlay this phenomenon. In the present work, we investigated the interplay between phonological similarity and phonological neighbourhood density in a non-native speech perception task. In particular, we studied how the presence of phonological neighbours in the native language lexicon affects the extent to which listeners are able to activate the correct translation of an auditorily presented word in thier native language. We used a translation elicitation task in which participants listened to words from an unfamiliar language, and then tried to guess their translation in their native language. We will henceforth refer to the auditory-presented words heard by participants on each trial as *presented words*, and the correct translation for the presented words as *target translations*. We explored listeners’ reliance on phonological similarity to succeed in the task by manipulating the amount of phonological similarity between the presented words and their target translations. Since participants were unfamiliar with the presented language, they could only use phonological similarity between the presented language and their native language to successfully translate the words. We therefore predicted that participants’ performance should increase when the translation pairs are phonologically more similar. We also predicted that there is a minimum threshold of phonological similarity to be sufficient for translation.

We further investigated the effect of phonological competitors. Even if the presented word and its target translation share high phonological similarity, participants may still provide incorrect translations if the presented word also shares high phonological similarity with other words in the native language. Words with denser phonological neighbourhoods are recognised more slowly and less accurately than words with sparser phonological neighbourhoods [@dufour2003lexical; @goldinger1989priming; @hamburger1996phonological; @luce1990similarity; @luce1998recognizing]. This is especially true if such neighbours share higher phonological similarity with the presented word, or are lexically more frequent than the target translation [@luce1998recognizing]. Since the interference effects of phonological neighbourhood density has also been reported across languages [@weber2004lexical], we hypothesised that the size of the facilitatory role of phonological similarity would be inversely proportional to the amount of higher-frequency phonological neighbours of the presented word in the target language. In the case of non-native speech recognition, the presence of cross-linguistic pairs which are phonologically similar but differ in meaning (e.g., false friends) may act as distractors during lexical access, obstructing the selection of the appropriate target translation in the listener’s lexicon. For instance, @otwinowska2019more reported that false friends were disadvantaged relative to non-cognates by Polish second language learners, in contrast to cognates which were known better. It is therefore important to investigate the joint effect of both cognates and false friends when investigating the effect cross-linguistic phonological similarity has on word recognition in a foreign language. This could shed light on available strategies and challenges associated with the early stages of second language acquisition. For instance, one could expect English participants to incorrectly translate the Spanish word *botón* as *bottom* instead of as its correct translation *button*, due to combined effect of the close phonological similarity between *botón* and *bottom* along with the high lexical frequency of *bottom* relative to *button*. To test this prediction, we developed a lexical frequency-dependent measure of cross-language phonological neighbourhood density, in which a neighbour is counted only if it is higher frequency and is one phoneme apart from the presented word. If the phonological neighbourhood density of the target translation affects participants’ performance negatively, this would suggest that competitors in the native language affect recognition of non-native words during recognition of foreign speech. We conducted a series of three experiments to test these predictions. 

In Experiment 1, we collected data from two groups of British English natives living in the United Kingdom. One group was presented with Catalan words, the other with Spanish words. We examined the extent to which participants were able to use the phonological similarity between the presented word (in Catalan or Spanish) and its target translation to provide accurate responses. In Experiment 2, we tested a group of (European) Spanish natives in the same task, who were presented with Catalan words. Catalan and Spanish are two Romance languages whose close typological distance is reflected in the fact that they share many cognates, where English is a Germanic language that shares considerably fewer cognates with Catalan and Spanish. By testing participants translating words from typologically close or distant languages, we expected to widen the range of the phonological similarity scores of the translation pairs involved in the experimental task, therefore allowing us to explore potential cross-language differences in participant’s performance. One unexpected finding was that participants in Experiments 1 and 2 were surprisingly good at translating a subset of words which had low phonological similarity with their correct translation. We were concerned that this may be caused by some prior knowledge of specific words by our participants, as some Spanish words are commonly seen in media or product labels, making them familiar even to monolingual speakers of English. In Experiment 3, we collected additional data from a new group of British English natives. The design was closely modelled after Experiment 1, except that after providing their response in each trial, participants reported whether they had previous knowledge of the presented word. In the final section of this article, we present analyses on the joint data sets of Experiments 1 to 3.

:::

::: {.content-visible when-format="docx"}

When some German speakers listened to the song *The Power* by SNAP! (*Coyote Ugly*, 2000), many of them misheard the line "I've got the power" as *Agatha Bauer!* When Michael Jackson vocalised "ma-ma-coo-sah" in his song *Wanna Be Startin' Somethin'* (*Thriller*, 1982), some Dutch speakers understood *mama appelsap* [mama applejuice]. When Spanish speakers listen to the line "Circumvent your thick ego" from the song *Pictures* by System of a Down (*Steal this Album!*, 2002), they tend to hear *Sácame de aquí, King-Kong* [take me out of here, King-Kong]. When Japanese speakers listen to the line "I want to hold your hand" in the homonym song by The Beatles (*Meet The Beatles*, 1964), they often mishear *Aho na hōnyōhan* [stupid urinator]. Outrageous as these examples might sound to speakers of other languages, many readers may know a few cases in their own native language. This auditory illusion can feel quite real, and often inevitable after the first time it happens. In Japanese, this phenomenon takes the name *Soramimi* (lit. "empty ear"), after the *Soramimi Hour*---a section of the program *Tamori Club* hosted by comedian Morita Kazuyoshi (Tamori)---in which instances of this illusion were presented with comedy purposes. Soramimi is a particular case of *homophonic translation*: words or phrases in one language are translated into similar-sounding words and phrases in a different language, without necessarily preserving the meaning. This phenomenon extends beyond song lyrics, and may occur anytime when listening to non-native speech. For instance, when listening to "You know me?" (/ju: nəʊ mi:/) in English, Japanese-native listeners tend to understand *yunomi* (/jʊnomi/), which is a type of Japanese teacup. Overall, the occurence of Soramimi---and more generally, homophonic translations---suggests that speech in an unfamiliar language has the potential to activate lexical representations in the native language.

Although homophonic translations do not necessarily take place with the preservation of meaning---which often results in outrageous mistranslations like the ones aforementioned---homophonic translations sometimes may lead to the activation of the intended meaning (i.e., a correct translation). For example, imagine a Spanish native with no previous familiarity with any other language listening to French for the first time. This listener may encounter the word /pɔKt/ (*porte*), translation of door in French. When comparing the phonemic transcription of the French to that of its Spanish translation /pweɾta/ (*puerta*), one may be led to infer that these two words do not have much overlap. But one thing to note is that the voiced uvular fricative consonant /ʁ/ and the open-mid back rounded vowel /ɔ/, which do not exist in Spanish as phonemic categories, can be perceived as allophonic variations the native (Spanish) phonemes /ʁ/ and /o/, as described by assimilation models of phoneme perception [@best2001discrimination; @best1988examination]. Although this phonetic mismatch between the non-native word and its translation can have a noticeable toll on comprehension @cutler2004patterns, the Spanish listener may still be able to activate the phonological representation of /pweɾta}/ (*puerta*) from hearing the word-form /pɔʁt/ (*porte*), ultimately allowing them to access the correct semantic representation. Given enough phonological similarity between the non-native word and its translation in the native language, a naïve listener may succeed at word comprehension when listening to an unfamiliar language. Such phonological similarity between translation equivalents---known as *cognateness*---is common across many languages, and is often due to typological closeness and/or socio-historical events involving the speakers of these languages (e.g., migration, social contact). For example, Romance languages such as Spanish and Catalan share many form-similar translation equivalents  [@schepens2012distributions], as in the case of *puerta* and *porta* (*door* in Spanish and Catalan, respectively). Given no prior knowledge of either language, a Spanish native speaker is likely to be much more successful at correctly translating Catalan *porta* than English *door*, due to the phonological and orthographic similarity of the former to the equivalent in their native language.

While Soramimi, and more generally homophonic translation, are popular and frequent phenomena, the psycholinguistic literature on the topic is relatively scarce, and has mostly focused on the phonological variables involved in the occurence of Soramimi. For instance, @otake2007interlingual analysed 194 instances of Soramimi, broadcasted between 1992 and 2007 by the TV show Soramimi hour. These instances consisted of homophonic translations of English song lyrics to Japanese words and phrases. The vast majority of the reported instances were phrases (96%) and a few, to single words (4%). The phonetic features of the presented English lyrics were preserved with relatively high variability, with some Japanese resulting words or phrases sharing high overlap with the original English strings of sounds, and some sharing very little overlap. When analysing the few instances of homophonic translations of single words, the author identified three phonological processes that might explain how Japanese listeners reconstructed the English input to generate the Japanese words: insertion (e.g., *cry* /krʌɪ/ to *kurai*, /kuɾai/ [dark]), deletion (e.g., *go*, /goʊ/ to *go*, /go/ [go], and alternation (e.g., *low*, /loʊ/ to *rou*, /ɾoo/ [wax]). This suggests that the reported homophonic translations can be explained, to some extent, as the result of the Japanese listeners accommodating the strings of English sounds to the Japanese phoneme inventory and Japanese phonotactics [@peperkamp2008perceptual; @dupoux1999epenthetic]. Native prosody also constrains the occurence of homophonic translations. For instance, @kentner2015rhythmic presented in the auditory modality French-native and German-native participants with English songs with lyrics. Homophonic translations followed language-specific segmentation strategies. Most homophonic translations reported by German-native participants resulted from inserting phrase and word boundaries *before* stressed syllables (following the more frequent trochaic patter in German). On the contrary, most homophonic translations reported by French-native participants resulted from inserting phrase and word boundaries *after* stressed syllables, following the more frequent iambic pattern in French.

While the previous studies have address the phonological aspects of homophonic translation, little attention has been paid to the dynamics of lexical activation and competition that underlay this phenomenon. In the present work we investigated the interplay between phonological similarity and phonological neighbourhood density to examine how listeners of an unfamiliar language activate lexical representations in their native language. In particular, we studied how the presence of phonological neighbours in the native language lexicon affects the extent to which listeners are able to activate the correct translation of an auditorily presented word in an unfamiliar language. We used a translation elicitation task in which participants listened to words from an unfamiliar language, and then tried to guess their translation in their native language. We will henceforth refer to the auditory-presented words heard by participants on each trial as presented words, and the correct translation for the presented words as target translations. We explored listeners’ reliance on phonological similarity to succeed in the task by manipulating the amount of phonological similarity between the presented words and their target translations. Since participants were unfamiliar with the presented language, they could only use phonological similarity between the presented language and their native language to successfully translate the words. We therefore predicted that participants’ performance should increase when the translation pairs are phonologically more similar. We also predicted that there is a minimum threshold of phonological similarity to be sufficient for translation.

We further investigated the effect of phonological competitors. Even if the presented word and its target translation share high phonological similarity, participants may still provide incorrect translations if the presented word also shares high phonological similarity with other words in the native language. Words with denser phonological neighbourhoods are recognised more slowly and less accurately than words with sparser phonological neighbourhoods [@dufour2003lexical; @goldinger1989priming; @hamburger1996phonological; @luce1990similarity; @luce1998recognizing]. This is especially true if such neighbours share higher phonological similarity with the presented word, or are lexically more frequent than the target translation [@luce1998recognizing; @dufour2010phonological]. Since the interference effects of phonological neighbourhood density has also been reported across languages [@weber2004lexical], we hypothesised that the size of the facilitatory role of phonological similarity would be inversely proportional to the amount of higher-frequency phonological neighbours of the presented word in the target language. In the case of non-native speech recognition, the presence of cross-linguistic pairs which are phonologically similar but differ in meaning (e.g., false friends) may act as distractors during lexical access, obstructing the selection of the appropriate target translation in the listener’s lexicon. For instance, @otwinowska2019more reported that false friends were disadvantaged relative to non-cognates by Polish second language learners, in contrast to cognates which were known better. It is therefore important to investigate the joint effect of both cognates and false friends when investigating the effect cross-linguistic phonological similarity has on word recognition in a foreign language. This could shed light on available strategies and challenges associated with the early stages of second language acquisition. For instance, one could expect English participants to incorrectly translate the Spanish word *botón* as *bottom* instead of as its correct translation *button*, due to combined effect of the close phonological similarity between *botón* and *bottom* along with the high lexical frequency of *bottom* relative to *button*. To test this prediction, we developed a lexical frequency-dependent measure of cross-language phonological neighbourhood density, in which a neighbour is counted only if it is higher frequency and is one phoneme apart from the presented word. If the phonological neighbourhood density of the target translation affects participants’ performance negatively, this would suggest that competitors in the native language affect recognition of non-native words during recognition of foreign speech. We conducted a series of three experiments to test these predictions. 

In Experiment 1, we collected data from two groups of British English natives living in the United Kingdom. One group was presented with Catalan words, the other with Spanish words. We examined the extent to which participants were able to use the phonological similarity between the presented word (in Catalan or Spanish) and its target translation to provide accurate responses. In Experiment 2, we tested a group of (European) Spanish natives in the same task, who were presented with Catalan words. Catalan and Spanish are both Romance languages whose close typological distance is reflected in the fact that they share many cognates, where English is a Germanic language that shares considerably fewer cognates with Catalan and Spanish. By testing participants translating words from typologically close or distant languages, we expected to widen the range of the phonological similarity scores of the translation pairs involved in the experimental task, therefore allowing us to explore potential cross-language differences in participant’s performance. One unexpected finding was that participants in Experiments 1 and 2 were surprisingly good at translating a subset of words which had low phonological similarity with their correct translation. We were concerned that this may be caused by some prior knowledge of specific words by our participants, as some Spanish words are commonly seen in media or product labels, making them familiar even to monolingual speakers of English. In Experiment 3, we collected additional data from a new group of British English natives. The design was closely modelled after Experiment 1, except that after providing their response in each trial, participants reported whether they had previous knowledge of the presented word. In the final section of this paper, we present analyses on the joint data sets of Experiments 1 to 3.

:::



# Experiment 1

## Methods

All materials, data, and code used in this study are hosted in the Open Science Framework [https://osf.io/9fjxm/](https://osf.io/9fjxm/?view_only=aab7636ce1af48cf832596a7ea9101c5/) and a GitHub repository [https://github.com/gongcastro/translation-elicitation.git](https://github.com/gongcastro/translation-elicitation.git), along with additional notes.

### Participants

```{r}
#| label: participants-numbers-1
participants_1 <- exp_participants |>
  filter(group != "cat-SPA")

collection_dates <- participants_1 |>
  pull(date) |>
  range() |>
  format("%B %dth, %Y")

n_participants <- dplyr::n_distinct(participants_1)
n_participants_group <- table(participants_1$group)
participants_age <- participants_1 |>
  summarise(across(age, lst(mean, sd, min, max)))
n_female <- table(participants_1$gender)
```
We collected data from `r n_participants` British English-native adults living in United Kingdom (*Mean* = `r round(participants_age$age_mean, 2)` years, *SD* = `r round(participants_age$age_sd, 2)`, Range = `r round(participants_age$age_min, 2)`-`r round(participants_age$age_max, 2)`, `r n_female[1]` female). Data collection took place from `r collection_dates[1]` to `r collection_dates[2]`. Participants were recruited via Prolific (5£ compensation) and SONA (compensation in academic credits). Participants gave informed consent before providing any data. The study was conducted in accordance with ethical standards of the Declaration of Helsinki and the protocol was by the University of Oxford Medical Sciences Inter-Divisional Research Ethics Committee (IDREC) (R60939/RE007). Participants were asked to complete the experiment using a laptop in a quiet place with good internet connection. 

```{r tbl-participants}
#| label: tbl-participants
#| tbl-cap: "Participant details."
tbl_data <- participants |>
  mutate(exp = case_when(
    source == "Experiment" & group != "cat-SPA" ~ "Experiment 1",
    source == "Experiment" & group == "cat-SPA" ~ "Experiment 2",
    source == "Questionnaire" ~ "Experiment 3",
  )) |>
  summarise(
    n = n(),
    n_excluded = sum(!valid_status == "Valid"),
    across(age, lst(mean, sd, min, max)),
    l2_lst = list(l2),
    .by = c(group, exp)
  ) |>
  mutate(
    l2_lst = map(
      l2_lst,
      function(x) {
        y <- table(x)
        y <- y[names(y) != "None"]
        y <- paste0(names(y), " (", y, ")")
        return(y)
      }
    ),
    n_excluded = paste0(n, " (", n_excluded, ")")
  ) |>
  select(-n)

tbl_data |>
  gt(groupname_col = "exp", rowname_col = "group") |>
  fmt_number(matches("age")) |>
  fmt_integer(matches("min|max")) |>
  cols_merge_uncert(age_mean, age_sd) |>
  cols_merge_range(age_min, age_max) |>
  tab_spanner("Age", matches("age")) |>
  cols_label(
    group = "Group",
    exp = "Experiment",
    n_excluded = "N",
    age_mean = "Mean ± SD ",
    age_min = "Range",
    l2_lst = "L2"
  ) |>
  tab_style(
    cell_text(style = "italic"),
    cells_column_labels()
  ) |>
  tab_style(
    cell_text(weight = "bold"),
    cells_column_spanners()
  ) |>
  tab_style(
    cell_text(align = "left"),
    list(cells_body(), cells_column_labels())
  ) |>
  tab_style(
    cell_text(
      weight = "bold",
      align = "center",
      style = "normal"
    ),
    cells_column_labels(c(l2_lst, n_excluded))
  ) |>
  tab_footnote(
    "Number of included participants (number of excluded participants.)",
    locations = cells_column_labels(n_excluded)
  )
```


### Stimuli

```{r}
#| label: stimuli-lengths
lengths <- stimuli |>
  select(group, ipa_1, ipa_2, word_1, word_2) |>
  mutate(across(ipa_1:word_2, nchar)) |>
  summarise(
    across(
      c(ipa_1, ipa_2, word_1, word_2),
      lst(mean, sd, min, max)
    ),
    .by = group
  )
lengths <- split(lengths, lengths$group)
names(lengths) <- janitor::make_clean_names(names(lengths))

durations <- stimuli |>
  summarise(across(duration, lst(mean, sd, min, max)),
    .by = group
  )
durations <- split(durations, durations$group)
names(durations) <- janitor::make_clean_names(names(durations))
```

We created two lists of input words to be presented to participants in the auditory modality: one in Catalan and one in Spanish. Words in the Catalan list were `r round(lengths$cat_eng$ipa_1_mean, 2)` phonemes long on average (*SD* = `r round(lengths$cat_eng$ipa_1_sd, 2)`, Range = `r round(lengths$cat_eng$ipa_1_min, 2)`-`r round(lengths$cat_eng$ipa_1_max, 2)`), and the orthographic forms of their English translations (which participants had to type) were `r round(lengths$cat_eng$word_2_mean, 2)` characters long on average (*SD* = `r round(lengths$cat_eng$word_2_sd, 2)`, Range = `r round(lengths$cat_eng$word_2_min, 2)`-`r round(lengths$cat_eng$word_2_max, 2)`). Words in the Spanish list were `r round(lengths$spa_eng$ipa_1_mean, 2)` phonemes long on average (*SD* = `r round(lengths$spa_eng$ipa_1_sd, 2)`, Range = `r round(lengths$spa_eng$ipa_1_min, 2)`-`r round(lengths$spa_eng$ipa_1_max, 2)`), and the orthographic form of their English translations were `r round(lengths$spa_eng$word_2_mean, 2)` characters long on average (*SD* = `r round(lengths$spa_eng$word_2_sd, 2)`, Range = `r round(lengths$spa_eng$word_2_min, 2)`-`r round(lengths$spa_eng$word_2_max, 2)`).

In each trial, participants listened to one audio file, wich contained a single word. The audio files were the same ones used in child experiments conducted in the Laboratori de Recerca en Infància of Universitat Pompeu Fabra (Barcelona, Spain). These audio files were recorded by a proficient Catalan-Spanish female bilingual from the Metropolitan Area of Barcelona in a child-directed manner. Catalan and Spanish words were recorded at 44,100 Hz in separate files in the same session, and then de-noised using Audacity and normalised at peak intensity using Praat [@broersma2021praat]. The average duration of the Catalan audio files was `r round(durations$cat_eng$duration_mean, 2)` seconds (*SD* = `r round(durations$cat_eng$duration_sd, 2)`, Range = `r round(durations$cat_eng$duration_min, 2)`-`r round(durations$cat_eng$duration_max, 2)`). The average duration of the Spanish audio files was `r round(durations$spa_eng$duration_mean, 2)` seconds (*SD* = `r round(durations$spa_eng$duration_sd, 2)`, Range = `r round(durations$spa_eng$duration_min, 2)`-`r round(durations$spa_eng$duration_max, 2)`).

```{r exc-stimuli}
stim_exc_freq <- stimuli |>
  mutate(is_na = is.na(freq_zipf_2)) |>
  count(group, is_na) |>
  filter(is_na)
stim_exc_freq <- split(stim_exc_freq, stim_exc_freq$group)
names(stim_exc_freq) <- janitor::make_clean_names(names(stim_exc_freq))

stim_exc_freq_n <- sum(map_int(stim_exc_freq, "n"))
```

For each word in the Catalan and Spanish lists pair, we defined three predictors of interest: the lexical frequency of the correct English translation (*Frequency*), the phonological similarity between the presented word (in Catalan or Spanish) and their correct English translation (*Similarity*), and the presented word's number of cross-language phonological neighbours (*CLPN*) in English. *Frequency* was included as a nuance predictor, under the hypothesis that---keeping other predictors constant---participants would translate higher-frequency words more accurately and faster than lower-frequency words. Lexical frequencies of correct translations were extracted from SUBTLEX-UK [@van2014subtlex], and transformed to Zipf scores. Words in the stimuli list without a lexical frequency value were excluded from data analysis (`r stim_exc_freq$cat_eng$n` in the Catalan list, `r stim_exc_freq$spa_eng$n` in the Spanish list).


We calculated *Similarity*, our main predictor of interest, by computing the Levenshtein similarity between the X-SAMPA transcriptions of each pair of translations using the `stringdist` R package [@van2014stringdist]. The Levenshtein distance computes the edit distance between two character strings---in this case, two phonological transcriptions---by counting the number of additions, deletions, and substitutions necessary to make both strings identical [@levenshtein1966binary]. We divided this edit distance by the number of characters included in the longest X-SAMPA transcription of the translation pair. This results in a proportion score, in which values closer to zero correspond to lower Levenshtein distances between phonological transcriptions (i.e., higher similarity), and values closer to 1 correspond to higher Levenshtein distances (i.e., lower similarity). This transformation accounts for the fact that the Levenshtein distance tends to increase with the length of the transcriptions. For interpretability, we subtracted this proportion from one, so that values closer to one correspond to higher similarity between phonological transcriptions, and values closer to zero correspond to lower similarity between phonological transcriptions. For example, the *table* (`teIb@l`)-*mesa* (`mesa`) translation pair had a `r scales::percent(stringdist::stringsim(enc2utf8("teɪbəl"), enc2utf8("mesa")))` similarity, while the *train* (`trEIn`)-*tren* (`tREn`) translation pair had a `r scales::percent(stringdist::stringsim(enc2utf8("trEIn"), enc2utf8("tɾEn")))` similarity. @tbl-stimuli summarises the lexical frequency, phonological neighbourhood density and phonological overlap of the words included in the Catalan and the Spanish lists.

For each Catalan and Spanish word, we calculated the number of *CLPN* by counting the number of English words with same or higher lexical frequency, and whose phonological transcription (in X-SAMPA format) different in up to one phoneme from that of the presented Catalan or Spanish word. Lexical frequencies and phonological transcriptions were extracted from the multilingual database CLEARPOND [@marian2012clearpond]^[[Phonological transcriptions in CLEARPOND were generated from eSPEAK, [http://espeak.sourceforge.net/](http://espeak.sourceforge.net/)]].


```{r tbl-stimuli}
#| label: tbl-stimuli
#| tbl-cap: "Stimuli details."
stimuli |>
  summarise(
    across(c(freq_zipf_2, lv, neigh_n_h),
      lst(mean, sd, min, max),
      na.rm = TRUE
    ),
    .by = c(group)
  ) |>
  gt(rowname_col = "group") |>
  tab_spanner("Frequency", matches("freq")) |>
  tab_spanner("Similarity", matches("lv")) |>
  tab_spanner("CLPN", matches("neigh")) |>
  fmt_number(is.numeric) |>
  fmt_integer(c(
    neigh_n_h_min,
    neigh_n_h_max
  )) |>
  cols_merge_uncert(freq_zipf_2_mean, freq_zipf_2_sd) |>
  cols_merge_uncert(lv_mean, lv_sd) |>
  cols_merge_uncert(neigh_n_h_mean, neigh_n_h_sd) |>
  cols_merge_range(freq_zipf_2_min, freq_zipf_2_max) |>
  cols_merge_range(lv_min, lv_max) |>
  cols_merge_range(neigh_n_h_min, neigh_n_h_max) |>
  cols_label(
    freq_zipf_2_mean = "Mean ± SD",
    freq_zipf_2_sd = "SD",
    freq_zipf_2_min = "Range",
    lv_mean = "Mean ± SD",
    lv_sd = "SD",
    lv_min = "Range",
    neigh_n_h_mean = "Mean ± SD",
    neigh_n_h_sd = "SD",
    neigh_n_h_min = "Range"
  ) |>
  tab_style(
    cell_text(weight = "bold"),
    cells_column_spanners()
  ) |>
  tab_style(
    cell_text(style = "italic"),
    cells_column_labels()
  ) |>
  grand_summary_rows(
    columns = matches("mean|sd"),
    fns = list(
      label = md("**Mean**"),
      id = "totals",
      fn = "mean"
    ),
    fmt = ~ fmt_number(., n_sigfig = 3, suffixing = TRUE)
  )
```


### Procedure

The experiment was implemented online using Psychopy/Pavlovia [@peirce2019psychopy2]. Participants accessed the study from a link provided by Prolific or SONA and completed the experiment from an internet browser (Chrome or Mozilla). After giving their consent for participating, participants answered a series of questions about their demographic status, their language background, and the set up they were using for completing the study. Then, participants completed the experimental task. Participants were informed that they would listen to a series of pre-recorded words in either Catalan or Spanish (English participants) or Catalan (Spanish participants). They were instructed to listen to each word, guess its meaning in English (English participants) or Spanish (Spanish participants), and type their answer as soon as possible. English participants were randomly assigned to the Catalan or Spanish lists. Participants in the Catalan list were presented with `r nrow(stimuli[stimuli$group=="cat-ENG",])` trials, and participants in the Spanish list were presented with `r nrow(stimuli[stimuli$group=="spa-ENG",])` trials.

Each trial started with a yellow fixation point presented during one second on the centre of the screen over a black background. After one second, the audio started playing while the dot remained being displayed until the audio ended. Upon the offset of the fixation point and audio, participants were prompted to write their answer by a `>` symbol. Typed letters were displayed in the screen in real time to provide visual feed-back to participants. Participants were allowed to correct their answer. Then, participants pressed the `RETURN/ENTER` key to confirm their answer and start and new trial.

![Schematic representation of a trial in the experimental task. The trial stars with the presentation of a fixation point in the centre of the screen (yellow dot). After 1,000 ms, the auditory word was presented while the fixation point remained on screen. After the offset of the audio, the fixation point disappeared and a visual prompt (`>`) was presented. Participants then wrote their and clicked `RETURN`, making the end of the trial.](_assets/img/design.png)


### Data analysis

#### Data processing

After data collection, participants' answers were manually coded into the following categories: *Correct*, *Typo*, *Wrong*, *False friend*, *Other*. A response was coded as *Correct* if the provided string of characters was identical to the orthographic form of the correct translation. A response was coded as *Typo* if the participant provided a string of characters only one edit distance (addition, deletion, or substitution) apart from the orthographic form of the correct translation (e.g., "pengiun" instead of "penguin"), as long as the response did not correspond to a distinct English word. A response was coded as *False friend* if the participant provided a phonologically similar incorrect translation. Responses not meeting the criteria for previous categories were labelled as *Wrong* or *Other* (see Data analysis section for more details). Both *Correct* and *Typo* responses were considered as correct, while *Wrong* and *False friend* responses were considered as incorrect. *Other* responses were excluded from data analysis. Trials in which participants took longer than 10 seconds to respond were also excluded. Participants contributed a total of `r format(nrow(exp_responses), big.mark = ",")` valid trials (`r format(nrow(exp_responses[exp_responses$group %in% c("cat-ENG", "cat-SPA"),]), big.mark = ",")` in Catalan, `r format(nrow(exp_responses[exp_responses$group %in% c("spa-ENG"),]), big.mark = ",")` in Spanish). The task took approximately 15 minutes to be completed.

#### Modelling approach and statistical inference

We modelled the probability of participants guessing the correct translation of each presented word using a generalised multilevel Bayesian regression model with a Bernoulli logit link distribution. We included as fixed effects the intercept, the main effects of *Frequency*, *Similarity*, and *CLPN*, and the two-way interaction between *Similarity* and *CLPN*. We also included participant-level random intercepts and slopes for the the main effects and the interaction. Eq. 1 shows a formal description of the model.

::: {.content-visible when-format="docx"}
![Eq. 1](_assets/img/model.PNG)
:::

::: {.content-visible when-format="pdf"}
$$
\begin{aligned}
&\textbf{Likelihood}  \\
y_{i} \sim & \text{Bernoulli}(p_{i}) \\ \\
&\textbf{Parameters}  \\
\text{Logit}(p_{i}) = &  \beta_{0[p,w]} + \beta_{1[p]} \text{Frequency}_{i} + \beta_{2[p]} \text{PTHN}_i + \beta_{3[p]} \text{Similarity}_i + \beta_{4[p]} (\text{PTHN}_i \times \text{Similarity}_i) \\
\beta_{0-6[p,w]} \sim & \mathcal{N}(\mu_{\beta_{j}}, \sigma_{\beta_{j}}) \text{, for participant } p \text{ in 1, ..., } P \text{ and  word } w \text{ in 1, ..., } W \\
\beta_{1-6[p]} \sim &  \mathcal{N}(\mu_{\beta_{j}}, \sigma_{\beta_{j}}) \text{, for participant } p \text{ in 1, ..., } P \\ \\
&\textbf{Prior}  \\
\mu_{\beta_{p,w}}  \sim &  \mathcal{N}(0, 0.1) \\
\sigma_{\beta_{p}},  \sigma_{\beta_{w}} \sim & \text{HalfCauchy}(0, 0.1) \\
\rho_{p}, \rho_{w} \sim & \text{LKJCorr}(8) \\
\end{aligned}
$$ {#eq-1}

::: 

To test the practical relevance of each predictor we followed @kruschke2018bayesian. We first specified a region of practical equivalence (ROPE) around zero ([-0.1, +0.1], in the logit scale). This area indicates the values of the regression coefficients that we considered as equivalent to zero. We then computed the 95% posterior credible intervals (CrI) of the regression coefficients of interest. Finally, we calculated the proportion of the 95% CrI that fell inside the ROPE. This proportion indicates the probability that the true value of the coefficient is equivalent to zero. All analyses were performed in R environment [@rcoreteam2013language]. We used the tidyverse family of R packages [@wickham2019welcome] to process data and to generate figures. We used the `brms` R package [@burkner2017brms] using the `cmdstanr` back-end to the Stan probabilistic language [@carpenter2017stan] to estimate and compare the models (see Appendix 1 for mode details on the models).


## Results

```{r dataset-1}
#| label: dataset-1
dataset <- exp_responses |>
  filter(group != "cat-SPA") |>
  group_split(group) |>
  set_names(c("spa-ENG", "cat-ENG")) |>
  janitor::clean_names()

dataset <- split(exp_responses, exp_responses$group)
names(dataset) <- janitor::make_clean_names(names(dataset))
r_validity <- map(
  dataset,
  \(x) janitor::clean_names(table(x$response_type))
)
n_participants <- map(dataset, \(x) dplyr::n_distinct(x$participant_id))

excluded_lang <- participants_1 |>
  filter(l2_writ_prod > 3 |
    l2_oral_comp > 3 |
    cat_oral_comp > 3 |
    cat_writ_prod > 3 |
    spa_oral_comp > 3 |
    spa_writ_prod > 3) |>
  nrow()

excluded_impairment <- participants_1 |>
  filter(valid_status == "Language impairment") |>
  nrow()

excluded_ntrials <- participants_1 |>
  filter(valid_status == "Insufficient trials") |>
  nrow()

dataset_clean <- exp_responses |>
  filter(group != "cat-SPA") |>
  dplyr::filter(valid_participant, valid_response) |>
  group_split(group) |>
  set_names(c("spa-ENG", "cat-ENG")) |>
  janitor::clean_names()

lengths <- dataset_1 |>
  mutate(len = nchar(response)) |>
  summarise(
    across(
      len,
      lst(mean, median, sd, min, max)
    ),
    .by = c(group)
  )
lengths <- split(lengths, lengths$group)

names(lengths) <- janitor::make_clean_names(names(lengths))
```

We collected data for a total of `r format(nrow(dataset$cat_eng) + nrow(dataset$spa_eng), big.mark = ",")` trials completed by `r n_participants$cat_eng + n_participants$spa_eng` unique participants. Of those trials, `r format(nrow(dataset$cat_eng), big.mark = ",")` were provided by `r n_participants$cat_eng` unique participants who listened to Catalan words, and `r format(nrow(dataset$spa_eng), big.mark = ",")` trials were provided by `r n_participants$spa_eng` unique participants who listened to Spanish words. We excluded trials in which participants did not enter any text (*n* = `r r_validity$cat_eng["blank"]`), in which a response in a language other than English was provided (e.g., "`agua`", *n* = `r r_validity$cat_eng["language"]`), in which participants did not provide a whole word (e.g., "`f`", *n* = `r r_validity$cat_eng["incomplete"]`), and in which participants added comments to the experimenter (e.g., "`unsure`", *n* = `r r_validity$cat_eng["other"]`). In addition, we excluded data from participants that self-rated their oral and/or written skills in Catalan and Spanish, or any other second language as four or higher in a five-point scale (*n* = `r excluded_lang`), were diagnosed with a language (*n* = `r excluded_impairment`), or did not contribute more than 80% of valid trials (*n* = `r excluded_ntrials`).

After applying trial-level and participant-level inclusion criteria, the resulting dataset included `r format(nrow(dataset_clean$cat_eng) + nrow(dataset_clean$cat_eng), big.mark = ",")` trials provided by `r dplyr::n_distinct(dataset_clean$cat_eng$participant_id) + dplyr::n_distinct(dataset_clean$cat_eng$participant_id)` unique participants. Of those trials, `r format(nrow(dataset_clean$cat_eng), big.mark = ",")` were provided by `r dplyr::n_distinct(dataset_clean$cat_eng$participant_id)` unique participants who listened to Catalan words, and `r format(nrow(dataset_clean$spa_eng), big.mark = ",")` trials were provided by `r dplyr::n_distinct(dataset_clean$spa_eng$participant_id)` unique participants who listened to Spanish words. Responses given by English participants to Catalan presented words were `r round(lengths$cat_eng$len_mean, 2)` characters long on average (*Median* = `r round(lengths$cat_eng$len_median, 2)`, *SD* = `r round(lengths$cat_eng$len_sd, 2)`, Range = `r round(lengths$cat_eng$len_min, 2)`-`r round(lengths$cat_eng$len_max, 2)`), while their translations to Spanish responses were `r round(lengths$spa_eng$len_mean, 2)` characters long on average (*Median* = `r round(lengths$spa_eng$len_median, 2)`, *SD* = `r round(lengths$spa_eng$len_sd, 2)`, Range = `r round(lengths$spa_eng$len_min, 2)`-`r round(lengths$spa_eng$len_max, 2)`).



```{r coefs-1}
c1 <- gather_draws(exp_1_m0, `b_.*`, `sd_.*`, regex = TRUE) |>
  summarise(across(.value, lst(
    .median = median,
    .lower = \(x) hdi(x)[1],
    .upper = \(x) hdi(x)[2],
    .rope = \(x) mean(between(x, -0.1, 0.1))
  ),
  .names = "{fn}"
  ))
c1 <- split(c1, c1$.variable)
names(c1) <- janitor::make_clean_names(names(c1))
```

@tbl-dataset shows a summary of participants' accuracy across Experiments 1, 2, and 3. MCMC chains in the model showed strong evidence of convergence ($\hat{R}<1.01$) (see Appendix 2 for more detailed model diagnostics). Participants translating Catalan words and participants translating Spanish words performed equivalently, as indicated by the regression coefficient of *Group* ($\beta$ = `r round(c1$b_group1$.median, 3)`, 95% CrI = [`r round(c1$b_group1$.lower, 3)`, `r round(c1$b_group1$.upper, 3)`], *p*(ROPE) = `r round(c1$b_group1$.rope, 3)`). Overall, participants responded less accurately to words with more CLPNs than to words with fewer CLPNs, regardless of the amount of phonological similarity between the presented word and its translation. This is indicated by the size of the regression coefficient of the two-way interaction between *Similarity* and *CLPN* ($\beta$ = `r round(c1$b_neigh_n_h_lv$.median, 3)`, 95% CrI = [`r round(c1$b_neigh_n_h_lv$.lower, 3)`, `r round(c1$b_neigh_n_h_lv$.upper, 3)`], *p*(ROPE) = `r round(c1$b_neigh_n_h_lv$.rope, 3)`). As anticipated, participants' performance benefited from an increase in *Similarity* ($\beta$ = `r round(c1$b_lv$.median, 3)`, 95% CrI = [`r round(c1$b_lv$.lower, 3)`, `r round(c1$b_lv$.upper, 3)`], *p*(ROPE) = `r round(c1$b_lv$.rope, 3)`), while the number of *CLND* had the opposite effect ($\beta$ = `r round(c1$b_neigh_n_h$.median, 3)`, 95% CrI = [`r round(c1$b_neigh_n_h$.lower, 3)`, `r round(c1$b_neigh_n_h$.upper, 3)`], *p*(ROPE) = `r round(c1$b_neigh_n_h$.rope, 3)`). @fig-epreds-1 illustrates the posterior of the average predictions of the model for words with different values of *Similarity* and *CLPN*.

```{r tbl-dataset}
#| label: tbl-dataset
sem <- function(x) mean(x) / (sqrt(length(x)))

#' Proportion adjusted from boundary values (Gelman, Hill & Vehtari, 2020)
#'
prop_adj <- function(x, n) {
  e <- (x + 2) / (n + 4)
  return(e)
}

#' Adjusted standard error of proportion (Gelman, Hill & Vehtari, 2020)
#'
prop_adj_se <- function(x, n) {
  e <- (x + 2) / (n + 4)
  se <- sqrt(e * (1 - e) / (n + 4))
  return(se)
}

#' Adjusted standard error of proportion (Gelman, Hill & Vehtari, 2020)
#'
prop_adj_ci <- function(x, n, .width = 0.95) {
  e <- (x + 2) / (n + 4)
  se <- sqrt(e * (1 - e) / (n + 4))
  ci <- e + qnorm(c((1 - .width) / 2, (1 - (1 - .width) / 2))) * se
  ci[1] <- ifelse(ci[1] < 0, 0, ci[1]) # truncate at 0
  ci[2] <- ifelse(ci[2] > 1, 1, ci[2]) # truncate at 1
  return(ci)
}

tbl_data <- list(
  "Experiment 1" = dataset_1,
  "Experiment 2" = dataset_2,
  "Experiment 3" = dataset_3
) |>
  bind_rows(.id = "exp") |>
  add_count(exp, group, participant_id, name = "trials") |>
  summarise(
    n_trials = n(),
    correct = sum(correct),
    .by = c(group, exp, participant_id)
  ) |>
  mutate(correct = prop_adj(correct, n_trials)) |>
  summarise(across(correct, lst(mean, sd, sem, min, max)),
    across(n_trials, lst(mean, sum, sd, min, max)),
    n_participants = dplyr::n_distinct(participant_id),
    .by = c(group, exp)
  ) |>
  relocate(n_participants, matches("correct"))

tbl_data |>
  gt(
    rowname_col = "group",
    groupname_col = "exp"
  ) |>
  fmt_number(is.numeric, decimals = 2) |>
  # gtExtras::gt_plt_dist(correct_list, type = "density") |>
  fmt_integer(c(matches("sum|min|max"), n_participants)) |>
  fmt_number(matches("correct"), scale_by = 100) |>
  tab_spanner("Accuracy (%)", matches("correct")) |>
  tab_spanner("Valid trials", matches("n_trials")) |>
  cols_merge_range(n_trials_min, n_trials_max) |>
  cols_merge_range(correct_min, correct_max) |>
  cols_label(
    n_trials_sum = "N trials",
    n_participants = "N",
    n_trials_mean = "Mean",
    n_trials_sd = "SD",
    n_trials_min = "Range",
    correct_mean = "Mean",
    correct_sd = "SD",
    correct_sem = "SE",
    correct_min = "Range"
  ) |>
  summary_rows(
    columns = is.integer,
    fns = list(
      label = md("*Sum*"),
      id = "totals",
      fn = "sum"
    ),
    fmt = ~ fmt_integer(.)
  ) |>
  summary_rows(
    columns = matches("mean|sd|sem"),
    fns = list(
      label = md("*Mean*"),
      id = "means",
      fn = "mean"
    ),
    fmt = ~ fmt_number(., scale_by = 100)
  ) |>
  tab_style(
    cell_text(align = "center"),
    cells_column_labels()
  ) |>
  tab_style(
    cell_text(style = "italic"),
    cells_column_labels()
  ) |>
  tab_style(
    cell_text(weight = "bold"),
    cells_column_spanners()
  ) |>
  tab_style(
    cell_text(align = "left"),
    list(cells_body(), cells_column_labels())
  )
```


```{r fig-epreds-1}
#| label: fig-epreds-1
#| cache: true
#| fig-height: 5
#| fig-width: 11
#| fig-cap: "Posterior model-predicted mean accuracy in Experiment 1. Predictions were generated from 4,000 posterior samples, extracted for different values of *CLPN* (0, 2, 4, 8, 12) and *Similarity* (1-100). Predictions are plotted separately for English participants translating Catalan words, and for English participants translating Spanish words. Lines indicate mean predictions, and intervals indicate 95%, 89%, 78%, 67%, and 50% credible intervals (CrI)."
get_epreds <- function(model, data, n = 1000,
                       lv = seq(0, 1, length.out = 100),
                       neigh_n_h = c(0, 2, 4, 8, 12),
                       ...) {
  knowledge <- unique(model$data$knowledge)
  confidence <- unique(model$data$confidence)
  freq_zipf_2 <- mean(data$freq_zipf_2, na.rm = TRUE)
  group <- unique(model$data$group)
  nd <- expand_grid(freq_zipf_2, neigh_n_h, lv, knowledge, confidence, group)
  epreds <- tidybayes::add_epred_draws(nd, model, re_formula = NA, ...)

  return(epreds)
}

get_epreds(exp_1_m0, dataset_1) |>
  mutate(
    neigh_n_h = paste0(neigh_n_h, " neighbours"),
    neigh_n_h = factor(neigh_n_h,
      levels = paste0(
        c(0, 2, 4, 8, 12),
        " neighbours"
      ),
      ordered = TRUE
    )
  ) |>
  ggplot(aes(lv, .epred)) +
  facet_grid(group ~ neigh_n_h) +
  stat_lineribbon(aes(fill_ramp = after_stat(level)),
    linewidth = 1 / 2,
    .width = c(0.95, 0.89, 0.78, 0.67, 0.50)
  ) +
  labs(
    x = "Similarity\n(Levenshtein distance with correct translation)",
    y = "p(Correct)",
    colour = "Cross-language neighbourhood density",
    fill = "Cross-language neighbourhood density",
    linetype = "Cross-language neighbourhood density",
    fill_ramp = "CrI (%)"
  ) +
  scale_x_continuous(labels = scales::percent) +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_brewer(palette = "Reds") +
  theme(
    legend.position = "none",
    legend.box = "vertical"
  )
```


## Discussion

In Experiment 1, we investigated the extent to which the phonological similarity between translation equivalents is sufficient for successful word translation, in the absence of conceptual knowledge about the presented word. We tested two groups of monolingual British English-native adults in a translation task that involved words in Catalan or Spanish, two languages participants reported having no prior familiarity with. Participants benefited strongly from phonological similarity when the correct translation of the presented words in Catalan or Spanish had few English phonological neighbours with higher lexical frequency. This suggests that word-forms in an unfamiliar language have a strong potential to activate their translation equivalents in the native language, provided some phonological similarity between both words, and the absence of more frequent phonological neighbors.

Participants in Experiment 1 were surprisingly good at translating words from Catalan and Spanish (two unfamiliar languages) to their native language. If English participants were likely to activate the correct English translations of the presented words in Catalan and Spanish, it is possible that speakers of typologically closer languages to Catalan and Spanish may benefit even more strongly from phonological similarity in the same task. English is a Germanic language (like Dutch or German), while Catalan and Spanish are Romance languages (like Italian, French, Portuguese). English shares fewer phonologically similar translations with Romance languages than Romance languages share with each other. It is possible that the probability of homophonic translations is higher when listening to an unfamiliar language from the same typological family as the native language. We tested this hypothesis in Experiment 2.


# Experiment 2

Results in Experiment 1 suggest that English natives were able to exploit the phonological similarity between unfamiliar words in Catalan and Spanish to provide accurate translations to English. English, a Germanic language, is relatively distant from Catalan and Spanish, two Romance languages. English shares fewer cognates with Catalan and Spanish than it does with typologically closer languages, like Dutch and German. In Experiment 2, we investigated whether listeners of an unfamiliar but typologically closer language benefit more strongly from phonological similarity when performing the same task as in Experiment 1. To this aim, we presented Spanish participants, who reported little-to-no prior familiarity with Catalan, with Catalan words.

## Methods

```{r participants-numbers-2}
#| label: participants-numbers-2
participants_2 <- exp_participants |>
  filter(group == "cat-SPA")

collection_dates <- participants_2 |>
  pull(date) |>
  range() |>
  format("%B %dth, %Y")

n_participants <- dplyr::n_distinct(participants_2)
n_participants_group <- table(participants_2$group)
participants_age <- participants_2 |>
  summarise(across(age, lst(mean, sd, min, max)))
n_female <- table(participants_2$gender)
```

We collected data from `r n_participants` Spanish native adults living in Spain (*Mean* = `r round(participants_age$age_mean, 2)` years, *SD* = `r round(participants_age$age_sd, 2)`, Range = `r round(participants_age$age_min, 2)`-`r round(participants_age$age_max, 2)`, `r n_female` female). Data collection took place from `r collection_dates[1]` to `r collection_dates[2]`. Participants in Spain were contacted via announcements at the University campus(es), and were compensated 5€ or an Amazon voucher for the same value. Participants gave informed consent before providing any data and the study was conducted in accordance with ethical standards of the Declaration of Helsinki and the protocol was approved by the Drug Research Ethical Committee (CEIm) of the IMIM Parc de Salut Mar (2020/9080/I).

Stimuli were the same list of Catalan stimuli as in Experiment 1. Procedure and data analysis were identical as in Experiment 1.

## Results

```{r dataset-2}
#| label: dataset-2
dataset <- exp_responses |>
  filter(group == "cat-SPA") |>
  group_split(group) |>
  set_names(c("cat-SPA")) |>
  janitor::clean_names()

dataset <- split(exp_responses, exp_responses$group)
names(dataset) <- janitor::make_clean_names(names(dataset))
r_validity <- map(
  dataset,
  \(x) janitor::clean_names(table(x$response_type))
)
n_participants <- map(dataset, \(x) dplyr::n_distinct(x$participant_id))

excluded_lang <- participants_2 |>
  filter(l2_writ_prod > 3 |
    l2_oral_comp > 3 |
    cat_oral_comp > 3 |
    cat_writ_prod > 3 |
    spa_oral_comp > 3 |
    spa_writ_prod > 3) |>
  nrow()

excluded_impairment <- participants_1 |>
  filter(valid_status == "Language impairment") |>
  nrow()

excluded_ntrials <- participants_1 |>
  filter(valid_status == "Insufficient trials") |>
  nrow()

dataset_clean <- exp_responses |>
  filter(group == "cat-SPA") |>
  dplyr::filter(valid_participant, valid_response) |>
  group_split(group) |>
  set_names(c("cat-SPA")) |>
  janitor::clean_names()

lengths <- dataset_2 |>
  mutate(len = nchar(response)) |>
  summarise(
    across(
      len,
      lst(mean, median, sd, min, max)
    ),
    .by = c(group)
  )
```

We collected data for a total of `r format(nrow(dataset$cat_spa) + nrow(dataset$cat_spa), big.mark = ",")` trials completed by `r n_participants$cat_spa` unique participants. We excluded trials in which participants did not enter any text (*n* = `r r_validity$cat_spa["blank"]`), in which a response in a language other than Spanish was provided (*n* = `r r_validity$cat_spa["language"]`), in which participants did not provide a whole word (*n* = `r r_validity$cat_spa["incomplete"]`), and in which participants added comments to the experimenter (*n* = `r r_validity$cat_spa["other"]`). In addition, we excluded data from participants that self-rated their oral and/or written skills in Catalan and Spanish, or any other second language as four or higher in a five-point scale (*n* = `r excluded_lang`), were diagnosed with a developmental language disorder (*n* = `r excluded_impairment`), or did not contribute more than 80% of valid trials (*n* = `r excluded_ntrials`). After applying trial-level and participant-level inclusion criteria, the resulting dataset included `r format(nrow(dataset_clean$cat_spa), big.mark = ",")` trials provided by `r dplyr::n_distinct(dataset_clean$cat_spa$participant_id) + dplyr::n_distinct(dataset_clean$cat_spa$participant_id)` unique participants. Of those trials, `r format(nrow(dataset_clean$cat_spa), big.mark = ",")` were provided by `r dplyr::n_distinct(dataset_clean$cat_spa$participant_id)` unique participants who listened to Catalan words, and `r format(nrow(dataset_clean$cat_spa), big.mark = ",")` trials were provided by `r dplyr::n_distinct(dataset_clean$cat_spa$participant_id)` unique participants who listened to Spanish words. Responses given by participants were `r round(lengths$len_mean, 2)` characters long on average (*Median* = `r round(lengths$len_median, 2)`, *SD* = `r round(lengths$len_sd, 2)`, Range = `r round(lengths$len_min, 2)`-`r round(lengths$len_max, 2)`).


```{r posterior-2}
c2 <- gather_draws(exp_2_m0, `b_.*`, `sd_.*`, regex = TRUE) |>
  summarise(across(.value, lst(
    .median = median,
    .lower = \(x) hdi(x)[1],
    .upper = \(x) hdi(x)[2],
    .rope = \(x) mean(between(x, -0.1, 0.1))
  ),
  .names = "{fn}"
  ))
c2 <- split(c2, c2$.variable)
names(c2) <- janitor::make_clean_names(names(c2))
```

MCMC chains in the model showed strong evidence of convergence ($\hat{R}<1.01$) (see Appendix 2 for more detailed model diagnostics). Overall, participants responded less accurately to words with more CLPNs than to words with fewer CLPNs, regardless of the amount of phonological similarity between the presented word and its translation. This is indicated by the size of the regression coefficient of the two-way interaction between *Similarity* and *CLPN* ($\beta$ = `r round(c2$b_neigh_n_h_lv$.median, 3)`, 95% CrI = [`r round(c2$b_neigh_n_h_lv$.lower, 3)`, `r round(c2$b_neigh_n_h_lv$.upper, 3)`], *p*(ROPE) = `r round(c2$b_neigh_n_h_lv$.rope, 3)`). As anticipated, participants' performance benefited from an increase in *Similarity* ($\beta$ = `r round(c2$b_lv$.median, 3)`, 95% CrI = [`r round(c2$b_lv$.lower, 3)`, `r round(c2$b_lv$.upper, 3)`], *p*(ROPE) = `r round(c2$b_lv$.rope, 3)`), while the number of *CLND* had the opposite effect ($\beta$ = `r round(c2$b_neigh_n_h$.median, 3)`, 95% CrI = [`r round(c2$b_neigh_n_h$.lower, 3)`, `r round(c2$b_neigh_n_h$.upper, 3)`], *p*(ROPE) = `r round(c2$b_neigh_n_h$.rope, 3)`). @fig-epreds-1 illustrates the posterior of the average predictions of the model for words with different values of *Similarity* and *CLPN*.

```{r fig-epreds-2}
#| label: fig-epreds-2
#| cache: true
#| fig-cap: "Posterior model-predicted mean accuracy in Experiment 1. Predictions were generated from 4,000 posterior samples, extracted for different values of *CLPN* (0, 2, 4, 8, 12) and *Similarity* (1-100). Predictions are plotted separately for English participants translating Catalan words, and for English participants translating Spanish words. Lines indicate mean predictions, and intervals indicate 95%, 89%, 78%, 67%, and 50% credible intervals (CrI)."
#| fig-height: 3.5
#| fig-width: 10
get_epreds(exp_2_m0, dataset_2) |>
  mutate(
    neigh_n_h = paste0(neigh_n_h, " neighbours"),
    neigh_n_h = factor(neigh_n_h,
      levels = paste0(
        c(0, 2, 4, 8, 12),
        " neighbours"
      ),
      ordered = TRUE
    )
  ) |>
  ggplot(aes(lv, .epred)) +
  facet_wrap(~neigh_n_h, nrow = 1) +
  stat_lineribbon(aes(fill_ramp = after_stat(level)),
    linewidth = 1 / 2,
    .width = c(0.99, 0.95, 0.89, 0.78, 0.67, 0.50)
  ) +
  labs(
    x = "Similarity\n(Levenshtein distance with correct translation)",
    y = "p(Correct)",
    colour = "Cross-language neighbourhood density",
    fill = "Cross-language neighbourhood density",
    linetype = "Cross-language neighbourhood density",
    fill_ramp = "CrI (%)"
  ) +
  scale_x_continuous(labels = scales::percent) +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_brewer(palette = "Reds") +
  theme(
    legend.position = "none",
    legend.box = "vertical"
  )
```

## Discussion

Experiment 2 was an extension of Experiment 1 to a population of monolinguals whose native language is typologically close to the presented language. We presented Catalan words to Spanish native adults who were reportedly unfamiliar with Catalan. Our results indicate a similar pattern of results as those in Experiment 1: participants were able to provide correct translations of presented Catalan words, provided that the Catalan words shared some degree of phonological similarity with their Spanish translation, and that the number of phonological neighbours with higher lexical frequency was reduced. In contrasts with the results in Experiment 1, the positive impact of phonological similarity on participants' performance in Experiment 2 was more resilient to the interference of phonological neighbourhood size. English natives in Experiment 1 provided significantly less accuracy responses when more than four phonological neighbors were present (even when translating high-similarity words), compared to when only one or neighbour were present. Spanish participants in Experiment 2 benefited from phonological similarity, even when eight neighbours were present. Spanish participants' performance declined after 8 neighbours, and was evident at 12 neighbours. Overall, this suggests that participants in Experiment 2, who were natives of a typologically similar language (Spanish) to the presented language (Catalan) benefited more strongly from phonological similarity than participants in Experiment 1, who were natives of typologically less similar language (English) to the presented language (Catalan, Spanish).

Participants from both Experiment 1 and 2 benefited strongly from phonological similarity to correctly translate words from a non-native, reportedly unfamiliar language. This pattern of results holds for most of the presented stimuli, but some low-similarity Catalan and Spanish words were responded to surprisingly accurately by English listeners. Given that participants were reportedly unfamiliar with both languages, it was expected that participants would be very unlikely to provide correct translations for words sharing little to no phonological similarity to their correct translation. @tbl-surprises a list of Catalan and Spanish words to which participants provided responses with $\geq$ 10 average accuracy.

```{r tbl-surprises}
#| label: tbl-surprises
#| tbl-cap: "List of items with unexpectedly high accuracy: the Levenshtein similarityscore betwen the presented word (in Catalan or Spanish) and their correct English translation is zero, but participants, who are reportedly unfamiliar with the presented language, were on average >10% likely to guess the correct translation."
ipa_to_tipa <- function(ipa) {
  dict <- c(
    "ɑ" = "A", 
    "ɒ" = "6",
    "æ" = "\\ae", 
    "ʌ" = "2", 
    "β" = "B",
    "ʤ" = "}\\\\textdyoghlig\\\\textipa{",
    "d͡ʒ" = "}\\\\textdyoghlig\\\\textipa{",
    "dʒ" = "}\\\\textdyoghlig\\\\textipa{", 
    "ð" = "D",
    "ə" = "@",
    "ɛ" = "E",
    "ɣ" = "G",
    "ɡ" = "g",
    "ɫ" = "\\|~l", 
    "ʎ" = "}\\\\textlambda\\\\textipa{",
    "ŋ" = "N",
    "ɲ" = "}\\\\textltailn\\\\textipa{", 
    "ɔ" = "O",
    "ɾ" = "R",
    "ʧ" = "}\\\\textteshlig\\\\textipa{", 
    "tʃ" = "}\\\\textteshlig\\\\textipa{", 
    "t͡ʃ" = "}\\\\textteshlig\\\\textipa{", 
    "θ" = "T", 
    "t͡s" = "}\\\\texttslig\\\\textipa{", 
    "ts" = "}\\\\texttslig\\\\textipa{", 
    "ʃ" = "S", 
    "ʒ" = "Z", 
    "ɪ" = "I", 
    "ʊ" = "U", 
    "ɜ" = "3", 
    "ɹ" = "\\*r", 
    "ː" = ":", 
    "ˈ" = '"'
  )
  tipa <- stringr::str_replace_all(ipa, dict)
  tipa <- paste0("\\textipa{", tipa, "}")
  return(tipa)
}


tbl_data <- list(
  "Experiment 1" = dataset_1,
  "Experiment 2" = dataset_2
) |>
  bind_rows(.id = "exp") |>
  mutate(
    translation = paste0(word_1, " - ",  word_2),
    ipa = ipa_to_tipa(paste0("/", ipa_1, "/ - /", ipa_2, "/"))
  ) |> 
  summarise(
    n = n(),
    correct = sum(correct),
    .by = c(exp, group, lv, translation, ipa)
  ) |>
  mutate(
    accuracy = prop_adj(correct, n),
    accuracy_se = prop_adj_se(correct, n)
  ) |>
  dplyr::filter(lv == 0, accuracy >= 0.1) |>
  select(exp, translation, ipa, group, accuracy, accuracy_se) |>
  arrange(exp, group, desc(accuracy)) |> 
  select(translation, ipa, accuracy, accuracy_se) |> 
  mutate(accuracy = round(accuracy * 100, 2),
         accuracy_se = round(accuracy_se * 100, 2))

names(tbl_data) <- c("Translation", "IPA", "Accuracy (\\%)", "SE")

tt(tbl_data) |> 
  style_tt(i = 0, bold = TRUE, line = "tb") |> 
  group_tt(i = list(
    "Experiment 1 (cat-ENG)" = 1,
    "Experiment 2 (spa-ENG)" = 6,
    "Experiment 3 (cat-SPA)" = 22)) |> 
  style_tt(i = 1, bold = TRUE, line = "b") |> 
  style_tt(i = 7, bold = TRUE, line = "tb") |> 
  style_tt(i = 24, bold = TRUE, line = "tb")
```


It is likely that participants had prior knowledge of these words despite having reported little to no familiarity with the presented language (Catalan or Spanish). One possibility is that participants had previously encountered these words embedded in English linguistic input. Spanish words percolate English speech with relative frequency, via different sources such as popular culture, songs, TV programs, etc. In addition, words from languages other than Spanish or Catalan, but with high similarity to the Spanish or Catalan words (e.g., cognates from Italian or French) might appear in English speech as well. Such prior knowledge might not be specific to the low-similarity words highlighted before. Participants may also have had prior knowledge about higher-similarity words, which could have contributed to participants responding to such words more accurately than without such prior knowledge. In the case of higher-similarity words, it is more difficult to disentangle the extent to which participants' accuracy is a function of pure phonological similarity, or prior knowledge they had about the meaning of Spanish words. Experiment 3 was addressed at investigating this issue.

# Experiment 3

Experiment 3 is a replication of Experiment 1, in which we collected additional data about participants' prior familiarity with the presented Catalan and Spanish words, in addition to the same translation task presented to participants in Experiment 1.

## Methods

```{r participants-numbers-3}
#| label: participants-numbers-3
participants_3 <- quest_participants |>
  filter(group != "cat-SPA")

collection_dates <- participants_3 |>
  pull(date) |>
  range() |>
  format("%B %dth, %Y")

n_participants <- dplyr::n_distinct(participants_3)
n_participants_group <- table(participants_3$group)
participants_age <- participants_3 |>
  summarise(across(age, lst(mean, sd, min, max)))
n_female <- table(participants_3$gender)
```

We collected data from `r n_participants` British English native adults living in United Kingdom (*Mean* = `r round(participants_age$age_mean, 2)` years, *SD* = `r round(participants_age$age_sd, 2)`, Range = `r round(participants_age$age_min, 2)`-`r round(participants_age$age_max, 2)`, `r n_female` female). Data collection took place from `r collection_dates[1]` to `r collection_dates[2]`. Participants were recruited via Prolific (5£ compensation) and SONA (compensation in academic credits), and gave informed consent before providing any data and the study was conducted in accordance with ethical standards of the Declaration of Helsinki and the protocol was by the University of Oxford Medical Sciences Inter-Divisional Research Ethics Committee (IDREC) (R60939/RE007). Participants were asked to complete the experiment using a laptop in a quiet place with good internet connection. Stimuli were the same list of Catalan stimuli as in Experiment 1. 



```{r participants-excluded-3}
#| label: participants-excluded-3
excluded_lang <- participants_3 |>
  filter(l2_writ_prod > 3 |
    l2_oral_comp > 3 |
    cat_oral_comp > 3 |
    cat_writ_prod > 3 |
    spa_oral_comp > 3 |
    spa_writ_prod > 3) |>
  nrow()

excluded_impairment <- participants_3 |>
  filter(valid_status == "Language impairment") |>
  nrow()

excluded_ntrials <- participants_3 |>
  filter(valid_status == "Insufficient trials") |>
  nrow()
```


The experiment was implemented online using Qualtrics (Qualtrics, Provo, UT). This platform was chosen to allow easier presentation of survey questions aimed to probe prior understanding of the presented words and participants’ confidence ratings of their answers. With the exception of these additional questions, we attempted to replicate the procedure of Experiment 1 as closely as possible. The Spanish and Catalan audio stimuli used were identical the materials in Experiment 1. Participants were randomly assigned to the Catalan or Spanish lists. The Catalan list had 83 trials and the Spanish list had 99 trials. Participants first completed the consent form followed by the questionnaire about demographic status, language background and set up. They then proceeded to the experimental task.

In each trial, participants listened to the audio stimulus by clicking on the `PLAY` button. For comparability to the PsychoPy version, participants were only allowed to play the audio one time. Participants were explicitly told that they would be only allowed to listen once. The `PLAY` button vanished after one playthrough. Participants then had to answer three questions based on the audio they had heard on that trial. These questions were presented on the same page, directly below the audio player. They were first asked whether or not they knew the presented word (multiple choice---*yes*/*no*). Regardless of their answer on the first question, participants were asked what they thought the translation of the word was in English (or their best guess), and instructed to type their answer in the provided text box. Finally, they were asked to rate how confident they were in their answer on a scale of 0 to 7, where 7 was "very confident" and 0 was "not confident". There was no time limit on the response phase. All questions had to be answered to proceed to the next trial.

Participants first completed 5 practice trials with English words as the audio stimulus (ambulance, cucumber, elephant, pear, turtle). The words were recorded by a female native speaker of English. These trials acted as attention checks, as participants should always answer "yes" to the first question on prior word knowledge and be able to accurately transcribe the word they heard. Following the practice phase, participants completed the test phase where they heard either Spanish words or Catalan words. 


## Results

```{r dataset-3}
#| label: dataset-3
dataset <- split(quest_responses, quest_responses$group)
names(dataset) <- janitor::make_clean_names(names(dataset))
r_validity <- map(
  dataset,
  \(x) janitor::clean_names(table(x$response_type))
)
n_participants <- map(dataset, \(x) dplyr::n_distinct(x$participant_id))

dataset <- quest_responses |>
  filter(group != "cat-SPA") |>
  group_split(group) |>
  set_names(c("spa-ENG", "cat-ENG")) |>
  janitor::clean_names()

excluded_lang <- participants_3 |>
  filter(l2_writ_prod > 3 |
    l2_oral_comp > 3 |
    cat_oral_comp > 3 |
    cat_writ_prod > 3 |
    spa_oral_comp > 3 |
    spa_writ_prod > 3) |>
  nrow()

excluded_impairment <- participants_3 |>
  filter(valid_status == "Language impairment") |>
  nrow()

excluded_ntrials <- participants_3 |>
  filter(valid_status == "Insufficient trials") |>
  nrow()

dataset_clean <- quest_responses |>
  filter(group != "cat-SPA") |>
  dplyr::filter(valid_participant, valid_response) |>
  group_split(group) |>
  set_names(c("spa-ENG", "cat-ENG")) |>
  janitor::clean_names()
```

We collected data for a total of `r format(nrow(dataset$cat_eng) + nrow(dataset$spa_eng), big.mark = ",")` trials completed by `r n_participants$cat_eng + n_participants$spa_eng` unique participants. Of those trials, `r format(nrow(dataset$cat_eng), big.mark = ",")` were provided by `r n_participants$cat_eng` unique participants who listened to Catalan words, and `r format(nrow(dataset$spa_eng), big.mark = ",")` trials were provided by `r n_participants$spa_eng` unique participants who listened to Spanish words. We excluded trials in which participants did not enter any text (*n* = 0), in which a response in a language other than English was provided (*n* = `r r_validity$cat_eng["language"]`), in which participants did not provide a whole word (*n* = 0), and in which participants added comments to the experimenter (*n* = 0). In addition, we excluded data from participants that self-rated their oral and/or written skills in Catalan and Spanish, or any other second language as four or higher in a five-point scale (*n* = `r excluded_lang`), were diagnosed with a language (*n* = `r excluded_impairment`), or did not contribute more than 80% of valid trials (*n* = `r excluded_ntrials`).

After applying trial-level and participant-level inclusion criteria, the resulting dataset included `r format(nrow(dataset_clean$cat_eng) + nrow(dataset_clean$spa_eng), big.mark = ",")` trials provided by `r dplyr::n_distinct(dataset_clean$cat_eng$participant_id) + dplyr::n_distinct(dataset_clean$spa_eng$participant_id)` unique participants. Of those trials, `r format(nrow(dataset_clean$cat_eng), big.mark = ",")` were provided by `r dplyr::n_distinct(dataset_clean$cat_eng$participant_id)` unique participants who listened to Catalan words, and `r format(nrow(dataset_clean$spa_eng), big.mark = ",")` trials were provided by `r dplyr::n_distinct(dataset_clean$spa_eng$participant_id)` unique participants who listened to Spanish words.

```{r knowledge}
knowledge_cateng <- dataset_clean$cat_eng |> count(knowledge)
fam_cateng <- dataset_clean$cat_eng |> 
  filter(knowledge) |> 
  summarise(confidence_mean = mean(confidence),
  confidence_sd = sd(confidence))

nofam_cateng <- dataset_clean$cat_eng |> 
  filter(!knowledge) |> 
  summarise(confidence_mean = mean(confidence),
  confidence_sd = sd(confidence))

knowledge_spaeng <- dataset_clean$spa_eng |> count(knowledge)
fam_spaeng <- dataset_clean$spa_eng |> 
  filter(knowledge) |> 
  summarise(confidence_mean = mean(confidence),
  confidence_sd = sd(confidence))

nofam_spaeng <- dataset_clean$spa_eng |> 
  filter(!knowledge) |> 
  summarise(confidence_mean = mean(confidence),
  confidence_sd = sd(confidence))
```

From the `r format(nrow(dataset_clean$cat_eng), big.mark = ",")` total responses provided by English participants who listened to Catalan words, participants reported having prior knowledge of the presented Catalan words in `r knowledge_cateng$n[2]` (`r round(100*knowledge_cateng$n[2] / sum(knowledge_cateng$n), 2)`%) of them. In those responses, participants reported an average of `r round(fam_cateng$confidence_mean, 2)` confidence in the 0-8 scale (*SD* = `r round(fam_cateng$confidence_sd, 2)`). In responses in which no prior knowledge of the presented word was reported, average confidence was `r round(nofam_cateng $confidence_mean, 2)` confidence in the 0-8 scale (*SD* = `r round(nofam_cateng$confidence_sd, 2)`). From the `r format(nrow(dataset_clean$spa_eng), big.mark = ",")` total responses provided by English participants who listened to Spanish words, participants reported having prior knowledge of the presented Spanish words in `r knowledge_spaeng$n[2]` (`r round(100*knowledge_spaeng$n[2] / sum(knowledge_spaeng$n), 2)`%) of them. In those responses, participants reported an average of `r round(fam_spaeng$confidence_mean, 2)` confidence in the 0-8 scale (*SD* = `r round(fam_spaeng$confidence_sd, 2)`). In responses in which no prior knowledge of the presented word was reported, average confidence was `r round(nofam_spaeng$confidence_mean, 2)` confidence in the 0-8 scale (*SD* = `r round(nofam_spaeng $confidence_sd, 2)`). Before data analysis, responses in which participants reported prior knowledge about the meaning of the presented Catalan or Spanish word were excluded from the dataset.


```{r lengths-2}
lengths <- dataset_3 |>
  filter(!knowledge) |> 
  mutate(len = map_int(strsplit(response, ""), length)) |>
  summarise(
    across(
      len,
      lst(mean, median, sd, min, max)
    ),
    .by = c(group)
  )
lengths <- split(lengths, lengths$group)

names(lengths) <- janitor::make_clean_names(names(lengths))
```

Responses given by English participants to Catalan presented words were `r round(lengths$cat_eng$len_mean, 2)` characters long on average (*Median* = `r round(lengths$cat_eng$len_median, 2)`, *SD* = `r round(lengths$cat_eng$len_sd, 2)`, Range = `r round(lengths$cat_eng$len_min, 2)`-`r round(lengths$cat_eng$len_max, 2)`), while their translations to Spanish responses were `r round(lengths$spa_eng$len_mean, 2)` characters long on average (*Median* = `r round(lengths$spa_eng$len_median, 2)`, *SD* = `r round(lengths$spa_eng$len_sd, 2)`, Range = `r round(lengths$spa_eng$len_min, 2)`-`r round(lengths$spa_eng$len_max, 2)`).

MCMC chains in the model showed strong evidence of convergence ($\hat{R}<1.01$) (see Appendix 2 for more detailed model diagnostics). Overall, participants reported prior knowledge more often for that Spanish words with unexpectedly high accuracy (see Discussion in Experiment 2) than for words with expected accuracy (see @fig-knowledge). Participants reported prior knowledge of Catalan words with unexpected accuracy as often as those with expected accuracy. This suggests that participants in Experiment 1 may have relied, to some extent, on their prior knowledge about form-meaning mappings to correctly translate some Spanish words. To isolate such an effect of prior Spanish knowledge, we run the same analysis as in Experiment 1 on the newly collected translations from Experiment 3, now excluding responses to words in which participants reported prior knowledge.


```{r fig-knowledge}
#| label: fig-knowledge
#| fig-width: 7
#| fig-height: 5
#| fig-cap: "Catalan and Spanish prior word knowledge, as reported by English native participants in Experiment 3. The X-axis indicates the proportion of participants that reported prior knowledge with the word. The Y-axis indicates participants' accuracy (the proportion of correct responses by participants). Each colour indicates whether the data point corresponds to a word that participants answered to with expected accuracy (red) or with unexpectedly high accuracy (blue), as defined by (no phonological similarity with the correct translation, by accuracy higher than 10%)."
p <- dataset_3 |>#| 
  mutate(surprise = translation %in% tbl_data$translation) |>
  summarise(
    knowledge = mean(knowledge),
    correct = sum(correct),
    n = n(),
    .by = c(translation, group, surprise)
  ) |>
  mutate(accuracy = prop_adj(correct, n),
  surprise = if_else(
    surprise,
    "Unexpected high accuracy\n(no phonological similarity)",
    "Expected accuracy"
  )) |> 
  ggplot(aes(knowledge, accuracy, colour = surprise, fill = surprise, shape = surprise)) +
  facet_wrap(~group) +
  geom_point(stroke = 1, size = 2) +
  #geom_rug(outside = TRUE, sides = "tr") +
  labs(
    x = "Reported knowledge",
    y = "Accuracy"
  ) +
  scale_x_continuous(labels = scales::percent, limits = c(0, 1)) +
  scale_y_continuous(labels = scales::percent, limits = c(0, 1)) +
  scale_shape_manual(values = c(1, 2)) +
  scale_colour_manual(values = c(
    RColorBrewer::brewer.pal(3, "Reds")[3], 
    RColorBrewer::brewer.pal(3, "Blues")[3]
  )) +
  theme(
    legend.position = "top",
    legend.title = element_blank(),
  )
```

```{r draws-3}
c3 <- gather_draws(exp_3_m1, `b_.*`, `sd_.*`, regex = TRUE) |>
  summarise(across(.value, lst(
    .median = median,
    .lower = \(x) hdi(x)[1],
    .upper = \(x) hdi(x)[2],
    .rope = \(x) mean(between(x, -0.1, 0.1))
  ),
  .names = "{fn}"
  ))
c3 <- split(c3, c3$.variable)
names(c3) <- janitor::make_clean_names(names(c3))
```

Participants translating Catalan words and participants translating Spanish words performed similarly, as indicated by the regression coefficient of *Group* ($\beta$ = `r round(c3$b_group1$.median, 3)`, 95% CrI = [`r round(c3$b_group1$.lower, 3)`, `r round(c3$b_neigh_n_h_lv$.upper, 3)`], *p*(ROPE) = `r round(c3$b_group1$.rope, 3)`). Overall, both groups of participants responded less accurately to words with more CLPNs than to words with fewer CLPNs, regardless of the amount of phonological similarity between the presented word and its translation. This is indicated by the size of the regression coefficient of the two-way interaction between *Similarity* and *CLPN* ($\beta$ = `r round(c3$b_neigh_n_h_lv$.median, 3)`, 95% CrI = [`r round(c3$b_neigh_n_h_lv$.lower, 3)`, `r round(c3$b_neigh_n_h_lv$.upper, 3)`], *p*(ROPE) = `r round(c3$b_neigh_n_h_lv$.rope, 3)`). As anticipated, participants' performance benefited from an increase in *Similarity* ($\beta$ = `r round(c3$b_lv$.median, 3)`, 95% CrI = [`r round(c3$b_lv$.lower, 3)`, `r round(c3$b_lv$.upper, 3)`], *p*(ROPE) = `r round(c3$b_lv$.rope, 3)`), while the number of *CLND* had the opposite effect ($\beta$ = `r round(c3$b_neigh_n_h$.median, 3)`, 95% CrI = [`r round(c3$b_neigh_n_h$.lower, 3)`, `r round(c3$b_neigh_n_h$.upper, 3)`], *p*(ROPE) = `r round(c3$b_neigh_n_h$.rope, 3)`). @fig-epreds-1 illustrates the posterior of the average predictions of the model for words with different values of *Similarity* and *CLPN*.


```{r fig-epreds-3}
#| label: fig-epreds-3
#| cache: true
#| fig-cap: "Posterior model-predicted mean accuracy in Experiment 2. Predictions were generated from 4,000 posterior samples, extracted for different values of *CLPN* (0, 2, 4, 8, 12) and *Similarity* (1-100). Predictions are plotted separately for English participants translating Catalan words, and for English participants translating Spanish words. Lines indicate mean predictions, and intervals indicate 95%, 89%, 78%, 67%, and 50% credible intervals (CrI)."
#| fig-height: 5
#| fig-width: 10
get_epreds(exp_3_m1, dataset_3) |>
  mutate(
    neigh_n_h = paste0(neigh_n_h, " neighbours"),
    neigh_n_h = factor(neigh_n_h,
      levels = paste0(
        c(0, 2, 4, 8, 12),
        " neighbours"
      ),
      ordered = TRUE
    )
  ) |>
  ggplot(aes(lv, .epred)) +
  facet_grid(group ~ neigh_n_h) +
  stat_lineribbon(aes(fill_ramp = after_stat(level)),
    linewidth = 1 / 2,
    .width = c(0.99, 0.95, 0.89, 0.78, 0.67, 0.50)
  ) +
  labs(
    x = "Similarity\n(Levenshtein distance with correct translation)",
    y = "p(Correct)",
    fill_ramp = "CrI (%)"
  ) +
  scale_x_continuous(labels = scales::percent) +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_brewer(palette = "Reds") +
  theme(
    legend.position = "none",
    legend.box = "vertical"
  )
```

## Discussion


```{r tbl-results}
#| label: tbl-results
tbl_data <- list(
  "Experiment 1" = dataset_1,
  "Experiment 2" = dataset_2,
  "Experiment 3" = dataset_3
) |>
  bind_rows(.id = "exp") |>
  count(exp, group, response_type) |>
  pivot_wider(
    names_from = response_type,
    values_from = n,
    names_repair = janitor::make_clean_names
  ) |>
  relocate(exp, group, correct, typo, wrong, false_friend) |>
  mutate(
    n_correct = rowSums(cbind(correct, typo)),
    n_incorrect = rowSums(cbind(wrong, false_friend)),
    n_total = rowSums(cbind(n_correct, n_incorrect)),
    across(correct:false_friend,
      \(x) x / n_total,
      .names = "{col}_prop"
    )
  ) |>
  select(
    exp, group, correct, typo, wrong, false_friend,
    matches("prop")
  ) |>
  relocate(
    correct, correct_prop,
    typo, typo_prop,
    wrong, wrong_prop,
    false_friend, false_friend_prop
  )


tbl_data |>
  gt(rowname_col = "group", groupname_col = "exp") |>
  fmt_missing(everything(), everything(), "--") |>
  tab_spanner(
    "Correct responses",
    matches("correct|typo")
  ) |>
  tab_spanner(
    "Incorrect responses",
    matches("wrong|false")
  ) |>
  fmt_integer(c(correct, typo, wrong, false_friend)) |>
  fmt_number(is.double, pattern = "({x})", scale_by = 100) |>
  cols_label(
    correct = "Correct",
    typo = "Typo",
    wrong = "Wrong",
    false_friend = "False friend",
    correct_prop = "(%)",
    typo_prop = "(%)",
    wrong_prop = "(%)",
    false_friend_prop = "(%)"
  ) |>
  tab_style(
    cell_text(weight = "bold"),
    cells_column_spanners()
  ) |>
  tab_style(
    cell_text(style = "italic"),
    cells_column_labels()
  ) |>
  summary_rows(
    columns = is.integer,
    fns = list(
      label = md("*Sum*"),
      id = "totals",
      fn = "sum"
    ),
    fmt = ~ fmt_integer(.)
  ) |>
  summary_rows(
    columns = matches("prop"),
    fns = list(
      label = md("*Mean*"),
      id = "means",
      fn = "mean"
    ),
    fmt = ~ fmt_number(., scale_by = 100)
  ) |>
  tab_style(
    cell_text(align = "center"),
    cells_column_labels()
  )

```


# General discussion

The present work explored the psycholinguistic bases of homophonic translation, a phenomenon in which listening to speech in a non-native---perhaps unfamiliar---language leads to the activation of lexical representations in the native language, without necessarily preserving the meaning. We investigated how phonological similarity and its interaction with phonological neighbourhood density impact the dynamics of lexical activation and selection during non-native word processing. We designed a translation elicitation task in which participants listened to individual words in a non-native, unfamiliar language. After listening to each word, participants were asked to provide their best-guess translation for each word in their native language. We run three experiments. In Experiment 1, British English-native adults listened to a list of words in Catalan or Spanish. Participants reported no prior familiarity with Catalan, Spanish, or any other Romance language, yet provided accurate translations for words that shared some degree of phonological similarity with their correct English translation. Participants only benefited from phonological similarity when the number of English phonological neighbours of the presented Catalan or Spanish word was small. Experiment 2 was aimed at extending our findings to a population of participants whose native language was typologically closer to the unfamiliar language presented. We tested a group of Spanish native adults who listened to a list of Catalan words. Again, participants reported no prior familiarity with Catalan, or any other Romance language other than Spanish. In line with the results in English natives, we found an interaction between phonological similarity and phonological neighbourhood size. The facilitation effect of phonological similarity in the Spanish participants was more robust to the interferring effect of phonological neighbourhood density: Spanish participants provided correct translations for words with high neighbourhood densities more often than English participants. We suspected that participants in Experiments 1 and 2 might have managed to provide correct translations for some low-phonological similarity words thanks to having prior experience with those words, despite having reported being unfamiliar with Catalan, Spanish, or any other Romance language. Experiment 3 was a replica of Experiment 1, in which we collected additional information about participants' prior familiarity with the presented Catalan and Spanish words. The design of Experiment 3 was closely modelled after Experiment 1, except that after providing their response in each trial, participants reported whether they had previous knowledge of the presented word. After removing responses in which participants reported prior knowledge about the meaning of the presented Catalan or Spanish word, we run the same analyses as in Experiment 1, finding parallel results. Results in Experiment 3 suggest participants in the present study did not simply rely on prior familiarity with some of the word-forms presented. Rather, participants relied on the phonological similarity between the presented Catalan and Spanish words and their correct translation.

Homophonic translation is an often spontaneous phenomenon that occurs commonly when listening to non-native speech, and is prevalent across many cultures and languages [@dembeck2015oberflachenubersetzung; @efimova2018homophonic]. The most famous case of homophonic translation is *Soramimi*, which refers to homophonic translation in the context of music listening, when particular sections of the lyrics of a song in a non-native language make listeners think of particular words and phrases in their own native language. This sometimes result in a completely different meaning than thhe originally intended. Homophonic translation occurs outside the context of music. For instance, it is often intentionally employed by translators to preserve the meter and "sound structure" (e.g., rhymes) of an original text in the language of translation [e.g., @levick2016translating; @pilshchikov2016semiotics]^[The rationale behind this translation approach is to "sacrifice[s] fidelity to the image rather than the melodiousness of verse" [@briusov1911ot]. For instance, Edith Grossman's translation of *El Quijote* [@cervantes] present a similar case, in which formal aspects of the original text in Spanish are favoured to some degree over equivalence to English, even sometimes keeping Spanish words from the original text in the English translation (e.g., *Señor*, *ínsula*).]. Although homophonic translation has mostly received attention as a perceptual curiosity or as a literary figure, little attention has been paid to homophonic translation as a spontanous *psycholinguistic* event. The psycholiguistic mechanisms behind spontaneous homophonic translations are informative of the mechanisms underlying native speech processing. How does the lexical system deal with a non-native speech signal in which form-meaning associations are scarce, if not absent?

How the perceptual system deals with non-native speech is a vastly studied subject [e.g., @best1988examination; @miyawaki1975effect; @pallier2001influence; @weber2004lexical; @van2015learning; see @baese2020perception for review]. The same cannot be said about how the perceptual system deals with speech in an *unfamiliar* language. Previous research had explored how bilinguals recognise, translate, and produce words as a function of their proficiency in each of their languages [e.g., @kroll1994category; @dijkstra2019multilink; @dijkstra2002architecture; @christoffels2006memory]. The case of a very low-proficiency bilingual may be close to that of a listener of an unfamiliar language, but being bilingual implies some form of proficiency in both languages, such as some knowledge about the meaning of some words. Even low-proficiency bilinguals are likely to have already acquired many form-meaning mappings in their lower proficiency language. Characterising the impact of phonological similarity on word processing is therefore challenging in bilinguals, due to the simultaneous contribution of phonological *and* conceptual links in the lexicon. By testing participants with *no* prior familiarity with the presented language (i.e., no proficiency), we ensured that no form-meaning associations were available for participants to correctly translate the unfamiliar words, if not by relying excluusively on phonological similarity.

We calculated phonological similarity between word pairs as the Levenshtein similarity between the X-SAMPA transcription of their phonological forms [see @floccia2018introduction for a similar approach]. Using this measure, we found that participants were able to exploit the phonological similarity between the presented words and their correct translation, even when both words-forms shared few phonemes. This points to listeners accommodating non-native phonological forms to their native phoneme inventory, resulting in the activation of native lexical representations. Future studies may consider comparing the suitability of other measures of phonological similarity that take into account finer-grained phonetic or prosodic cues present in the acoustic signal presented to participants. Phonological transcriptions like X-SAMPA are abstract representations that ignore non-phonological contrasts (e.g., phones), and consider different symbols as completely different phonemes, disregarding the fact that some phonemes share more phonetic features than others. Additionally, prosodic cues such as lexical stress, which our measure of phonological similarity does not include, might also provide participants further information about the correct translation of the presented word-forms. Altogether, it is likely that participants in the present study were able to exploit additional cues in the acoustic signal to provide correct translations. Future studies may explore the relative contribution of such cues in the occurrence of homophonic translation. Overall, the present study provides some insights into the role of phonological similarity in the auditory presentation of words in an unfamiliar language.

Our findings also suggest that the facilitation effect of phonological familiarity was moderated by phonological neighbourghood density. This is in line which previous studies suggesting that the presence of (high-frequency) phonological neighbours interferes with the lexical selection in word comprehension tasks [e.g., @dufour2010phonological; @luce1998recognizing; @vitevitch1999probabilistic; @vitevitch1998words; @grainger1990word]. Our translation elicitation task can be understood as a particular instance of an auditory word recognition task, in which the presented unfamiliar words have the potential to activate phonological neighbours in participants' lexicon. We calculated a measure of cross-linguistic neighbourhood size by counting, for each presented word (in Catalan or Spanish in Experiments 1 and 3, Catalan in Experiment 2) the number of phonologically related words (with a Levenshtein distance of one) in the target language (English in Experiments 1 and 3, Spanish in Experiment 2). To account for the fact that competition between phonological neighbours is sensitive to lexical frequency [high-frequency neighbours produce stronger interference effects, @luce1998recognizing; @dufour2010phonological], we only counted neighbours with a lexical frequency higher than the correct translation. Using this measure, we found that participants' ability to exploit phonological similarity to produce correct translations declined as the number of phonological neighbours increased. Overall, this suggests that listening to words in an unfamiliar language trigger the same mechanisms of lexical activation and selection that listening to words in a native language does. 

A comparison between results in Experiments 1 and 2 suggested that the performance of Spanish participants translating Catalan was more resilient to the interfering effects of phonological neighbourhood density than the performance of English participants translating Catalan or Spanish. As highlighted before, Catalan and Spanish (both Romance languages) are typologically closer to each other than to English (a Germanic language). Catalan and Spanish share a higher proportion of cognates (i.e., form-similar translation equivalents) than Catalan and English, or Spanish and English. Previous studies in bilinguals show that this kind of cross-linguistic similarity impacts lexical processing: words that share high phonological similarity with their translations are processed faster and more accurately than words sharing lower phonological similarity. This phenomenon has provided strong evidence in favour of the language-non selective hypothesis of bilingual lexical access, which states that bilinguals activate lexical representations in both languages, even during monolingual situations. This facilitation effect has been reported for comprehension [@dijkstra2010cross; @midgley2011effects; @thierry2007brain], production [@costa2000cognate], learning [@de2000hard; @elias2022crosslanguage; @lotto1998effects; @valente2018does], and translation [@christoffels2006memory]. To our knowledge, this present study is the first to investigate the role of cross-linguistic phonological similarity in a monolingual population, in which the role of conceptual links in the lexicon is ruled out, given participants lack of familiarity with the presented language. This provided a convenient case to investigate the role of phonological similarity in lexical processing, disantangling the contribution of phonological similarity from the contribution of conceptual equivalence. Overall, our findings suggest that the typological distance between the presented and target language is associated with a more robust facilitation effect of phonological similarity.

The specific mechanisms behind this effect are unclear. One possibility is that, as aforementioned, Catalan provided additional cues to Spanish participants in Experiment 2 compared to English participants in Experiment 1. For instance, it is likely that subphonemic cues not accounted by our measure of phonological similarity were more informative to Spanish participants than to English participants. In addition, it remains to be tested whether such association is linear (i.e., robustness of facilitation increases linearly with typological distance), proportional, or even extends beyond the specific pair of languages involved in the present work remains unclear. Do other close or distant language pairs show the same pattern of results? The generalisability of these findings can also be tested by increasing the repertoire of words involved in the translation elicitation task. In Experiments 1 to 3, participants answered to a maximum of 105 words. All words were of high-frequency, and had a low age-of-acquisition. In order to characterise listeners' ability to translate correctly words from an unfamiliar language, words with varying levels of difficulty should be included.

In summary, the present paper provides insights into the psycholinguistic mechanisms underlying homophonic translation. English and Spanish native adults were tested in a translation elicitation task in which they had to guess the English or Spanish translation of a series of words in an unfamiliar language. Participants successfully exploited phonological similarity between the presented words and their correct translations to provide correct answers. Participants' performance in the task only benefited from phonological similarity when the presented word had few higher-frequency phonological neighbours in the target language. Finally, the facilitation effect of phonological similarity was more robust in the Spanish native participants, who translated words from a typologically closer language than English participants. Overall, the findings presented in the present paper suggest that the processing of words in a non-native, unfamiliar language recruits mechanisms of lexical activation, selection, and interference parallel to those recruited by listening to words in a native language.

# References


