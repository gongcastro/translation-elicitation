---
title: "The role of cognateness in non-native spoken word recognition"
---


```{r}
#| label: setup
library(targets)
library(dplyr)
library(tidyr)
library(ggplot2)
library(patchwork)
library(gt)
library(gtExtras)
library(brms)
library(tidybayes)

set.seed(1234)

# import objects
tar_config_set(store = here::here("_targets"),
               script = here::here("_targets.R"))
tar_load_globals()
tar_load(participants)
tar_load(exp_participants)
tar_load(exp_responses)
tar_load(quest_responses)
tar_load(quest_participants)
tar_load(stimuli)
tar_load(epreds)
tar_load(dataset_1)
tar_load(dataset_2)
tar_load(dataset_3)
tar_load(exp_1_m0)
tar_load(exp_2_m0)
tar_load(exp_3_m1)


theme_set(theme_ggdist() + 
              theme(panel.grid.major.y = element_line(colour = "grey",
                                                      linetype = "dotted")))

```

# Introduction

Listening to non-native speech is costlier than listening to native speech, even for highly proficient bilinguals, and especially in acoustically adverse situations [@lecumberri2010non; @takata1990english]. However, there are some sources of information which may make non-native speech comprehension easier. One of the sources of this increased difficulty stems from the possible mismatch between the phonology of the native and the non-native languages: some acoustic features embedded in the non-native speech signal do not overlap perfectly with any phonemic category in the listener's native language. For example, imagine a Spanish native, with no previous familiarity with any other language, who listens for the first time to French may encounter the word /`r knitr::asis_output("\U0070\U0254\U0281\U0074")`/ (*porte*), translation of door in French. The voiced uvular fricative consonant /`r knitr::asis_output("\U0281")`/ and the open-mid back rounded vowel /`r knitr::asis_output("\U0254")`/ do not exist in Spanish as phonemic categories. These discrepancies can nonetheless be perceived as allophonic variations the native (Spanish) phonemes /r/ and /o/ [@best2001discrimination], and the non-native speech signal may engage lexical processing mechanisms [@weber2004lexical], leading to the activation and selection, and therefore the recognition of its translation to Spanish /`r knitr::asis_output("\U02C8\U0070\U0077\U0065\U0072\U002E\U0074\U0061")`/ (*puerta*). The recognition of this French, unfamiliar word relies almost entirely on its similarity with its Spanish translation. Conversely, imagine now that the same Spanish listener hears English for the first time and encounters the word /`r knitr::asis_output("\U0064\U0254\U02D0")`/ (*door*). This time, phonological similarity is of little help: /`r knitr::asis_output("\U02C8\U0070\U0077\U0065\U0072\U002E\U0074\U0061")`/ and /`r knitr::asis_output("\U0064\U0254\U02D0")`/ share very few phonemes. It would come as a surprise if the listener was able to translate the English word successfully, if not by relying on previously learnt word-concept associations (which they lack, as they are unfamiliar to English).

Cross-language phonological similarity seems, at face value, an important source of information for unfamiliar listeners during auditory word recognition, even if translations are not phonologically identical. Although this mismatch has a noticeable toll on comprehension [@cutler2004patterns], the fact that word recognition can take place at all in such circumstances illustrates that non-native listeners–even low-proficiency ones–are rarely completely naïve to the language they are listening to. In the present study, we investigate the extent to which adult listeners rely on the phonological similarity between their native and non-native languages during word recognition, capitalising on the role of *cognates.*

Many languages share, to some extent, similarities at the lexical level. This is frequently due to their typological closeness and/or socio-historical events involving the speakers of these languages (e.g., migration, social contact). Cognates embody a particular case of such commonalities. Cognates are defined as cross-language synonyms which have similar forms whose form is similar (at the phonological level, orthographic level, signed level), commonly due to their shared etymological origin^[Some form-similar cross-language synonyms are technically not cognates. For example, *sun* and *sol* (in Spanish) share phonological onset but their etymology points to different origins. The complimentary case is equally problematic when defining cognateness: cross-language synonyms that share etymological origin may not be necessarily be recognised as cognates by the average listener. Such is the case of *cheese* and *queso* which share Latin root but are phonologically dissimilar. For simplicity, we will use the term *cognateness* in this paper to include all cross-language synonyms that fulfill a specified threshold of form similarity, regardless of etymology. We do not expect that etymology plays a direct role on language perception if it is not via form-similarity, since it is not necessary for participants in psycholinguistic experiments to be aware of the etymology of the words they encounter in the tasks to be subject to the effect of form-similarity.]. For example, Romance languages such as Spanish and Catalan share many cognates [@schepens2012distributions], as in the case of *puerta* and *porta* (door in Spanish and Catalan, respectively). Cognates play a pivotal role in many models of bilingual lexical processing because they provide evidence that bilinguals access the lexicon in a language non-selective way. For instance, pictures are named faster by Spanish-Catalan bilinguals when their associated labels in both languages are cognates (e.g., *puerta*-*porta*), compared to when their labels are non-cognates (*mesa*-*taula*, [table]), which suggests that word representations of both languages are activated during word retrieval [@costa2000cognate]. This cognate advantage is not restricted to production, but also extends to word recognition [@thierry2007brain; @midgley2011effects], word learning [@de2000hard; @lotto1998effects; @valente2018does; @elias2022cross], and word translation [@christoffels2006memory].

## Lexical and conceptual routes for translation

Early theoretical accounts of this 'Cognate effect' proposed that the impact of cognateness on lexical processing depends on bilinguals' proficiency in their second language (L2) [@potter1984lexical; see @chen1989patterns for similar results]. The reasoning behind this claim was the assumption that second-language learners start acquiring words in their L2 by associating them to their translation in their L1 (lexical route), instead of directly to their shared concept (conceptual route). This word-to-word connectivity should be sensitive to form-similarity (i.e., cognateness): the more similar the two word- forms are, the stronger the connection. As learners become more proficient, the connection between L2 representations and their meaning grows stronger, and L2 word processing becomes less reliant on word-word connections between L1 and L2 representations. Cognates subsequently exert less impact on lexical processing in proficient speakers [see @andras2022cognate for recent experimental evidence for this claim]. The Revised Hierarchical Model [RHM, @kroll1994category] captured this hypothesis and predicted that translating words from L1 to L2 should take longer than translating from L2 to L1. The rationale behind this prediction was that L1-to-L2 translation relies more strongly on word-to-word links between L1 and L2 representations, where L2-to-L1 translation relies more strongly on the mediation between the concept and the two word- forms. L1-to-L2 translation should therefore be more sensitive to the form similarity between the L1 and the L2 representations: cognate words should be retrieved faster than non-cognate words during L1-to-L2 translation.

To test these predictions, @degroot1994forward and @de1992determinants asked Dutch natives with high (but non-native) English proficiency to translate Dutch words to English (L1-to-L2) or English words to Dutch (L2-to-L1). Although participants' performance in both conditions was roughly equivalent for cognates, non-cognates were translated more slowly from Dutch to English than from English to Dutch, suggesting that L1-to-L2 translation is more sensitive to form-similarity than L2-to-L1 translation. Conversely, participants' performance was more sensitive to semantic variables (e.g., concreteness) when translating English words to Dutch, compared to when translating Dutch words to English, suggesting that L2-to-L1 translation was more sensitive to the shared conceptual properties of the translation pair.

These findings support a soft version of the RHM model, in which both the conceptual and the lexical translation routes are active during translation, but where L1-to-L2 translation relies more strongly on word-to-word links between L1 and L2 representations than L2-to-L1 translation does, and therefore is more sensitive to cognateness. Further evidence for the presence of this difference between L1-to-L2 and L2-to-L1 translations on the degree of reliance on the lexical and or the conceptual routes between L1-to-L2 and L2-to-L1 translation was provided by @phillips2006erp, who used eletrophysiological recordings of bilinguals translating words from L1 to L2 or *vice versa* after semantic or phonological priming. The authors reported stronger N400 responses (associated with semantic priming) in L1-to-L2 translation than in L2-to-L12 translation, and stronger phonological mismatch negativity responses (associated with phonological priming) during L2-to-L1 translation, compared to L1-to-L2 translation.

## Competition from lexical neighbours

Since the RHM model was proposed, later studies on monolingual populations have focused how on the role of the network of connections that words establish networks of connections with each other at the form-level (phonological or orthographic) or at the conceptual level. For instance, the Neighbourhood Activation Model [NAM, @luce1998recognizing] highlighted how lexical selection is affected by the number of phonological neighbours that the selected word has. Words surrounded by a larger number of phonological neighbours (words that only differ in one phoneme from the target word) are responded to more slowly and less accurately than words from sparser phonological neighbourhoods [@goldinger1989priming; @luce1990similarity], especially when these neighbours' lexical frequency is higher [@luce1998recognizing]. 

The existence of phonological links between words is supported with experimental evidence from the visual world paradigm and phonological priming. @meyer2007activation showed that target pictures were named faster when presented together with a phonologically-related distractor, suggesting that the activation of shared phonological information allowed the target to be activated and produced faster. there are also findings suggesting that facilitatory or inhibitory effects on target recognition depend on the degree of similarity the competitor shares with the target [@hamburger1996phonological; @dufour2003lexical]. When there is only mismatch of one phoneme between prime and target, inhibitory effects are exerted on target recognition [@dufour2003lexical]. This inhibition was proposed to be due to strong lexical activation of the prime which results in lexical interference, as opposed to simple phonological activation from less similar primes that triggers facilitation.

These findings of phonological links between words in the monolingual lexicon pose an important question in the field of bilingualism research: do word representations in one language form part of the phonological or orthographic neighbours of the other language? @weber2004lexical observed that bilinguals displayed lexical competition effects from words in their native language when performing a word-referent matching task in their L2. This highlights that lexical competition for bilinguals during language comprehension may not only come from the tested language but also the native language, despite the native language not being actively required for the task. Other studies have similarly found interference when spoken target words in one language had phonological overlap with a word in the participant's other language, even when the other language was not required by the task [@spivey1999cross; @desroches2022dynamics]. Such patterns have even been found with young children less than 4 years old [@von2012language].

The Bilingual Interactive Activation (BIA/BIA+) [@van1998orthographic; @dijkstra2002architecture] addressed this issue and suggested that lexical representations from both languages establish both excitatory and inhibitory connections with each other, resulting in an integrated lexicon for both languages. This is, in line with connectionist approaches to lexical processing [@mcclelland1981interactive]. Evidence supporting such an integrated lexicon is provided by the fact that bilinguals' performance in word recognition tasks is sensitive to the orthographic neighbourhood density of the presented word's translation. When bilingual participants are presented with words in one language in a lexical decision task, translating them to the other language took longer when they were part of large neighbourhoods [@van1998orthographic]. This suggests that presented words activated orthographic neighbourhoods in the other language, which competed for selection with the target word during recognition. 


## The present study

In the present study, we tested listeners' reliance on phonological similarity when translating words from an unfamiliar language to their native language, in order to investigate the plausibility of word-to-word connections during L1-to-L2 translation. We tested English native speakers and Spanish native speakers in a translation elicitation task. This paradigm has been used by bilingualism researchers to identify easily-recognisable cognates for experimental studies. Using this paradigm, we aim to delve deeper into the factors affecting foreign word recognition by monolingual participants with no prior exposure to the language.

Participants were presented with spoken words in an unfamiliar (non-native language) and were asked to translate them to their native language. We will henceforth refer to the auditory-presented words heard by participants on each trial as presented words, and the correct translation for the presented words as target translations. The motivation behind testing participants in an unfamiliar language was that these participants can be considered a particular (extreme) case of unbalanced bilingualism, in which they do not know any words in L2, and therefore word-to-concept connections are, in principle, absent. As a consequence, these participants should only be able use word-to-word connections to succeed in the translation of words in an unfamiliar language. We predicted that, if the information provided by the phonological form of the unfamiliar presented words is sufficient for translation, participants' performance should increase when the translation pairs are phonologically more similar.

Following @luce1998recognizing's NAM model, we further hypothesised that if participants are able to correctly translate words from an unfamiliar language based on their phonological similarity to the target word, similar sounding but incorrect translations (e.g., false friends) should be activated too. If this is the case, non-native words that have more phonological neighbours should be less likely to be correctly translated, especially if such neighbours have higher lexical frequency and are phonologically closer to the presented word than the target translation. For instance, one could expect English participants to incorrectly translate the Spanish word *botón* as bottom instead of as its correct translation *button* (because the lexical frequency of *bottom* is higher than that of *button* and also phonological closer). This combined constraint led us to calculate a specific form of phonological neighbours where a neighbour is counted only if it is higher frequency and has stronger phonological similarity than the target translation as judged by Levenshtein distance. If the phonological neighbourhood density of the target translation affects participants' performance negatively, this would suggest that non-native words trigger competition between words in the native language during translation.

In Experiment 1, we collected data from two groups of British English natives. One group was presented with Catalan words, the other with Spanish words. We examined the extent to which participants were able to use the phonological similarity between the presented (Catalan or Spanish) word and its translation (i.e., cognateness) to provide accurate responses. In Experiment 2, we tested a group of (European) Spanish natives in the same task, who were presented with Catalan words. Catalan and Spanish are both Romance languages whose close typological distance is reflected in the fact that they share many cognates, where English is a Germanic language that shares considerably fewer cognates with Catalan and Spanish. By testing participants translating words from typologically close and distant languages, we expected to widen the range of the phonological similarity scores of the translation pairs involved in the experimental task, therefore allowing us to explore potential cross-language differences in participant's performance. Participants in Experiments 1 and 2 were surprisingly good at translation some words, which had low phonological similarity with their correct translation. In Experiment 3, we collected additional data from a new group of British English natives, who were presented with Catalan and Spanish words. In contrast with the procedure in Experiment 1, participants in Experiment 3 where asked, after providing a response in each trial, to report whether they had previous knowledge of the meaning of the presented word (e.g., some Spanish words are common knowledge, even for monolingual speakers of English). Finally, we present analyses on the joint data sets of Experiments 1 to 3.


# Experiment 1

## Methods

All materials, data, and code used in this study are hosted in the Open Science Framework [https://osf.io/9fjxm/](https://osf.io/9fjxm/) and a GitHub repository [https://github.com/gongcastro/translation-elicitation.git](https://github.com/gongcastro/translation-elicitation.git), along with additional notes.


### Participants

```{r}
#| label: participants-numbers-1
participants_1 <- exp_participants |> 
    filter(group!="cat-SPA")

collection_dates <- participants_1 |> 
    pull(date) |> 
    range() |> 
    format("%B %dth, %Y")

n_participants <- n_distinct(participants_1)
n_participants_group <- table(participants_1$group)
participants_age <- participants_1 |> 
    summarise(across(age, lst(mean, sd, min, max))) 
n_female <- table(participants_1$gender)
```

Participants were recruited via Prolific (5£ compensation) and SONA (compensation in academic credits), and gave informed consent before providing any data and the study was conducted in accordance with ethical standards of the Declaration of Helsinki and the protocol was by the University of Oxford Medical Sciences Inter-Divisional Research Ethics Committee (IDREC) (R60939/RE007). Participants were asked to complete the experiment using a laptop in a quiet place with good internet connection.  Data collection took place from `r collection_dates[1]` to `r collection_dates[2]`. We collected data from `r n_participants` British English native participants living in United Kingdom (*Mean* = `r round(participants_age$age_mean, 2)` years, *SD* = `r round(participants_age$age_sd, 2)`, Range = `r round(participants_age$age_min, 2)`-`r round(participants_age$age_max, 2)`, `r n_female` female). 

```{r tbl-participants}
#| label: tbl-participants
tbl_data <- participants |> 
    mutate(exp = case_when(
        source=="Experiment" & group!= "cat-SPA" ~ "Experiment 1",
        source=="Experiment" & group== "cat-SPA" ~ "Experiment 2",
        source=="Questionnaire" ~ "Experiment 3",
    )) |> 
    summarise(n = n(),
              n_excluded = sum(!valid_status=="Valid"),
              across(age, lst(mean, sd, min, max)),
              knows_l2 = sum(l2 != "None"),
              l2_lst = lst(unique(l2)[unique(l2)!="None"]),
              .by = c(group, exp)) 

tbl_data |> 
    gt(groupname_col = "exp", rowname_col = "group") |> 
    fmt_number(matches("age")) |> 
    fmt_integer(matches("min|max")) |> 
    cols_merge_range(age_min, age_max) |> 
    tab_spanner("Age", matches("age")) |> 
    tab_spanner("Sample size", starts_with("n")) |> 
    tab_spanner("L2", matches("l2")) |> 
    cols_label(group = "Group",
               exp = "Experiment",
               n = "N",
               n_excluded = "Excluded",
               age_mean = "M",
               age_sd = "SD",
               age_min = "Range",
               knows_l2 = "N knows L2?",
               l2_lst = "Which L2?") |> 
    tab_style(cell_text(style = "italic"),
              cells_column_labels()) |> 
    tab_style(cell_text(weight = "bold"),
              cells_column_spanners()) |> 
    tab_style(cell_text(align = "left"),
              list(cells_body(), cells_column_labels()))
```


### Stimuli

```{r}
#| label: stimuli-lengths
lengths <- stimuli |>  
    select(group, ipa_flat_1, ipa_flat_2, word_1, word_2) |>  
    mutate(across(ipa_flat_1:word_2, nchar)) |> 
    summarise(across(c(ipa_flat_1, ipa_flat_2, word_1, word_2), 
                     lst(mean, sd, min, max)),
              .by = group)
lengths <- split(lengths, lengths$group)
names(lengths) <- janitor::make_clean_names(names(lengths))

durations <- stimuli |> 
    summarise(across(duration, lst(mean, sd, min, max)),
              .by = group) 
durations <- split(durations, durations$group)
names(durations) <- janitor::make_clean_names(names(durations))
```

We arranged two lists of input words to be presented to participants in the auditory modality: one in Catalan and one in Spanish. Words in the Catalan list were `r round(lengths$cat_eng$ipa_flat_1_mean, 2)` phonemes long on average (*SD* = `r round(lengths$cat_eng$ipa_flat_1_sd, 2)`, *Range* = `r round(lengths$cat_eng$ipa_flat_1_min, 2)`-`r round(lengths$cat_eng$ipa_flat_1_max, 2)`), and the orthographic form of their English translations (which participants had to type), were `r round(lengths$cat_eng$word_2_mean, 2)` characters long on average (*SD* = `r round(lengths$cat_eng$word_2_sd, 2)`, *Range* = `r round(lengths$cat_eng$word_2_min, 2)`-`r round(lengths$cat_eng$word_2_max, 2)`. Words in the Spanish list were `r round(lengths$spa_eng$ipa_flat_1_mean, 2)` phonemes long on average (*SD* = `r round(lengths$spa_eng$ipa_flat_1_sd, 2)`, *Range* = `r round(lengths$spa_eng$ipa_flat_1_min, 2)`-`r round(lengths$spa_eng$ipa_flat_1_max, 2)`), and the orthographic form of their English translations (which participants had to type), were `r round(lengths$spa_eng$word_2_mean, 2)` characters long on average (*SD* = `r round(lengths$spa_eng$word_2_sd, 2)`, *Range* = `r round(lengths$spa_eng$word_2_min, 2)`-`r round(lengths$spa_eng$word_2_max, 2)`.

Participants listened to one audio file in each trial, each containing a single word presented in isolation. The audio files were the same ones used in child experiments conducted in the Laboratori de Recerca en Infància of Universitat Pompeu Fabra (Barcelona, Spain). These audio files were recorded by a proficient Catalan-Spanish female bilingual from the Metropolitan Area of Barcelona in a child-directed manner. Catalan and Spanish words were recorded at 44,100 Hz in separate files in the same session, and then de-noised using Audacity and normalised at peak intensity using Praat [@broersma2021praat]. The average duration of the Catalan audio files was `r round(durations$cat_eng$duration_mean, 2)` (*SD* = `r round(durations$cat_eng$duration_sd, 2)`, *Range* = `r round(durations$cat_eng$duration_min, 2)`-`r round(durations$cat_eng$duration_max, 2)`). The average duration of the Catalan audio files was `r round(durations$spa_eng$duration_mean, 2)` (*SD* = `r round(durations$spa_eng$duration_sd, 2)`, *Range* = `r round(durations$spa_eng$duration_min, 2)`-`r round(durations$spa_eng$duration_max, 2)`).


```{r}
stim_exc_freq <- stimuli |> 
    mutate(is_na = is.na(freq_zipf_2)) |> 
    count(group, is_na) |> 
    filter(is_na)
stim_exc_freq <- split(stim_exc_freq, stim_exc_freq$group)
names(stim_exc_freq) <- janitor::make_clean_names(names(stim_exc_freq))

stim_exc_freq_n <- sum(map_int(stim_exc_freq, "n"))
```

For each translation pair, we defined three predictors of interest: the lexical frequency of the correct translation (*Frequency*), the phonological similarity between the presented (Catalan or Spanish) word and their correct English translation (*Cognateness*), and presented word's number of cross-language phonological neighbours (*CLPN*).

We included *Frequency* under the hypothesis that, keeping other predictors constant, participants would translate higher-frequency words more accurately and faster than lower-frequency words. Lexical frequencies of correct translations were extracted from SUBTLEX-UK [@van2014subtlex], and transformed to Zipf scores [@van2014subtlex]. Words in the stimuli list without a lexical frequency value were excluded from data analysis (`r stim_exc_freq$cat_eng$n` in the Catalan list, `r stim_exc_freq$spa_eng$n` in the Spanish list).

We calculated *Cognatenesss*, our main predictor of interest, by computing the Levenshtein similarity between the X-SAMPA transcriptions of each pair of translations using the `stringdist` R package [@van2014stringdist]. The Levenshtein distance computes the edit distance between two character strings (in this case, phoneme transcriptions) by counting the number of additions, deletions, and substitutions necessary to make both strings identical [@levenshtein1966binary]. We divided this edit distance by the number of characters included in the longest X-SAMPA transcription of the translation pair. This results in a proportion score, in which values closer to zero correspond to lower Levenshtein distances between phonological transcriptions (i.e., higher similarity), and values closer to 1 correspond to higher Levenshtein distances (i.e., lower similarity). This transformation also help account for the fact that the Levenshtein distance tends to increase with the length of the transcriptions. For interpretability, we subtracted this proportion from one, so that values closer to one correspond to higher similarity between phonological transcriptions, and values closer to zero correspond to lower similarity between phonological transcriptions. For example, the *table* (teɪbəl)-*mesa* (mesa) translation pair had a `r scales::percent(stringsim(enc2utf8("teɪbəl"), enc2utf8("mesa")))` similarity, while the *train* (trEIn)-*tren* (t`r knitr::asis_output("\U027E")`En) translation pair had a `r scales::percent(stringsim(enc2utf8("trEIn"), enc2utf8("tɾEn")))` similarity. Figure 1 summarises the lexical frequency, phonological neighbourhood density and phonological overlap of the words included in the Catalan and the Spanish lists.

For each Catalan and Spanish word, we calculated the number of *CLPN* by counting the number of English words with same or higher lexical frequency, and whose phonological transcription (in X-SAMPA format) different in up to one phoneme from that of the presented Catalan or Spanish word. Lexical frequencies and phonological transcriptions were extracted from the multilingual database CLEARPOND [@marian2012clearpond]^[[Phonological trnascriptions in CLEARPOND were generated from eSPEAK, [http://espeak.sourceforge.net/](http://espeak.sourceforge.net/)]].


```{r tbl-stimuli}
#| label: tbl-stimuli
stimuli |> 
    summarise(across(c(freq_zipf_2, lv, neigh_n_h),
                     lst(mean, sd, min, max), 
                     na.rm = TRUE),
              .by = c(group)) |> 
    gt(rowname_col = "group") |> 
    tab_spanner("Frequency", matches("freq")) |> 
    tab_spanner("Cognateness", matches("lv")) |> 
    tab_spanner("CLPN", matches("neigh")) |> 
    fmt_number(is.numeric) |> 
    fmt_integer(c(neigh_n_h_min,
                  neigh_n_h_max)) |> 
    cols_merge_uncert(freq_zipf_2_mean, freq_zipf_2_sd) |> 
    cols_merge_uncert(lv_mean, lv_sd) |> 
    cols_merge_uncert(neigh_n_h_mean, neigh_n_h_sd) |> 
    cols_merge_range(freq_zipf_2_min, freq_zipf_2_max) |> 
    cols_merge_range(lv_min, lv_max) |> 
    cols_merge_range(neigh_n_h_min, neigh_n_h_max) |> 
    cols_label(freq_zipf_2_mean = "Mean ± SD",
               freq_zipf_2_sd = "SD",
               freq_zipf_2_min = "Range",
               lv_mean = "Mean ± SD",
               lv_sd = "SD",
               lv_min = "Range",
               neigh_n_h_mean = "Mean ± SD",
               neigh_n_h_sd = "SD",
               neigh_n_h_min = "Range") |> 
    tab_style(cell_text(weight = "bold"),
              cells_column_spanners()) |> 
    tab_style(cell_text(style = "italic"),
              cells_column_labels())

```


### Procedure

The experiment was implemented online using Psychopy/Pavlovia [@peirce2019psychopy2]. Participants accessed the study from a link provided by Prolific or SONA and completed the experiment from an internet browser (Chrome or Mozilla). After giving their consent for participating, participants answered a series of questions about their demographic status, their language background, and the set up they were using for completing the study. Then, participants completed the experimental task. Participants were informed that they would listen to a series of pre-recorded words in either Catalan or Spanish (English participants) or Catalan (Spanish participants). They were instructed to listen to each word, guess its meaning in English (English participants) or Spanish (Spanish participants), and type their answer as soon as possible. English participants were randomly assigned to the Catalan or Spanish lists. Participants in the Catalan list were presented with `r nrow(stimuli[stimuli$group=="cat-ENG",])` trials, and participants in the Spanish list were presented with `r nrow(stimuli[stimuli$group=="spa-ENG",])` trials.

Each trial started with a yellow fixation point presented during one second on the centre of the screen over a black background. After one second, the audio started playing while the dot remained being displayed until the audio ended. Upon the offset of the fixation point and audio, participants were prompted to write their answer by a `>` symbol. Typed letters were displayed in the screen in real time to provide visual feed-back to participants. Participants were allowed to correct their answer. Then, participants pressed the `RETURN/ENTER` key to confirm their answer and start and new trial. We excluded trials where the participant's answer was not a valid word in the target language or no answer was given. Trials in which the response was mistyped by only one character were counted as correct (e.g., "pengiun" instead of "penguin"), as long as the response did not correspond to a distinct word. Trials in which participants took longer than 10 seconds to respond were also excluded. Participants contributed a total of `r nrow(exp_responses)` valid trials (`r nrow(exp_responses[exp_responses$group %in% c("cat-ENG", "cat-SPA"),])` in Catalan, `r nrow(exp_responses[exp_responses$group %in% c("spa-ENG"),])` in Spanish). The task took approximately 15 minutes to be completed.



```{r}
#| label: procedure-figure
#| echo: false
#| message: false
#| warning: false
#| out-width: "80%"
#| fig-cap: "Schematic representation of a trial in the experimental task."
knitr::include_graphics(here::here("img", "design.png"))
```



### Data analysis

We modelled the probability of participants guessing the correct translation of each presented word using a generalised multilevel Bayesian regression model with a Bernoulli logit link distribution. We included as fixed effects the intercept, the main effects of *Frequency*, *Cognateness*, and *CLPN*, and the two-way interaction between *Cognateness* and *CLPN*. We also included participant-level random intercepts and slopes for the the main effects and the interaction. Eq. 1 shows a formal description of the model.


![Eq. 1](_assets/img/model.PNG)

To test the practical relevance of each predictor we followed [@kruschke2018bayesian]. We first specified a region of practical equivalence (*ROPE*) around zero ([-0.1, +0.1], in the logit scale). This area indicates the values of the regression coefficients that we considered as equivalent to zero. We then computed the 95% posterior credible intervals (CrI) of the regression coefficients of interest. Finally, we calculated the proportion of the 95% CrI that fell inside the ROPE. This proportion indicates the probability that the true value of the coefficient is equivalent to zero.

All analyses were performed in R environment [@rcore2019r]. We used the tidyverse family of R packages [@wickham2019tidyverse] to process data and to generate figures. We used the `brms` R package [@burkner2017brms] using the `cmdstanr` back-end to the Stan probabilistic language [@carpenter2017stan] to estimate and compare the models (see Appendix 1 for mode details on the models).


## Results

```{r dataset-1}
#| label: dataset-1
dataset <- exp_responses |> 
    filter(group!="cat-SPA") |> 
    group_split(group) |> 
    set_names(c("spa-ENG", "cat-ENG")) |> 
    janitor::clean_names() 

dataset <- split(exp_responses, exp_responses$group)
names(dataset) <- janitor::make_clean_names(names(dataset))
r_validity <- map(dataset, 
                  \(x) janitor::clean_names(table(x$response_type)))
n_participants <- map(dataset, \(x) n_distinct(x$participant_id))

excluded_lang <- participants_1 |> 
    filter(l2_writ_prod > 3 |
               l2_oral_comp > 3 |
               cat_oral_comp > 3 | 
               cat_writ_prod > 3 |
               spa_oral_comp > 3 |
               spa_writ_prod > 3) |> 
    nrow()

excluded_impairment <- participants_1 |> 
    filter(valid_status=="Language impairment") |> 
    nrow()

excluded_ntrials <- participants_1 |> 
    filter(valid_status=="Insufficient trials") |> 
    nrow()

dataset_clean <- exp_responses |> 
    filter(group!="cat-SPA") |> 
    dplyr::filter(valid_participant, valid_response) |> 
    group_split(group) |> 
    set_names(c("spa-ENG", "cat-ENG")) |> 
    janitor::clean_names() 

lengths <- dataset_1 |> 
    mutate(len = nchar(response)) |> 
    summarise(across(len,
                     lst(mean, median, sd, min, max)),
              .by = c(group)) 
lengths <- split(lengths, lengths$group)

names(lengths) <- janitor::make_clean_names(names(lengths))
```

We collected data for a total of `r format(nrow(dataset$cat_eng) + nrow(dataset$spa_eng), big.mark = ",")` trials completed by `r n_participants$cat_eng + n_participants$spa_eng` unique participants. Of those trials, `r format(nrow(dataset$cat_eng), big.mark = ",")` were provided by `r n_participants$cat_eng` unique participants who listened to Catalan words, and `r format(nrow(dataset$spa_eng), big.mark = ",")` trials were provided by `r n_participants$spa_eng` unique participants who listened to Spanish words. We excluded trials in which participants did not enter any text (*n* = `r r_validity$cat_eng["blank"]`), in which a response in a language other than English was provided (e.g., `agua`, *n* = `r r_validity$cat_eng["language"]`), in which participants did not provide a whole word (e.g., `f`, *n* = `r r_validity$cat_eng["incomplete"]`), and in which participants added comments to the experimenter (e.g., `unsure`, (*n* = `r r_validity$cat_eng["other"]`). In addition, we excluded data from participants that self-rated their oral and/or written skills in Catalan and Spanish, or any other second language as four or higher in a five-point scale (*n* = `r excluded_lang`), were diagnosed with a language (*n* = `r excluded_impairment`), or did not contribute more than 80% of valid trials (*n* = `r excluded_ntrials`). After applying trial-level and participant-level inclusion criteria, the resulting dataset included `r format(nrow(dataset_clean$cat_eng) + nrow(dataset_clean$cat_eng), big.mark = ",")` trials provided by `r n_distinct(dataset_clean$cat_eng$participant_id) + n_distinct(dataset_clean$cat_eng$participant_id)` unique participants. Of those trials, `r format(nrow(dataset_clean$cat_eng), big.mark = ",")` were provided by `r n_distinct(dataset_clean$cat_eng$participant_id)` unique participants who listened to Catalan words, and `r format(nrow(dataset_clean$spa_eng), big.mark = ",")` trials were provided by `r n_distinct(dataset_clean$spa_eng$participant_id)` unique participants who listened to Spanish words. Responses given by English participants to Catalan presented words were `r round(lengths$cat_eng$len_mean, 2)` characters long on average (*Median* = `r round(lengths$cat_eng$len_median, 2)`, *SD* = `r round(lengths$cat_eng$len_sd, 2)`, Range = `r round(lengths$cat_eng$len_min, 2)`-`r round(lengths$cat_eng$len_max, 2)`), while their translations to Spanish responses were `r round(lengths$spa_eng$len_mean, 2)` characters long on average (*Median* = `r round(lengths$spa_eng$len_median, 2)`, *SD* = `r round(lengths$spa_eng$len_sd, 2)`, Range = `r round(lengths$spa_eng$len_min, 2)`-`r round(lengths$spa_eng$len_max, 2)`).

```{r tbl-results}
#| label: tbl-results
tbl_data <- list("Experiment 1" = dataset_1,
                 "Experiment 2" = dataset_2,
                 "Experiment 3" = dataset_3) |> 
    bind_rows(.id = "exp") |> 
    count(exp, group, response_type) |> 
    pivot_wider(names_from = response_type,
                values_from = n,
                names_repair = janitor::make_clean_names) |>
    relocate(exp, group, correct, typo, wrong, false_friend) |> 
    mutate(n_correct = rowSums(cbind(correct, typo)),
           n_incorrect = rowSums(cbind(wrong, false_friend)),
           n_total = rowSums(cbind(n_correct, n_incorrect)),
           across(correct:false_friend, 
                  \(x) x / n_total,
                  .names = "{col}_prop")) |> 
    select(exp, group, correct, typo, wrong, false_friend,
           matches("prop"))


tbl_data |> 
    gt(rowname_col = "group", groupname_col = "exp") |> 
    fmt_missing(everything(), everything(), "--") |> 
    tab_spanner("Correct responses", 
                c(correct, typo)) |> 
    tab_spanner("Incorrect responses", 
                c(wrong, false_friend)) |> 
    fmt_integer(c(correct, typo, wrong, false_friend)) |> 
    fmt_percent(is.double) |> 
    cols_merge_n_pct(correct, col_pct = correct_prop) |> 
    cols_merge_n_pct(typo, col_pct = typo_prop) |> 
    cols_merge_n_pct(wrong, col_pct = wrong_prop) |> 
    cols_merge_n_pct(false_friend, col_pct = false_friend_prop) |> 
    
    cols_label(correct = "Correct",
               typo = "Typo",
               wrong = "Wrong translation",
               false_friend = "False friend") |> 
    tab_style(cell_text(weight = "bold"), 
              cells_column_spanners()) |> 
    tab_style(cell_text(style = "italic"), 
              cells_column_labels())

```


```{r}
c1 <- gather_draws(exp_1_m0, `b_.*`, `sd_.*`, regex = TRUE) |> 
    summarise(across(.value, lst(.median = median, 
                                 .lower = \(x) hdi(x)[1],
                                 .upper = \(x) hdi(x)[2],
                                 .rope = \(x) mean(between(x, -0.1, 0.1))),
                     .names = "{fn}"))
c1 <- split(c1, c1$.variable)
names(c1) <- janitor::make_clean_names(names(c1)) 
```

@tbl-results shows a summary of participants' accuracy across Experiments 1, 2, and 3. MCMC chains in the model showed strong evidence of convergence ($\hat{R}<1.01$) (see Appendix 2 for more detailed model diagnostics). Participants translating Catalan words and participants translating Spanish words performed equivalently, as indicated by the regression coefficient of *Group* ($\beta$ = `r round(c1$b_group1$.median, 3)`, 95% CrI = [`r round(c1$b_group1$.lower, 3)`, `r round(c1$b_group1$.upper, 3)`], *p*(ROPE) = `r round(c1$b_group1$.rope, 3)`). Overall, participants responded less accurately to words with more CLPNs than to words with fewer CLPNs, regardless of the amount of phonological similarity between the presented word and its translation. This is indicated by the size of the regression coefficient of the two-way interaction between *Cognateness* and *CLPN* ($\beta$ = `r round(c1$b_neigh_n_h_lv$.median, 3)`, 95% CrI = [`r round(c1$b_neigh_n_h_lv$.lower, 3)`, `r round(c1$b_neigh_n_h_lv$.upper, 3)`], *p*(ROPE) = `r round(c1$b_neigh_n_h_lv$.rope, 3)`). As anticipated, participants' performance benefited from an increase in *Cognateness* ($\beta$ = `r round(c1$b_lv$.median, 3)`, 95% CrI = [`r round(c1$b_lv$.lower, 3)`, `r round(c1$b_lv$.upper, 3)`], *p*(ROPE) = `r round(c1$b_lv$.rope, 3)`), while the number of *CLND* had the opposite effect ($\beta$ = `r round(c1$b_neigh_n_h$.median, 3)`, 95% CrI = [`r round(c1$b_neigh_n_h$.lower, 3)`, `r round(c1$b_neigh_n_h$.upper, 3)`], *p*(ROPE) = `r round(c1$b_neigh_n_h$.rope, 3)`). @fig-epreds-1 illustrates the posterior of the average predictions of the model for words with different values of *Cognateness* and *CLPN*. @fig-coefs shows a graphic summary of the posterior distribution of the regression coefficients of interest.

```{r tbl-dataset}
#| label: tbl-dataset
sem <- function(x) mean(x) / (sqrt(length(x)))

#' Proportion adjusted from boundary values (Gelman, Hill & Vehtari, 2020)
#' 
prop_adj <- function(x, n){
    e <- (x+2)/(n+4)
    return(e)
}

#' Adjusted standard error of proportion (Gelman, Hill & Vehtari, 2020)
#' 
prop_adj_se <- function(x, n) {
    e <- (x+2)/(n+4)
    se <- sqrt(e*(1-e)/(n+4))
    return(se)
}

#' Adjusted standard error of proportion (Gelman, Hill & Vehtari, 2020)
#' 
prop_adj_ci <- function(x, n, .width = 0.95) {
    e <- (x+2)/(n+4)
    se <- sqrt(e*(1-e)/(n+4))
    ci <-  e + qnorm(c((1-.width)/2, (1-(1-.width)/2)))*se
    ci[1] <- ifelse(ci[1]<0, 0, ci[1]) # truncate at 0
    ci[2] <- ifelse(ci[2]>1, 1, ci[2]) # truncate at 1
    return(ci)
}

tbl_data <- list("Experiment 1" = dataset_1,
                 "Experiment 2" = dataset_2,
                 "Experiment 3" = dataset_3) |> 
    bind_rows(.id = "exp") |> 
    add_count(exp, group, participant_id, name = "trials") |> 
    summarise(n_trials = n(),
              correct = sum(correct),
              .by = c(group, exp, participant_id)) |> 
    mutate(correct = prop_adj(correct, n_trials)) |> 
    summarise(across(correct, lst(mean, sd, sem, min, max)),
              across(n_trials, lst(mean, sum, sd, min, max)),
              n_participants = n_distinct(participant_id),
              .by = c(group, exp)) |> 
    relocate(n_participants, matches("correct"))

tbl_data |> 
    gt(rowname_col = "group",
       groupname_col = "exp") |> 
    fmt_number(is.numeric, decimals = 2) |> 
    #gtExtras::gt_plt_dist(correct_list, type = "density") |> 
    fmt_integer(c(matches("sum|min|max"), n_participants)) |> 
    fmt_number(matches("correct"), scale_by = 100) |> 
    tab_spanner("Accuracy (%)", matches("correct")) |> 
    tab_spanner("Valid trials", matches("n_trials")) |> 
    cols_merge_uncert(n_trials_mean, n_trials_sd) |> 
    cols_merge_range(n_trials_min, n_trials_max) |> 
    cols_merge_uncert(n_trials_mean, n_trials_sd) |> 
    cols_merge_uncert(correct_mean, correct_sd) |> 
    cols_merge_range(correct_min, correct_max) |> 
    cols_label(n_trials_sum = "N trials",
               n_participants = "N",
               n_trials_mean = "Mean ± SD",
               n_trials_min = "Range",
               correct_mean = "Mean ± SD",
               correct_sem = "SE",
               correct_min = "Range") |> 
    tab_style(cell_text(style = "italic"),
              cells_column_labels()) |> 
    tab_style(cell_text(weight = "bold"),
              cells_column_spanners()) |> 
    tab_style(cell_text(align = "left"),
              list(cells_body(), cells_column_labels()))
```


```{r fig-epreds-1}
#| label: fig-epreds-1
#| fig-height: 6
#| fig-width: 10
get_epreds <- function(model, data, n = 100, 
                       lv = seq(0, 1, length.out = 100), 
                       neigh_n_h = c(0, 2, 4, 8, 12),
                       ...) {
    
    knowledge <- unique(model$data$knowledge)
    confidence <- unique(model$data$confidence)
    freq_zipf_2 <- mean(data$freq_zipf_2, na.rm = TRUE)
    group <- unique(model$data$group)
    nd <- expand_grid(freq_zipf_2, neigh_n_h, lv, knowledge, confidence, group)
    epreds <- tidybayes::add_epred_draws(nd, model, re_formula = NA, ...)
    
    return(epreds)
}

get_epreds(exp_1_m0, dataset_1) |> 
    mutate(neigh_n_h = paste0(neigh_n_h, " neighbours"),
           neigh_n_h = factor(neigh_n_h,
                              levels = paste0(c(0, 2, 4, 8, 12),
                                              " neighbours"),
                              ordered = TRUE)) |> 
    ggplot(aes(lv, .epred)) +
    facet_grid(group~neigh_n_h) +
    stat_lineribbon(aes(fill_ramp = after_stat(level)),
                    linewidth = 1/2,
                    .width = c(0.95, 0.89, 0.78, 0.67, 0.50)) +
    labs(x = "Cognateness\n(Levenshtein distance with correct translation)",
         y = "p(Correct)",
         colour = "Cross-language neighbourhood density",
         fill = "Cross-language neighbourhood density",
         linetype = "Cross-language neighbourhood density",
         fill_ramp = "CrI (%)") +
    scale_x_continuous(labels = scales::percent) +
    scale_y_continuous(labels = scales::percent) +
    scale_fill_brewer(palette = "Reds") +
    theme(legend.position = "none",
          legend.box = "vertical") 
```


## Discussion

In the present experiment, we investigated the extent to which cognateness (phonological similarity between translation equivalents) is sufficient for successful word translation, in the absence of semantic knowledge. We tested two groups of monolingual British English natives in a translation task that involved words in Catalan or Spanish, two languages participants reported having no familiarity with. Participants benefited strongly from cognateness, when the correct translation of the presented words in Catalan or Spanish had few English phonological neighbours with higher lexical frequency. This suggests that, in the absence of distractors, even naïve participants can efficiently use phonological similarity to succeed in a translation task. Our results suggest that word-to-word connections at the phonological level might play a strong role during L2 speech comprehension, specially in low-proficiency listeners.

# Experiment 2

## Methods

### Participants

```{r participants-numbers-2}
#| label: participants-numbers-2
participants_2 <- exp_participants |> 
    filter(group=="cat-SPA")

collection_dates <- participants_2 |> 
    pull(date) |> 
    range() |> 
    format("%B %dth, %Y")

n_participants <- n_distinct(participants_2)
n_participants_group <- table(participants_2$group)
participants_age <- participants_2 |> 
    summarise(across(age, lst(mean, sd, min, max))) 
n_female <- table(participants_2$gender)
```

Participants in Spain were contacted via announcements in Faculties, and were compensated 5€ or an Amazon voucher for the same value. Participants gave informed consent before providing any data and the study was conducted in accordance with ethical standards of the Declaration of Helsinki and the protocol was approved by the Drug Research Ethical Committee (CEIm) of the IMIM Parc de Salut Mar (2020/9080/I). Data collection took place from `r collection_dates[1]` to `r collection_dates[2]`. We collected data from `r n_participants` Spanish native participants living in Spain (*Mean* = `r round(participants_age$age_mean, 2)` years, *SD* = `r round(participants_age$age_sd, 2)`, Range = `r round(participants_age$age_min, 2)`-`r round(participants_age$age_max, 2)`, `r n_female` female). 



### Stimuli

Same list of Catalan stimuli as in Experiment 1.

### Procedure

Same as in Experiment 1.

### Data analysis

Same approach as in Experiment 1.

## Results


```{r dataset-2}
#| label: dataset-2
dataset <- exp_responses |> 
    filter(group=="cat-SPA") |> 
    group_split(group) |> 
    set_names(c("cat-SPA")) |> 
    janitor::clean_names() 

dataset <- split(exp_responses, exp_responses$group)
names(dataset) <- janitor::make_clean_names(names(dataset))
r_validity <- map(dataset, 
                  \(x) janitor::clean_names(table(x$response_type)))
n_participants <- map(dataset, \(x) n_distinct(x$participant_id))

excluded_lang <- participants_2 |> 
    filter(l2_writ_prod > 3 |
               l2_oral_comp > 3 |
               cat_oral_comp > 3 | 
               cat_writ_prod > 3 |
               spa_oral_comp > 3 |
               spa_writ_prod > 3) |> 
    nrow()

excluded_impairment <- participants_1 |> 
    filter(valid_status=="Language impairment") |> 
    nrow()

excluded_ntrials <- participants_1 |> 
    filter(valid_status=="Insufficient trials") |> 
    nrow()

dataset_clean <- exp_responses |> 
    filter(group=="cat-SPA") |> 
    dplyr::filter(valid_participant, valid_response) |> 
    group_split(group) |> 
    set_names(c("cat-SPA")) |> 
    janitor::clean_names() 

lengths <- dataset_2 |> 
    mutate(len = nchar(response)) |> 
    summarise(across(len,
                     lst(mean, median, sd, min, max)),
              .by = c(group)) 
```

We collected data for a total of `r format(nrow(dataset$cat_spa) + nrow(dataset$cat_spa), big.mark = ",")` trials completed by `r n_participants$cat_spa` unique participants. We excluded trials in which participants did not enter any text (*n* = `r r_validity$cat_spa["blank"]`), in which a response in a language other than English was provided (e.g., `agua`, *n* = `r r_validity$cat_spa["language"]`), in which participants did not provide a whole word (e.g., `f`, *n* = `r r_validity$cat_spa["incomplete"]`), and in which participants added comments to the experimenter (e.g., `unsure`, (*n* = `r r_validity$cat_spa["other"]`). In addition, we excluded data from participants that self-rated their oral and/or written skills in Catalan and Spanish, or any other second language as four or higher in a five-point scale (*n* = `r excluded_lang`), were diagnosed with a language (*n* = `r excluded_impairment`), or did not contribute more than 80% of valid trials (*n* = `r excluded_ntrials`). After applying trial-level and participant-level inclusion criteria, the resulting dataset included `r format(nrow(dataset_clean$cat_spa), big.mark = ",")` trials provided by `r n_distinct(dataset_clean$cat_spa$participant_id) + n_distinct(dataset_clean$cat_spa$participant_id)` unique participants. Of those trials, `r format(nrow(dataset_clean$cat_spa), big.mark = ",")` were provided by `r n_distinct(dataset_clean$cat_spa$participant_id)` unique participants who listened to Catalan words, and `r format(nrow(dataset_clean$cat_spa), big.mark = ",")` trials were provided by `r n_distinct(dataset_clean$cat_spa$participant_id)` unique participants who listened to Spanish words. Responses given by participants were `r round(lengths$len_mean, 2)` characters long on average (*Median* = `r round(lengths$len_median, 2)`, *SD* = `r round(lengths$len_sd, 2)`, Range = `r round(lengths$len_min, 2)`-`r round(lengths$len_max, 2)`).


```{r}
c2 <- gather_draws(exp_2_m0, `b_.*`, `sd_.*`, regex = TRUE) |> 
    summarise(across(.value, lst(.median = median, 
                                 .lower = \(x) hdi(x)[1],
                                 .upper = \(x) hdi(x)[2],
                                 .rope = \(x) mean(between(x, -0.1, 0.1))),
                     .names = "{fn}"))
c2 <- split(c2, c2$.variable)
names(c2) <- janitor::make_clean_names(names(c2)) 
```

@tbl-results-1 shows a summary of participants' accuracy across Experiments 1, 2, and 3. MCMC chains in the model showed strong evidence of convergence ($\hat{R}<1.01$) (see Appendix 2 for more detailed model diagnostics). Overall, participants responded less accurately to words with more CLPNs than to words with fewer CLPNs, regardless of the amount of phonological similarity between the presented word and its translation. This is indicated by the size of the regression coefficient of the two-way interaction between *Cognateness* and *CLPN* ($\beta$ = `r round(c2$b_neigh_n_h_lv$.median, 3)`, 95% CrI = [`r round(c2$b_neigh_n_h_lv$.lower, 3)`, `r round(c2$b_neigh_n_h_lv$.upper, 3)`], *p*(ROPE) = `r round(c2$b_neigh_n_h_lv$.rope, 3)`). As anticipated, participants' performance benefited from an increase in *Cognateness* ($\beta$ = `r round(c2$b_lv$.median, 3)`, 95% CrI = [`r round(c2$b_lv$.lower, 3)`, `r round(c2$b_lv$.upper, 3)`], *p*(ROPE) = `r round(c2$b_lv$.rope, 3)`), while the number of *CLND* had the opposite effect ($\beta$ = `r round(c2$b_neigh_n_h$.median, 3)`, 95% CrI = [`r round(c2$b_neigh_n_h$.lower, 3)`, `r round(c2$b_neigh_n_h$.upper, 3)`], *p*(ROPE) = `r round(c2$b_neigh_n_h$.rope, 3)`). @fig-epreds-1 illustrates the posterior of the average predictions of the model for words with different values of *Cognateness* and *CLPN*.

```{r fig-epreds-2}
#| label: fig-epreds-2
#| fig-height: 3.5
#| fig-width: 10
get_epreds(exp_2_m0, dataset_2) |> 
    mutate(neigh_n_h = paste0(neigh_n_h, " neighbours"),
           neigh_n_h = factor(neigh_n_h,
                              levels = paste0(c(0, 2, 4, 8, 12),
                                              " neighbours"),
                              ordered = TRUE)) |> 
    ggplot(aes(lv, .epred)) +
    facet_wrap(~neigh_n_h, nrow = 1) +
    stat_lineribbon(aes(fill_ramp = after_stat(level)),
                    linewidth = 1/2,
                    .width = c(0.99, 0.95, 0.89, 0.78, 0.67, 0.50)) +
    labs(x = "Cognateness\n(Levenshtein distance with correct translation)",
         y = "p(Correct)",
         colour = "Cross-language neighbourhood density",
         fill = "Cross-language neighbourhood density",
         linetype = "Cross-language neighbourhood density",
         fill_ramp = "CrI (%)") +
    scale_x_continuous(labels = scales::percent) +
    scale_y_continuous(labels = scales::percent) +
    scale_fill_brewer(palette = "Reds") +
    theme(legend.position = "none",
          legend.box = "vertical") 
```

## Discussion

Experiment 2 was an extension of Experiment 1 to a population of monolinguals whose native language is typologically similar to the presented language. Particularly, we presented Catalan words to Spanish native speakers who were reportedly unfamiliar with Catalan. Our results indicate a similar pattern of results as those in Experiment 1: participants were able to provide correct translations of presented Catalan words, provided that the Catalan words shared some degree of phonological similarity with their Spanish translation, and that the number of phonological neighbours with higher lexical frequency was reduced. In contrasts with the results in Experiment 1, the positive impact of phonological similarity on participants' performance in Experiment 2 was more resilient to the interference of phonological neighbours. English natives in Experiment 1 provided significantly less accuracy responses when more than four phonological neighbors were present (even when translating high-similarity words), compared to when only one or neighbour were present. Spanish participants in Experiment 2 benefited from phonological similarity, even when eight neighbours were present. Spanish participants' performance declined after 8 neighbours, and was evident at 12 neighbours. Overall, this suggests that participants in Experiment 2, who were natives of a typologically similar language (Spanish) to the presented language (Catalan) benefited more strongly from phonological similarity than participants in Experiment 1, who were natives of typologically less similar language (English) to the presented language (Catalan, Spanish).

Participants from both Experiment 1 and 2 benefited strongly from phonological similarity to correctly translate words from a non-native, reportedly unfamiliar language. This pattern of results holds for most of the presented stimuli, but some low-similarity Catalan and Spanish words were responded to surprisingly accurately by English listeners. Given that participants were reportedly unfamiliar with both languages, it was expected that participants would be very unlikely to provide correct translations for words sharing little to no phonological similarity to their correct translation. @tbl-surprises a list of Catalan and Spanish words to which participants provided responses with $\geq$ 10 average accuracy.

```{r tbl-surprises}
#| label: tbl-surprises
#| tbl-cap: "List of items with unexpectedly high accuracy: the Levenshtein similarityscore betwen th presented word (in Catalan or Spanish) and their correct Enlgish or Spanish translation is zero, but participants, who are reportedly unfamiliar with the presented language, were on average >10% likely to guess the correct translation."
tbl_data <- list("Experiment 1" = dataset_1,
                 "Experiment 2" = dataset_2) |>
    bind_rows(.id = "exp") |> 
    summarise(n = n(),
              correct = sum(correct),
              .by = c(exp, group, lv, translation)) |> 
    mutate(accuracy = prop_adj(correct, n),
           accuracy_se = prop_adj_se(correct, n)) |>    
    dplyr::filter(lv==0, accuracy >= 0.1) |> 
    select(exp, translation, group, accuracy, accuracy_se) |> 
    arrange(exp, group, desc(accuracy))

tbl_data |> 
    gt(rowname_col = "translation",
       groupname_col = c("exp", "group")) |> 
    fmt_number(is.numeric, scale_by = 100) |> 
    cols_label(accuracy = "Accuracy (%)", accuracy_se = "SE")
```


It is likely that participants had prior knowledge of these words despite having reported little to no familiarity with the presented language (Catalan or Spanish). One possibility is that participants had previously encountered these words embedded in English linguistic input. Spanish words percolate English speech with relative frequency, via different sources such as popular culture, songs, TV programs, etc. In addition, words from languages other than Spanish, but with high similarity to the Spanish words (e.g., cognates from Italian or French) might appear in English speech as well. Such prior knowledge might not be specific to the low-similarity words highlighted before. Participants may also have had prior knowledge about higher-similarity words, which could have contributed to participants responding to such words more accurately than without such prior knowledge. In the case of higher-similarity words, it is more difficult to disentangle the extent to which participants' accuracy is a function of pure phonological similarity, or prior knowledge they had about the meaning of Spanish words. To investigate this issue, we run Experiment 3.

[ADD HERE LEXICAL FREQUENCY OF CATALAN AND SPANISH WORDS IN ENGLISH, AND OF CATALAN WORDS IN SPANISH]


# Experiment 3

Experiment 3 is a replication of Experiment 1, in which we collected additional data about participants' prior familiarity with the presented Catalan and Spanish words, in addition to the same translation task presented to participants in Experiment 1.

## Methods

### Participants


```{r participants-numbers-3}
#| label: participants-numbers-3
participants_3 <- quest_participants |> 
    filter(group!="cat-SPA")

collection_dates <- participants_3 |> 
    pull(date) |> 
    range() |> 
    format("%B %dth, %Y")

n_participants <- n_distinct(participants_3)
n_participants_group <- table(participants_3$group)
participants_age <- participants_3 |> 
    summarise(across(age, lst(mean, sd, min, max))) 
n_female <- table(participants_3$gender)
```

Data collection took place from `r collection_dates[1]` to `r collection_dates[2]`. We collected data from `r n_participants` British English native participants living in United Kingdom (*Mean* = `r round(participants_age$age_mean, 2)` years, *SD* = `r round(participants_age$age_sd, 2)`, Range = `r round(participants_age$age_min, 2)`-`r round(participants_age$age_max, 2)`, `r n_female` female). 


```{r}
#| label: participants-excluded-3
excluded_lang <- participants_3 |> 
    filter(l2_writ_prod > 3 |
               l2_oral_comp > 3 |
               cat_oral_comp > 3 | 
               cat_writ_prod > 3 |
               spa_oral_comp > 3 |
               spa_writ_prod > 3) |> 
    nrow()

excluded_impairment <- participants_3 |> 
    filter(valid_status=="Language impairment") |> 
    nrow()

excluded_ntrials <- participants_3 |> 
    filter(valid_status=="Insufficient trials") |> 
    nrow()
```

Participants were recruited via Prolific (5£ compensation) and SONA (compensation in academic credits), and gave informed consent before providing any data and the study was conducted in accordance with ethical standards of the Declaration of Helsinki and the protocol was by the University of Oxford Medical Sciences Inter-Divisional Research Ethics Committee (IDREC) (R60939/RE007). Participants were asked to complete the experiment using a laptop in a quiet place with good internet connection. We excluded data from participants that (a) self-rated their oral and/or written skills in Catalan and Spanish, or any other second language as four or higher in a five-point scale (*n* = `r excluded_lang`), (b) were diagnosed with a language (*n* = `r excluded_impairment`), or (c) did not contribute more than 80% of valid trials (*n* = `r excluded_ntrials`).

### Stimuli

Same as in Experiment 1.

### Procedure

[TO BE COMPLETED]

### Data analysis

Same approach as in Experiment 1.



## Results


```{r dataset-3}
#| label: dataset-3
dataset <- quest_responses |> 
    filter(group!="cat-SPA") |> 
    group_split(group) |> 
    set_names(c("spa-ENG", "cat-ENG")) |> 
    janitor::clean_names() 

dataset <- split(quest_responses, quest_responses$group)
names(dataset) <- janitor::make_clean_names(names(dataset))
r_validity <- map(dataset, 
                  \(x) janitor::clean_names(table(x$response_type)))
n_participants <- map(dataset, \(x) n_distinct(x$participant_id))

excluded_lang <- participants_3 |> 
    filter(l2_writ_prod > 3 |
               l2_oral_comp > 3 |
               cat_oral_comp > 3 | 
               cat_writ_prod > 3 |
               spa_oral_comp > 3 |
               spa_writ_prod > 3) |> 
    nrow()

excluded_impairment <- participants_3 |> 
    filter(valid_status=="Language impairment") |> 
    nrow()

excluded_ntrials <- participants_3 |> 
    filter(valid_status=="Insufficient trials") |> 
    nrow()

dataset_clean <- quest_responses |> 
    filter(group!="cat-SPA") |> 
    dplyr::filter(valid_participant, valid_response) |> 
    group_split(group) |> 
    set_names(c("spa-ENG", "cat-ENG")) |> 
    janitor::clean_names() 
```

We collected data for a total of `r format(nrow(dataset$cat_eng) + nrow(dataset$spa_eng), big.mark = ",")` trials completed by `r n_participants$cat_eng + n_participants$spa_eng` unique participants. Of those trials, `r format(nrow(dataset$cat_eng), big.mark = ",")` were provided by `r n_participants$cat_eng` unique participants who listened to Catalan words, and `r format(nrow(dataset$spa_eng), big.mark = ",")` trials were provided by `r n_participants$spa_eng` unique participants who listened to Spanish words. We excluded trials in which participants did not enter any text (*n* = `r dataset$cat_eng$blank`), in which a response in a language other than English was provided (e.g., `agua`, *n* = `r dataset$cat_eng$language`), in which participants did not provide a whole word (e.g., `f`, *n* = `r dataset$cat_eng$incomplete`), and in which participants added comments to the experimenter (e.g., `unsure`, (*n* = `r dataset$cat_eng$other`). In addition, we excluded data from participants that self-rated their oral and/or written skills in Catalan and Spanish, or any other second language as four or higher in a five-point scale (*n* = `r excluded_lang`), were diagnosed with a language (*n* = `r excluded_impairment`), or did not contribute more than 80% of valid trials (*n* = `r excluded_ntrials`). After applying trial-level and participant-level inclusion criteria, the resulting dataset included `r format(nrow(dataset_clean$cat_eng) + nrow(dataset_clean$cat_eng), big.mark = ",")` trials provided by `r n_distinct(dataset_clean$cat_eng$participant_id) + n_distinct(dataset_clean$cat_eng$participant_id)` unique participants. Of those trials, `r format(nrow(dataset_clean$cat_eng), big.mark = ",")` were provided by `r n_distinct(dataset_clean$cat_eng$participant_id)` unique participants who listened to Catalan words, and `r format(nrow(dataset_clean$spa_eng), big.mark = ",")` trials were provided by `r n_distinct(dataset_clean$spa_eng$participant_id)` unique participants who listened to Spanish words.


```{r}
lengths <- dataset_3 |> 
    mutate(len = map_int(strsplit(response, ""), length)) |> 
    summarise(across(len,
                     lst(mean, median, sd, min, max)),
              .by = c(group)) 
lengths <- split(lengths, lengths$group)

names(lengths) <- janitor::make_clean_names(names(lengths))
```

Responses given by English participants to Catalan presented words were `r round(lengths$cat_eng$len_mean, 2)` characters long on average (*Median* = `r round(lengths$cat_eng$len_median, 2)`, *SD* = `r round(lengths$cat_eng$len_sd, 2)`, Range = `r round(lengths$cat_eng$len_min, 2)`-`r round(lengths$cat_eng$len_max, 2)`), while their translations to Spanish responses were `r round(lengths$spa_eng$len_mean, 2)` characters long on average (*Median* = `r round(lengths$spa_eng$len_median, 2)`, *SD* = `r round(lengths$spa_eng$len_sd, 2)`, Range = `r round(lengths$spa_eng$len_min, 2)`-`r round(lengths$spa_eng$len_max, 2)`).

MCMC chains in the model showed strong evidence of convergence ($\hat{R}<1.01$) (see Appendix 2 for more detailed model diagnostics). Overall, participants reported prior knowledge more often for that Spanish words with unexpectedly high accuracy (see Discussion in Experiment 2) than for words with expected accuracy (see @fig-knowledge). Participants reported prior knowledge of Catalan words with unexpected accuracy as often as those with expected accuracy. This suggests that participants in Experiment 1 may have relied, to some extent, on their prior knowledge about form-meaning mappings to correctly translate some Spanish words. To isolate such an effect of prior Spanish knowledge, we run the same analysis as in Experiment 1 on the newly collected translations from Experiment 3, now excluding responses to words in which participants reported prior knowledge.

```{r fig-knowledge}
#| label: fig-knowledge
#| fig-width: 7
#| fig-height: 5
#| fig-cap: "Catalan/Spanish prior word knowledge as reported by English native participants in Experiment 3. (A) Average proportion of participants that reported prior knowledge for words with surprisingly high accuracy (no phonological similarity with the correct translation, accuracy higher than 10%), and for words with expected accuracy (low similarity-low accuracy, or high similarity-high accuracy). (B) Average accuracy for words with expected and unexpected accuracy."
dataset_3 |> 
    mutate(surprise = translation %in% tbl_data$translation) |> 
    summarise(knowledge = mean(knowledge),
              .by = c(translation, group, surprise)) |> 
    select(translation, group, surprise, knowledge) |> 
    mutate(surprise = if_else(
        surprise,
        "Unexpectedly high accuracy\n(no phonological similarity)",
        "Expected accuracy")) |> 
    ggplot(aes(knowledge, group, shape = group)) +
    facet_wrap(~surprise, ncol = 1, switch = "y") +
    stat_summary(fun = mean,
                 size = 2,
                 geom = "point") +
    stat_summary(fun.data = mean_se,
                 geom = "errorbar",
                 width = 0.1) +
    labs(x = "Prior knowledge reported",
         y = "Group") +
    scale_x_continuous(labels = scales::percent) +
    
    dataset_3 |> 
    mutate(surprise = translation %in% tbl_data$translation) |> 
    mutate(surprise = if_else(
        surprise,
        "Unexpectedly high accuracy\n(no phonological similarity)",
        "Expected accuracy")) |> 
    summarise(correct = sum(correct),
              n = n(),
              .by = c(translation, group, surprise, knowledge)) |> 
    mutate(accuracy = prop_adj(correct, n)) |> 
    select(translation, group, accuracy, surprise, knowledge) |> 
    ggplot(aes(accuracy, group, shape = group)) +
    facet_wrap(~surprise, ncol = 1, switch = "y") +
    stat_summary(fun = mean,
                 size = 2,
                 position = position_dodge(width = 0.75),
                 geom = "point") +
    stat_summary(fun.data = mean_se,
                 geom = "errorbar",
                 width = 0.1,
                 position = position_dodge(width = 0.75)) +
    labs(x = "Accuracy",
         y = "Group") +
    scale_x_continuous(labels = scales::percent,
                       limits = c(0, 1)) +
    theme(axis.title.y = element_blank(),
          axis.text.y = element_blank(),
          strip.text = element_blank()) +
    
    plot_layout(nrow = 1, guides = "collect") &
    plot_annotation(tag_levels = "A") &
    labs(colour = "Group",
         shape = "Group") &
    theme(panel.border = element_rect(fill = NA,
                                      colour = "grey",
                                      linewidth = 1),
          axis.line = element_blank(),
          axis.text.y = element_blank(),
          axis.title.y = element_blank(),
          axis.ticks.y = element_blank(),
          legend.position = "top",
          panel.grid.major.x = element_line(colour = "grey",
                                            linetype = "dotted"),
          panel.grid.major.y = element_blank()) 
```

```{r}
c3 <- gather_draws(exp_3_m1, `b_.*`, `sd_.*`, regex = TRUE) |> 
    summarise(across(.value, lst(.median = median, 
                                 .lower = \(x) hdi(x)[1],
                                 .upper = \(x) hdi(x)[2],
                                 .rope = \(x) mean(between(x, -0.1, 0.1))),
                     .names = "{fn}"))
c3 <- split(c3, c3$.variable)
names(c3) <- janitor::make_clean_names(names(c3)) 
```

Participants translating Catalan words and participants translating Spanish words, as indicated by the regression coefficient of *Group* ($\beta$ = `r round(c3$b_group1$.median, 3)`, 95% CrI = [`r round(c3$b_group1$.lower, 3)`, `r round(c3$b_neigh_n_h_lv$.upper, 3)`], *p*(ROPE) = `r round(c3$b_group1$.rope, 3)`). Overall, participants responded less accurately to words with more CLPNs than to words with fewer CLPNs, regardless of the amount of phonological similarity between the presented word and its translation. This is indicated by the size of the regression coefficient of the two-way interaction between *Cognateness* and *CLPN* ($\beta$ = `r round(c3$b_neigh_n_h_lv$.median, 3)`, 95% CrI = [`r round(c3$b_neigh_n_h_lv$.lower, 3)`, `r round(c3$b_neigh_n_h_lv$.upper, 3)`], *p*(ROPE) = `r round(c3$b_neigh_n_h_lv$.rope, 3)`). As anticipated, participants' performance benefited from an increase in *Cognateness* ($\beta$ = `r round(c3$b_lv$.median, 3)`, 95% CrI = [`r round(c3$b_lv$.lower, 3)`, `r round(c3$b_lv$.upper, 3)`], *p*(ROPE) = `r round(c3$b_lv$.rope, 3)`), while the number of *CLND* had the opposite effect ($\beta$ = `r round(c3$b_neigh_n_h$.median, 3)`, 95% CrI = [`r round(c3$b_neigh_n_h$.lower, 3)`, `r round(c3$b_neigh_n_h$.upper, 3)`], *p*(ROPE) = `r round(c3$b_neigh_n_h$.rope, 3)`). @fig-epreds-1 illustrates the posterior of the average predictions of the model for words with different values of *Cognateness* and *CLPN*.


```{r fig-epreds-3}
#| label: fig-epreds-3
#| fig-height: 6
#| fig-width: 10
get_epreds(exp_3_m1, dataset_3) |> 
    mutate(neigh_n_h = paste0(neigh_n_h, " neighbours"),
           neigh_n_h = factor(neigh_n_h,
                              levels = paste0(c(0, 2, 4, 8, 12),
                                              " neighbours"),
                              ordered = TRUE)) |> 
    ggplot(aes(lv, .epred)) +
    facet_grid(group~neigh_n_h) +
    stat_lineribbon(aes(fill_ramp = after_stat(level)),
                    linewidth = 1/2,
                    .width = c(0.99, 0.95, 0.89, 0.78, 0.67, 0.50)) +
    labs(x = "Cognateness\n(Levenshtein distance with correct translation)",
         y = "p(Correct)",
         fill_ramp = "CrI (%)") +
    scale_x_continuous(labels = scales::percent) +
    scale_y_continuous(labels = scales::percent) +
    scale_fill_brewer(palette = "Reds") +
    theme(legend.position = "top",
          legend.box = "vertical") 
```

## Discussion

# Joint analyses

Across Experiments 1 and 3, we found strong evidence that participants efficiently exploited phonological similarity to provide accurate translations for words in an unfamiliar language, provided that few phonological neighbours of higher lexical frequency were present. @fig-coefs summarizes the posterior distribution of the regression coefficients of the models in Experiments 1 to 3.

```{r fig-coefs}
#| label: fig-coefs
#| fig-width: 9
#| fig-height: 5
var_labels <- c("b_Intercept" = "Intercept",
                "b_freq_zipf_2" = "Frequency",
                "b_lv" = "Cognateness",
                "b_neigh_n_h" = "CLPN",
                "b_neigh_n_h:lv" = "Cognateness × CLPN",
                "b_group1" = "cat-ENG vs. spa-ENG") 

list(exp_1_m0, exp_2_m0, exp_3_m1) |> 
    set_names(paste0("Experiment ", 1:3)) |> 
    map_dfr(gather_draws, `b_.*`, regex = TRUE, .id = "exp") |> 
    dplyr::filter(.variable != "b_Intercept") |> 
    mutate(.value = .value/4,
           .variable = factor(.variable, 
                              levels = names(var_labels),
                              labels = var_labels)) |> 
    ggplot(aes(.value, reorder(exp, desc(exp)))) +
    facet_wrap(~.variable, scales = "free_x") +
    annotate(geom = "rect",
             xmin = -0.1/4,
             xmax = 0.1/4,
             ymin = -Inf,
             ymax = Inf,
             fill = "grey",
             alpha = 3/4,
             colour = NA) +
    geom_vline(xintercept = 0, 
               colour = "grey",
               linewidth = 3/4) +
    stat_slab(aes(fill = after_stat(level)), 
              point_interval = mode_hdi, 
              .width = c(0.95, 0.89, 0.67, 1),
              outline_bars = TRUE,
              colour = "white",
              linewidth = 0.1,
              breaks = 20,
              normalize = "panels") +
    stat_spike(at = "median",
               size = 1,
               aes(linetype = "Median"),
               normalize = "panels") +
    labs(x = "Estimate", y = "Experiment",
         colour = "CrI",
         fill = "CrI",
         linetype = "") +
    scale_fill_brewer(palette = "Reds") +
    theme(legend.position = c(1, 0),
          legend.box = "horizontal",
          legend.justification = c(1, 0),
          axis.title.x = element_blank(),
          axis.title.y = element_blank())
```


# General discussion

# References


