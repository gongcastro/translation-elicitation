```{r}
#| label: setup
library(targets)
library(dplyr)
library(tidyr)
library(ggplot2)
library(patchwork)
library(gt)
library(gtExtras)
library(brms)
library(tidybayes)

set.seed(1234)

# import objects
tar_config_set(store = here::here("_targets"),
               script = here::here("_targets.R"))
tar_load_globals()
tar_load(participants)
tar_load(exp_participants)
tar_load(exp_responses)
tar_load(quest_responses)
tar_load(quest_participants)
tar_load(stimuli)
tar_load(epreds)
tar_load(dataset_1)
tar_load(dataset_2)
tar_load(dataset_3)
tar_load(exp_1_m0)
tar_load(exp_2_m0)
tar_load(exp_3_m1)


theme_set(theme_ggdist() + 
              theme(panel.grid.major.y = element_line(colour = "grey",
                                                      linetype = "dotted")))

```

# Introduction


::: {.content-visible when-format="pdf"}

When some German speakers listened to the song *The Power* by SNAP! (*Coyote Ugly*, 2000), many of them misheard the line "I've got the power" as *Agatha Bauer!* When Michael Jackson vocalised "ma-ma-coo-sah" in his song *Wanna Be Startin' Somethin'* (*Thriller*, 1982), some Dutch speakers understood *mama appelsap* [mama applejuice]. When Spanish speakers listen to the line "Circumvent your thick ego" from the song *Pictures* by System of a Down (*Steal this Album!*, 2002), they tend to hear *Sácame de aquí, King-Kong* [take me out of here, King-Kong]. When Japanese speakers listen to the line "I want to hold your hand" in the homonym song by The Beatles (*Meet The Beatles*, 1964), they often mishear *Aho na hōnyōhan* [stupid urinator]. Outrageous as these examples might sound to speakers of other languages, many readers may know a few cases in their own native language. This auditory illusion can feel quite real, and often inevitable after the first time it happens. In Japanese, this phenomenon takes the name *Soramimi* (lit. "empty ear"), after the *Soramimi Hour*---a section of the program *Tamori Club* hosted by comedian Morita Kazuyoshi (Tamori)---in which instances of this illusion were presented with comedy purposes. Soramimi is a particular case of *homophonic translation*: words or phrases in one language are translated into similar-sounding words and phrases in a different language, without necessarily preserving the meaning. This phenomenon extends beyond song lyrics, and may occur anytime when listening to non-native speech. For instance, when listening to "You know me?" (\textipa{/ju: n@U mi:}/) in English, Japanese-native listeners tend to understand *yunomi* (/\textipa{jUnomi}/), which is a type of Japanese teacup. Overall, the occurence of Soramimi---and more generally, homophonic translations---suggests that speech in an unfamiliar language has the potential to activate lexical representations in the native language.

Although homophonic translations do not necessarily take place with the preservation of meaning---which often results in outrageous mistranslations like the ones aforementioned---homophonic translations sometimes may lead to the activation of the intended meaning (i.e., a correct translation). For example, imagine a Spanish native with no previous familiarity with any other language listening to French for the first time. This listener may encounter the word /\textipa{"pOKt}/ (*porte*), translation of door in French. When comparing the phonemic transcription of the French to that of its Spanish translation /\textipa{"pweR.ta}/ (*puerta*), one may be led to infer that these two words do not have much overlap. But one thing to note is that the voiced uvular fricative consonant /\textipa{K}/ and the open-mid back rounded vowel /\textipa{O}/, which do not exist in Spanish as phonemic categories, can be perceived as allophonic variations the native (Spanish) phonemes /\textipa{R}/ and /o/, as described by assimilation models of phoneme perception [@best2001discrimination; @best1988examination]. Although this phonetic mismatch between the non-native word and its translation can have a noticeable toll on comprehension @cutler2004patterns, the Spanish listener may still be able to activate the phonological representation of /\textipa{"pweR.ta}/ (*puerta*) from hearing the word-form /\textipa{"pOKt}/ (*porte*), ultimately allowing them to access the correct semantic representation. Given enough phonological similarity between the non-native word and its translation in the native language, a naïve listener may succeed at word comprehension when listening to an unfamiliar language. Such phonological similarity between translation equivalents---known as *cognateness*---is common across many languages, and is often due to typological closeness and/or socio-historical events involving the speakers of these languages (e.g., migration, social contact). For example, Romance languages such as Spanish and Catalan share many form-similar translation equivalents  [@schepens2012distributions], as in the case of *puerta* and *porta* (*door* in Spanish and Catalan, respectively). Given no prior knowledge of either language, a Spanish native speaker is likely to be much more successful at correctly translating Catalan *porta* than English *door*, due to the phonological and orthographic similarity of the former to the equivalent in their native language.

While Soramimi, and more generally homophonic translation, are popular and frequent phenomena, the psycholinguistic literature on the topic is relatively scarce, and has mostly focused on the phonological variables involved in the occurence of Soramimi. For instance, @otake2007interlingual analysed 194 instances of Soramimi, broadcasted between 1992 and 2007 by the TV show Soramimi hour. These instances consisted of homophonic translations of English song lyrics to Japanese words and phrases. The vast majority of the reported instances were phrases (96%) and a few, to single words (4%). The phonetic features of the presented English lyrics were preserved with relatively high variability, with some Japanese resulting words or phrases sharing high overlap with the original English strings of sounds, and some sharing very little overlap. When analysing the few instances of homophonic translations of single words, the author identified three phonological processes that might explain how Japanese listeners reconstructed the English input to generate the Japanese words: insertion (e.g., *cry* /\textipa{kraI}/ to *kurai*, /\textipa{kuRai}/ [dark]), deletion (e.g., *go*, /\textipa{goU}/ to *go*, /go/ [go], and alternation (e.g., *low*, /\textipa{loU}/ to *rou*, /\textipa{Roo}/ [wax]). This suggests that the reported homophonic translations can be explained, to some extent, as the result of the Japanese listeners accommodating the strings of English sounds to the Japanese phoneme inventory and Japanese phonotactics [@peperkamp2008perceptual; @dupoux1999epenthetic]. Native prosody also constrains the occurence of homophonic translations. For instance, @kentner2015rhythmic presented in the auditory modality French-native and German-native participants with English songs with lyrics. Homophonic translations followed language-specific segmentation strategies. Most homophonic translations reported by German-native participants resulted from inserting phrase and word boundaries *before* stressed syllables (following the more frequent trochaic patter in German). On the contrary, most homophonic translations reported by French-native participants resulted from inserting phrase and word boundaries *after* stressed syllables, following the more frequent iambic pattern in French.

While the previous studies have address the phonological aspects of homophonic translation, little attention has been paid to the dynamics of lexical activation and competition that underlay this phenomenon. In the present work we investigated the interplay between phonological similarity and phonological neighbourhood density to examine how listeners of an unfamiliar language activate lexical representations in their native language. In particular, we studied how the presence of phonological neighbours in the native language lexicon affects the extent to which listeners are able to activate the correct translation of an auditorily presented word in an unfamiliar language. We used a translation elicitation task in which participants listened to words from an unfamiliar language, and then tried to guess their translation in their native language. We will henceforth refer to the auditory-presented words heard by participants on each trial as presented words, and the correct translation for the presented words as target translations. We explored listeners’ reliance on phonological similarity to succeed in the task by manipulating the amount of phonological similarity between the presented words and their target translations. Since participants were unfamiliar with the presented language, they could only use phonological similarity between the presented language and their native language to successfully translate the words. We therefore predicted that participants’ performance should increase when the translation pairs are phonologically more similar. We also predicted that there is a minimum threshold of phonological similarity to be sufficient for translation.

We further investigated the effect of phonological competitors. Even if the presented word and its target translation share high phonological similarity, participants may still provide incorrect translations if the presented word also shares high phonological similarity with other words in the native language. Words with denser phonological neighbourhoods are recognised more slowly and less accurately than words with sparser phonological neighbourhoods [@dufour2003lexical; @goldinger1989priming; @hamburger1996phonological; @luce1990similarity; @luce1998recognizing]. This is especially true if such neighbours share higher phonological similarity with the presented word, or are lexically more frequent than the target translation [@luce1998recognizing]. Since the interference effects of phonological neighbourhood density has also been reported across languages [@weber2004lexical], we hypothesised that the size of the facilitatory role of phonological similarity would be inversely proportional to the amount of higher-frequency phonological neighbours of the presented word in the target language. In the case of non-native speech recognition, the presence of cross-linguistic pairs which are phonologically similar but differ in meaning (e.g., false friends) may act as distractors during lexical access, obstructing the selection of the appropriate target translation in the listener’s lexicon. For instance, @otwinowska2019more reported that false friends were disadvantaged relative to non-cognates by Polish second language learners, in contrast to cognates which were known better. It is therefore important to investigate the joint effect of both cognates and false friends when investigating the effect cross-linguistic phonological similarity has on word recognition in a foreign language. This could shed light on available strategies and challenges associated with the early stages of second language acquisition. For instance, one could expect English participants to incorrectly translate the Spanish word *botón* as *bottom* instead of as its correct translation *button*, due to combined effect of the close phonological similarity between *botón* and *bottom* along with the high lexical frequency of *bottom* relative to *button*. To test this prediction, we developed a lexical frequency-dependent measure of cross-language phonological neighbourhood density, in which a neighbour is counted only if it is higher frequency and is one phoneme apart from the presented word. If the phonological neighbourhood density of the target translation affects participants’ performance negatively, this would suggest that competitors in the native language affect recognition of non-native words during recognition of foreign speech. We conducted a series of three experiments to test these predictions. 

In Experiment 1, we collected data from two groups of British English natives living in the United Kingdom. One group was presented with Catalan words, the other with Spanish words. We examined the extent to which participants were able to use the phonological similarity between the presented word (in Catalan or Spanish) and its target translation to provide accurate responses. In Experiment 2, we tested a group of (European) Spanish natives in the same task, who were presented with Catalan words. Catalan and Spanish are both Romance languages whose close typological distance is reflected in the fact that they share many cognates, where English is a Germanic language that shares considerably fewer cognates with Catalan and Spanish. By testing participants translating words from typologically close or distant languages, we expected to widen the range of the phonological similarity scores of the translation pairs involved in the experimental task, therefore allowing us to explore potential cross-language differences in participant’s performance. One unexpected finding was that participants in Experiments 1 and 2 were surprisingly good at translating a subset of words which had low phonological similarity with their correct translation. We were concerned that this may be caused by some prior knowledge of specific words by our participants, as some Spanish words are commonly seen in media or product labels, making them familiar even to monolingual speakers of English. In Experiment 3, we collected additional data from a new group of British English natives. The design was closely modelled after Experiment 1, except that after providing their response in each trial, participants reported whether they had previous knowledge of the presented word. In the final section of this paper, we present analyses on the joint data sets of Experiments 1 to 3.

:::

::: {.content-visible when-format="docx"}

When some German speakers listened to the song *The Power* by SNAP! (*Coyote Ugly*, 2000), many of them misheard the line "I've got the power" as *Agatha Bauer!* When Michael Jackson vocalised "ma-ma-coo-sah" in his song *Wanna Be Startin' Somethin'* (*Thriller*, 1982), some Dutch speakers understood *mama appelsap* [mama applejuice]. When Spanish speakers listen to the line "Circumvent your thick ego" from the song *Pictures* by System of a Down (*Steal this Album!*, 2002), they tend to hear *Sácame de aquí, King-Kong* [take me out of here, King-Kong]. When Japanese speakers listen to the line "I want to hold your hand" in the homonym song by The Beatles (*Meet The Beatles*, 1964), they often mishear *Aho na hōnyōhan* [stupid urinator]. Outrageous as these examples might sound to speakers of other languages, many readers may know a few cases in their own native language. This auditory illusion can feel quite real, and often inevitable after the first time it happens. In Japanese, this phenomenon takes the name *Soramimi* (lit. "empty ear"), after the *Soramimi Hour*---a section of the program *Tamori Club* hosted by comedian Morita Kazuyoshi (Tamori)---in which instances of this illusion were presented with comedy purposes. Soramimi is a particular case of *homophonic translation*: words or phrases in one language are translated into similar-sounding words and phrases in a different language, without necessarily preserving the meaning. This phenomenon extends beyond song lyrics, and may occur anytime when listening to non-native speech. For instance, when listening to "You know me?" (/ju: nəʊ mi:/) in English, Japanese-native listeners tend to understand *yunomi* (/jʊnomi/), which is a type of Japanese teacup. Overall, the occurence of Soramimi---and more generally, homophonic translations---suggests that speech in an unfamiliar language has the potential to activate lexical representations in the native language.

Although homophonic translations do not necessarily take place with the preservation of meaning---which often results in outrageous mistranslations like the ones aforementioned---homophonic translations sometimes may lead to the activation of the intended meaning (i.e., a correct translation). For example, imagine a Spanish native with no previous familiarity with any other language listening to French for the first time. This listener may encounter the word /pɔKt/ (*porte*), translation of door in French. When comparing the phonemic transcription of the French to that of its Spanish translation /pweɾta/ (*puerta*), one may be led to infer that these two words do not have much overlap. But one thing to note is that the voiced uvular fricative consonant /ʁ/ and the open-mid back rounded vowel /ɔ/, which do not exist in Spanish as phonemic categories, can be perceived as allophonic variations the native (Spanish) phonemes /ʁ/ and /o/, as described by assimilation models of phoneme perception [@best2001discrimination; @best1988examination]. Although this phonetic mismatch between the non-native word and its translation can have a noticeable toll on comprehension @cutler2004patterns, the Spanish listener may still be able to activate the phonological representation of /pweɾta}/ (*puerta*) from hearing the word-form /pɔʁt/ (*porte*), ultimately allowing them to access the correct semantic representation. Given enough phonological similarity between the non-native word and its translation in the native language, a naïve listener may succeed at word comprehension when listening to an unfamiliar language. Such phonological similarity between translation equivalents---known as *cognateness*---is common across many languages, and is often due to typological closeness and/or socio-historical events involving the speakers of these languages (e.g., migration, social contact). For example, Romance languages such as Spanish and Catalan share many form-similar translation equivalents  [@schepens2012distributions], as in the case of *puerta* and *porta* (*door* in Spanish and Catalan, respectively). Given no prior knowledge of either language, a Spanish native speaker is likely to be much more successful at correctly translating Catalan *porta* than English *door*, due to the phonological and orthographic similarity of the former to the equivalent in their native language.

While Soramimi, and more generally homophonic translation, are popular and frequent phenomena, the psycholinguistic literature on the topic is relatively scarce, and has mostly focused on the phonological variables involved in the occurence of Soramimi. For instance, @otake2007interlingual analysed 194 instances of Soramimi, broadcasted between 1992 and 2007 by the TV show Soramimi hour. These instances consisted of homophonic translations of English song lyrics to Japanese words and phrases. The vast majority of the reported instances were phrases (96%) and a few, to single words (4%). The phonetic features of the presented English lyrics were preserved with relatively high variability, with some Japanese resulting words or phrases sharing high overlap with the original English strings of sounds, and some sharing very little overlap. When analysing the few instances of homophonic translations of single words, the author identified three phonological processes that might explain how Japanese listeners reconstructed the English input to generate the Japanese words: insertion (e.g., *cry* /krʌɪ/ to *kurai*, /kuɾai/ [dark]), deletion (e.g., *go*, /goʊ/ to *go*, /go/ [go], and alternation (e.g., *low*, /loʊ/ to *rou*, /ɾoo/ [wax]). This suggests that the reported homophonic translations can be explained, to some extent, as the result of the Japanese listeners accommodating the strings of English sounds to the Japanese phoneme inventory and Japanese phonotactics [@peperkamp2008perceptual; @dupoux1999epenthetic]. Native prosody also constrains the occurence of homophonic translations. For instance, @kentner2015rhythmic presented in the auditory modality French-native and German-native participants with English songs with lyrics. Homophonic translations followed language-specific segmentation strategies. Most homophonic translations reported by German-native participants resulted from inserting phrase and word boundaries *before* stressed syllables (following the more frequent trochaic patter in German). On the contrary, most homophonic translations reported by French-native participants resulted from inserting phrase and word boundaries *after* stressed syllables, following the more frequent iambic pattern in French.

While the previous studies have address the phonological aspects of homophonic translation, little attention has been paid to the dynamics of lexical activation and competition that underlay this phenomenon. In the present work we investigated the interplay between phonological similarity and phonological neighbourhood density to examine how listeners of an unfamiliar language activate lexical representations in their native language. In particular, we studied how the presence of phonological neighbours in the native language lexicon affects the extent to which listeners are able to activate the correct translation of an auditorily presented word in an unfamiliar language. We used a translation elicitation task in which participants listened to words from an unfamiliar language, and then tried to guess their translation in their native language. We will henceforth refer to the auditory-presented words heard by participants on each trial as presented words, and the correct translation for the presented words as target translations. We explored listeners’ reliance on phonological similarity to succeed in the task by manipulating the amount of phonological similarity between the presented words and their target translations. Since participants were unfamiliar with the presented language, they could only use phonological similarity between the presented language and their native language to successfully translate the words. We therefore predicted that participants’ performance should increase when the translation pairs are phonologically more similar. We also predicted that there is a minimum threshold of phonological similarity to be sufficient for translation.

We further investigated the effect of phonological competitors. Even if the presented word and its target translation share high phonological similarity, participants may still provide incorrect translations if the presented word also shares high phonological similarity with other words in the native language. Words with denser phonological neighbourhoods are recognised more slowly and less accurately than words with sparser phonological neighbourhoods [@dufour2003lexical; @goldinger1989priming; @hamburger1996phonological; @luce1990similarity; @luce1998recognizing]. This is especially true if such neighbours share higher phonological similarity with the presented word, or are lexically more frequent than the target translation [@luce1998recognizing]. Since the interference effects of phonological neighbourhood density has also been reported across languages [@weber2004lexical], we hypothesised that the size of the facilitatory role of phonological similarity would be inversely proportional to the amount of higher-frequency phonological neighbours of the presented word in the target language. In the case of non-native speech recognition, the presence of cross-linguistic pairs which are phonologically similar but differ in meaning (e.g., false friends) may act as distractors during lexical access, obstructing the selection of the appropriate target translation in the listener’s lexicon. For instance, @otwinowska2019more reported that false friends were disadvantaged relative to non-cognates by Polish second language learners, in contrast to cognates which were known better. It is therefore important to investigate the joint effect of both cognates and false friends when investigating the effect cross-linguistic phonological similarity has on word recognition in a foreign language. This could shed light on available strategies and challenges associated with the early stages of second language acquisition. For instance, one could expect English participants to incorrectly translate the Spanish word *botón* as *bottom* instead of as its correct translation *button*, due to combined effect of the close phonological similarity between *botón* and *bottom* along with the high lexical frequency of *bottom* relative to *button*. To test this prediction, we developed a lexical frequency-dependent measure of cross-language phonological neighbourhood density, in which a neighbour is counted only if it is higher frequency and is one phoneme apart from the presented word. If the phonological neighbourhood density of the target translation affects participants’ performance negatively, this would suggest that competitors in the native language affect recognition of non-native words during recognition of foreign speech. We conducted a series of three experiments to test these predictions. 

In Experiment 1, we collected data from two groups of British English natives living in the United Kingdom. One group was presented with Catalan words, the other with Spanish words. We examined the extent to which participants were able to use the phonological similarity between the presented word (in Catalan or Spanish) and its target translation to provide accurate responses. In Experiment 2, we tested a group of (European) Spanish natives in the same task, who were presented with Catalan words. Catalan and Spanish are both Romance languages whose close typological distance is reflected in the fact that they share many cognates, where English is a Germanic language that shares considerably fewer cognates with Catalan and Spanish. By testing participants translating words from typologically close or distant languages, we expected to widen the range of the phonological similarity scores of the translation pairs involved in the experimental task, therefore allowing us to explore potential cross-language differences in participant’s performance. One unexpected finding was that participants in Experiments 1 and 2 were surprisingly good at translating a subset of words which had low phonological similarity with their correct translation. We were concerned that this may be caused by some prior knowledge of specific words by our participants, as some Spanish words are commonly seen in media or product labels, making them familiar even to monolingual speakers of English. In Experiment 3, we collected additional data from a new group of British English natives. The design was closely modelled after Experiment 1, except that after providing their response in each trial, participants reported whether they had previous knowledge of the presented word. In the final section of this paper, we present analyses on the joint data sets of Experiments 1 to 3.

:::



# Experiment 1

## Methods

All materials, data, and code used in this study are hosted in the Open Science Framework [https://osf.io/9fjxm/](https://osf.io/9fjxm/?view_only=aab7636ce1af48cf832596a7ea9101c5/) and a GitHub repository [https://github.com/gongcastro/translation-elicitation.git](https://github.com/gongcastro/translation-elicitation.git), along with additional notes.

### Participants

```{r}
#| label: participants-numbers-1
participants_1 <- exp_participants |> 
    filter(group!="cat-SPA")

collection_dates <- participants_1 |> 
    pull(date) |> 
    range() |> 
    format("%B %dth, %Y")

n_participants <- n_distinct(participants_1)
n_participants_group <- table(participants_1$group)
participants_age <- participants_1 |> 
    summarise(across(age, lst(mean, sd, min, max))) 
n_female <- table(participants_1$gender)
```

Participants were recruited via Prolific (5£ compensation) and SONA (compensation in academic credits), and gave informed consent before providing any data and the study was conducted in accordance with ethical standards of the Declaration of Helsinki and the protocol was by the University of Oxford Medical Sciences Inter-Divisional Research Ethics Committee (IDREC) (R60939/RE007). Participants were asked to complete the experiment using a laptop in a quiet place with good internet connection.  Data collection took place from `r collection_dates[1]` to `r collection_dates[2]`. We collected data from `r n_participants` British English native participants living in United Kingdom (*Mean* = `r round(participants_age$age_mean, 2)` years, *SD* = `r round(participants_age$age_sd, 2)`, Range = `r round(participants_age$age_min, 2)`-`r round(participants_age$age_max, 2)`, `r n_female` female). 

```{r tbl-participants}
#| label: tbl-participants
tbl_data <- participants |> 
    mutate(exp = case_when(
        source=="Experiment" & group!= "cat-SPA" ~ "Experiment 1",
        source=="Experiment" & group== "cat-SPA" ~ "Experiment 2",
        source=="Questionnaire" ~ "Experiment 3",
    )) |> 
    summarise(n = n(),
              n_excluded = sum(!valid_status=="Valid"),
              across(age, lst(mean, sd, min, max)),
              l2_lst = list(l2),
              .by = c(group, exp)) |> 
    mutate(l2_lst = map(l2_lst, 
                        function(x) {
                            y <- table(x)
                            y <- y[names(y) != "None"]
                            y <- paste0(names(y), " (", y, ")")
                            return(y)
                        }),
           n_excluded = paste0(n, " (", n_excluded, ")")) |> 
    select(-n)

tbl_data |> 
    gt(groupname_col = "exp", rowname_col = "group") |> 
    fmt_number(matches("age")) |> 
    fmt_integer(matches("min|max")) |> 
    cols_merge_uncert(age_mean, age_sd) |> 
    cols_merge_range(age_min, age_max) |> 
    tab_spanner("Age", matches("age")) |> 
    cols_label(group = "Group",
               exp = "Experiment",
               n_excluded = "N",
               age_mean = "Mean ± SD ",
               age_min = "Range",
               l2_lst = "L2") |> 
    tab_style(cell_text(style = "italic"),
              cells_column_labels()) |> 
    tab_style(cell_text(weight = "bold"),
              cells_column_spanners()) |> 
    tab_style(cell_text(align = "left"),
              list(cells_body(), cells_column_labels())) |> 
    tab_style(cell_text(weight = "bold",
                        align = "center",
                        style = "normal"),
              cells_column_labels(c(l2_lst, n_excluded))) |> 
    tab_footnote(
        "Number of included participants (number of excluded participants.)", 
        locations = cells_column_labels(n_excluded))
```


### Stimuli

```{r}
#| label: stimuli-lengths
lengths <- stimuli |>  
    select(group, ipa_flat_1, ipa_flat_2, word_1, word_2) |>  
    mutate(across(ipa_flat_1:word_2, nchar)) |> 
    summarise(across(c(ipa_flat_1, ipa_flat_2, word_1, word_2), 
                     lst(mean, sd, min, max)),
              .by = group)
lengths <- split(lengths, lengths$group)
names(lengths) <- janitor::make_clean_names(names(lengths))

durations <- stimuli |> 
    summarise(across(duration, lst(mean, sd, min, max)),
              .by = group) 
durations <- split(durations, durations$group)
names(durations) <- janitor::make_clean_names(names(durations))
```

We arranged two lists of input words to be presented to participants in the auditory modality: one in Catalan and one in Spanish. Words in the Catalan list were `r round(lengths$cat_eng$ipa_flat_1_mean, 2)` phonemes long on average (*SD* = `r round(lengths$cat_eng$ipa_flat_1_sd, 2)`, Range = `r round(lengths$cat_eng$ipa_flat_1_min, 2)`-`r round(lengths$cat_eng$ipa_flat_1_max, 2)`), and the orthographic forms of their English translations (which participants had to type) were `r round(lengths$cat_eng$word_2_mean, 2)` characters long on average (*SD* = `r round(lengths$cat_eng$word_2_sd, 2)`, Range = `r round(lengths$cat_eng$word_2_min, 2)`-`r round(lengths$cat_eng$word_2_max, 2)`. Words in the Spanish list were `r round(lengths$spa_eng$ipa_flat_1_mean, 2)` phonemes long on average (*SD* = `r round(lengths$spa_eng$ipa_flat_1_sd, 2)`, Range = `r round(lengths$spa_eng$ipa_flat_1_min, 2)`-`r round(lengths$spa_eng$ipa_flat_1_max, 2)`), and the orthographic form of their English translations (which participants had to type), were `r round(lengths$spa_eng$word_2_mean, 2)` characters long on average (*SD* = `r round(lengths$spa_eng$word_2_sd, 2)`, Range = `r round(lengths$spa_eng$word_2_min, 2)`-`r round(lengths$spa_eng$word_2_max, 2)`.

Participants listened to one audio file in each trial, each containing a single word presented in isolation. The audio files were the same ones used in child experiments conducted in the Laboratori de Recerca en Infància of Universitat Pompeu Fabra (Barcelona, Spain). These audio files were recorded by a proficient Catalan-Spanish female bilingual from the Metropolitan Area of Barcelona in a child-directed manner. Catalan and Spanish words were recorded at 44,100 Hz in separate files in the same session, and then de-noised using Audacity and normalised at peak intensity using Praat [@broersma2021praat]. The average duration of the Catalan audio files was `r round(durations$cat_eng$duration_mean, 2)` (*SD* = `r round(durations$cat_eng$duration_sd, 2)`, Range = `r round(durations$cat_eng$duration_min, 2)`-`r round(durations$cat_eng$duration_max, 2)`). The average duration of the Catalan audio files was `r round(durations$spa_eng$duration_mean, 2)` (*SD* = `r round(durations$spa_eng$duration_sd, 2)`, Range = `r round(durations$spa_eng$duration_min, 2)`-`r round(durations$spa_eng$duration_max, 2)`).

```{r exc-stimuli}
stim_exc_freq <- stimuli |> 
    mutate(is_na = is.na(freq_zipf_2)) |> 
    count(group, is_na) |> 
    filter(is_na)
stim_exc_freq <- split(stim_exc_freq, stim_exc_freq$group)
names(stim_exc_freq) <- janitor::make_clean_names(names(stim_exc_freq))

stim_exc_freq_n <- sum(map_int(stim_exc_freq, "n"))
```

For each translation pair, we defined three predictors of interest: the lexical frequency of the correct translation (*Frequency*), the phonological similarity between the presented (Catalan or Spanish) word and their correct English translation (*Similarity*), and the presented word's number of cross-language phonological neighbours (*CLPN*).

We included *Frequency* as a nuance predictor, under the hypothesis that---keeping other predictors constant---participants would translate higher-frequency words more accurately and faster than lower-frequency words. Lexical frequencies of correct translations were extracted from SUBTLEX-UK [@van2014subtlex], and transformed to Zipf scores. Words in the stimuli list without a lexical frequency value were excluded from data analysis (`r stim_exc_freq$cat_eng$n` in the Catalan list, `r stim_exc_freq$spa_eng$n` in the Spanish list).


We calculated *Similarity*, our main predictor of interest, by computing the Levenshtein similarity between the X-SAMPA transcriptions of each pair of translations using the `stringdist` R package [@van2014stringdist]. The Levenshtein distance computes the edit distance between two character strings---in this case, phoneme transcriptions---by counting the number of additions, deletions, and substitutions necessary to make both strings identical [@levenshtein1966binary]. We divided this edit distance by the number of characters included in the longest X-SAMPA transcription of the translation pair. This results in a proportion score, in which values closer to zero correspond to lower Levenshtein distances between phonological transcriptions (i.e., higher similarity), and values closer to 1 correspond to higher Levenshtein distances (i.e., lower similarity). This transformation also help account for the fact that the Levenshtein distance tends to increase with the length of the transcriptions. For interpretability, we subtracted this proportion from one, so that values closer to one correspond to higher similarity between phonological transcriptions, and values closer to zero correspond to lower similarity between phonological transcriptions. For example, the *table* (`teIb@l`)-*mesa* (`mesa`) translation pair had a `r scales::percent(stringsim(enc2utf8("teɪbəl"), enc2utf8("mesa")))` similarity, while the *train* (`trEIn`)-*tren* (`tREn`) translation pair had a `r scales::percent(stringsim(enc2utf8("trEIn"), enc2utf8("tɾEn")))` similarity. @tbl-stimuli summarises the lexical frequency, phonological neighbourhood density and phonological overlap of the words included in the Catalan and the Spanish lists.


For each Catalan and Spanish word, we calculated the number of *CLPN* by counting the number of English words with same or higher lexical frequency, and whose phonological transcription (in X-SAMPA format) different in up to one phoneme from that of the presented Catalan or Spanish word. Lexical frequencies and phonological transcriptions were extracted from the multilingual database CLEARPOND [@marian2012clearpond]^[[Phonological transcriptions in CLEARPOND were generated from eSPEAK, [http://espeak.sourceforge.net/](http://espeak.sourceforge.net/)]].


```{r tbl-stimuli}
#| label: tbl-stimuli
#| tbl-cap: "Stimuli properties."
stimuli |> 
    summarise(across(c(freq_zipf_2, lv, neigh_n_h),
                     lst(mean, sd, min, max), 
                     na.rm = TRUE),
              .by = c(group)) |> 
    gt(rowname_col = "group") |> 
    tab_spanner("Frequency", matches("freq")) |> 
    tab_spanner("Similarity", matches("lv")) |> 
    tab_spanner("CLPN", matches("neigh")) |> 
    fmt_number(is.numeric) |> 
    fmt_integer(c(neigh_n_h_min,
                  neigh_n_h_max)) |> 
    cols_merge_uncert(freq_zipf_2_mean, freq_zipf_2_sd) |> 
    cols_merge_uncert(lv_mean, lv_sd) |> 
    cols_merge_uncert(neigh_n_h_mean, neigh_n_h_sd) |> 
    cols_merge_range(freq_zipf_2_min, freq_zipf_2_max) |> 
    cols_merge_range(lv_min, lv_max) |> 
    cols_merge_range(neigh_n_h_min, neigh_n_h_max) |> 
    cols_label(freq_zipf_2_mean = "Mean ± SD",
               freq_zipf_2_sd = "SD",
               freq_zipf_2_min = "Range",
               lv_mean = "Mean ± SD",
               lv_sd = "SD",
               lv_min = "Range",
               neigh_n_h_mean = "Mean ± SD",
               neigh_n_h_sd = "SD",
               neigh_n_h_min = "Range") |> 
    tab_style(cell_text(weight = "bold"),
              cells_column_spanners()) |> 
    tab_style(cell_text(style = "italic"),
              cells_column_labels()) |> 
    grand_summary_rows(columns = matches("mean|sd"),
                       fns = list(label = md("**Mean**"), 
                                  id = "totals",
                                  fn = "mean"),
                       fmt = ~fmt_number(., n_sigfig = 3, suffixing = TRUE))
```


### Procedure

The experiment was implemented online using Psychopy/Pavlovia [@peirce2019psychopy2]. Participants accessed the study from a link provided by Prolific or SONA and completed the experiment from an internet browser (Chrome or Mozilla). After giving their consent for participating, participants answered a series of questions about their demographic status, their language background, and the set up they were using for completing the study. Then, participants completed the experimental task. Participants were informed that they would listen to a series of pre-recorded words in either Catalan or Spanish (English participants) or Catalan (Spanish participants). They were instructed to listen to each word, guess its meaning in English (English participants) or Spanish (Spanish participants), and type their answer as soon as possible. English participants were randomly assigned to the Catalan or Spanish lists. Participants in the Catalan list were presented with `r nrow(stimuli[stimuli$group=="cat-ENG",])` trials, and participants in the Spanish list were presented with `r nrow(stimuli[stimuli$group=="spa-ENG",])` trials.

Each trial started with a yellow fixation point presented during one second on the centre of the screen over a black background. After one second, the audio started playing while the dot remained being displayed until the audio ended. Upon the offset of the fixation point and audio, participants were prompted to write their answer by a `>` symbol. Typed letters were displayed in the screen in real time to provide visual feed-back to participants. Participants were allowed to correct their answer. Then, participants pressed the `RETURN/ENTER` key to confirm their answer and start and new trial.

![Schematic representation of a trial in the experimental task](_assets/img/design.png)


### Data analysis

#### Data processing

After data collection, participants answers were manually coded into the following categories: *Correct*, *Typo*, *Wrong*, *False friend*, *Other*. Responses were coded as *Correct* if the provided string of characters was identical to the orthographic form of the correct translation. Trials were coded as *Typo* if the participant provided a string of characters only one edit distance (addition, deletion, or substitution) apart from the orthographic form of the correct translation (e.g., "pengiun" instead of "penguin"), as long as the response did not correspond to a distinct English word. Responses were coded as *False friend* if the participant provided a phonologically similar incorrect translation. Responses were coded as *Other* (see Data analysis section for more details). Both *Correct* and *Typo* responses were considered as correct, while *Wrong* and *False friend* responses were considered as incorrect. *Other* responses were excluded from data analysis. Trials in which participants took longer than 10 seconds to respond were also excluded. Participants contributed a total of `r nrow(exp_responses)` valid trials (`r format(nrow(exp_responses[exp_responses$group %in% c("cat-ENG", "cat-SPA"),]), big.mark = ",")` in Catalan, `r format(nrow(exp_responses[exp_responses$group %in% c("spa-ENG"),]), big.mark = ",")` in Spanish). The task took approximately 15 minutes to be completed.

#### Modelling approach and statistical inference

We modelled the probability of participants guessing the correct translation of each presented word using a generalised multilevel Bayesian regression model with a Bernoulli logit link distribution. We included as fixed effects the intercept, the main effects of *Frequency*, *Similarity*, and *CLPN*, and the two-way interaction between *Similarity* and *CLPN*. We also included participant-level random intercepts and slopes for the the main effects and the interaction. Eq. 1 shows a formal description of the model.

::: {.content-visible when-format="docx"}
![Eq. 1](_assets/img/model.PNG)
:::

::: {.content-visible when-format="pdf"}
$$
\begin{aligned}
&\textbf{Likelihood}  \\
y_{i} \sim & \text{Bernoulli}(p_{i}) \\ \\
&\textbf{Parameters}  \\
\text{Logit}(p_{i}) = &  \beta_{0[p,w]} + \beta_{1[p]} \text{Frequency}_{i} + \beta_{2[p]} \text{PTHN}_i + \beta_{3[p]} \text{Similarity}_i + \beta_{4[p]} (\text{PTHN}_i \times \text{Similarity}_i) \\
\beta_{0-6[p,w]} \sim & \mathcal{N}(\mu_{\beta_{j}}, \sigma_{\beta_{j}}) \text{, for participant } p \text{ in 1, ..., } P \text{ and  word } w \text{ in 1, ..., } W \\
\beta_{1-6[p]} \sim &  \mathcal{N}(\mu_{\beta_{j}}, \sigma_{\beta_{j}}) \text{, for participant } p \text{ in 1, ..., } P \\ \\
&\textbf{Prior}  \\
\mu_{\beta_{p,w}}  \sim &  \mathcal{N}(0, 0.1) \\
\sigma_{\beta_{p}},  \sigma_{\beta_{w}} \sim & \text{HalfCauchy}(0, 0.1) \\
\rho_{p}, \rho_{w} \sim & \text{LKJCorr}(8) \\
\end{aligned}
$$
:::

To test the practical relevance of each predictor we followed [@kruschke2018bayesian]. We first specified a region of practical equivalence (*ROPE*) around zero ([-0.1, +0.1], in the logit scale). This area indicates the values of the regression coefficients that we considered as equivalent to zero. We then computed the 95% posterior credible intervals (CrI) of the regression coefficients of interest. Finally, we calculated the proportion of the 95% CrI that fell inside the ROPE. This proportion indicates the probability that the true value of the coefficient is equivalent to zero. All analyses were performed in R environment [@rcoreteam2013language]. We used the tidyverse family of R packages [@wickham2019welcome] to process data and to generate figures. We used the `brms` R package [@burkner2017brms] using the `cmdstanr` back-end to the Stan probabilistic language [@carpenter2017stan] to estimate and compare the models (see Appendix 1 for mode details on the models).


## Results

```{r dataset-1}
#| label: dataset-1
dataset <- exp_responses |> 
    filter(group!="cat-SPA") |> 
    group_split(group) |> 
    set_names(c("spa-ENG", "cat-ENG")) |> 
    janitor::clean_names() 

dataset <- split(exp_responses, exp_responses$group)
names(dataset) <- janitor::make_clean_names(names(dataset))
r_validity <- map(dataset, 
                  \(x) janitor::clean_names(table(x$response_type)))
n_participants <- map(dataset, \(x) n_distinct(x$participant_id))

excluded_lang <- participants_1 |> 
    filter(l2_writ_prod > 3 |
               l2_oral_comp > 3 |
               cat_oral_comp > 3 | 
               cat_writ_prod > 3 |
               spa_oral_comp > 3 |
               spa_writ_prod > 3) |> 
    nrow()

excluded_impairment <- participants_1 |> 
    filter(valid_status=="Language impairment") |> 
    nrow()

excluded_ntrials <- participants_1 |> 
    filter(valid_status=="Insufficient trials") |> 
    nrow()

dataset_clean <- exp_responses |> 
    filter(group!="cat-SPA") |> 
    dplyr::filter(valid_participant, valid_response) |> 
    group_split(group) |> 
    set_names(c("spa-ENG", "cat-ENG")) |> 
    janitor::clean_names() 

lengths <- dataset_1 |> 
    mutate(len = nchar(response)) |> 
    summarise(across(len,
                     lst(mean, median, sd, min, max)),
              .by = c(group)) 
lengths <- split(lengths, lengths$group)

names(lengths) <- janitor::make_clean_names(names(lengths))
```

We collected data for a total of `r format(nrow(dataset$cat_eng) + nrow(dataset$spa_eng), big.mark = ",")` trials completed by `r n_participants$cat_eng + n_participants$spa_eng` unique participants. Of those trials, `r format(nrow(dataset$cat_eng), big.mark = ",")` were provided by `r n_participants$cat_eng` unique participants who listened to Catalan words, and `r format(nrow(dataset$spa_eng), big.mark = ",")` trials were provided by `r n_participants$spa_eng` unique participants who listened to Spanish words. We excluded trials in which participants did not enter any text (*n* = `r r_validity$cat_eng["blank"]`), in which a response in a language other than English was provided (e.g., `agua`, *n* = `r r_validity$cat_eng["language"]`), in which participants did not provide a whole word (e.g., `f`, *n* = `r r_validity$cat_eng["incomplete"]`), and in which participants added comments to the experimenter (e.g., `unsure`, *n* = `r r_validity$cat_eng["other"]`). In addition, we excluded data from participants that self-rated their oral and/or written skills in Catalan and Spanish, or any other second language as four or higher in a five-point scale (*n* = `r excluded_lang`), were diagnosed with a language (*n* = `r excluded_impairment`), or did not contribute more than 80% of valid trials (*n* = `r excluded_ntrials`).

After applying trial-level and participant-level inclusion criteria, the resulting dataset included `r format(nrow(dataset_clean$cat_eng) + nrow(dataset_clean$cat_eng), big.mark = ",")` trials provided by `r n_distinct(dataset_clean$cat_eng$participant_id) + n_distinct(dataset_clean$cat_eng$participant_id)` unique participants. Of those trials, `r format(nrow(dataset_clean$cat_eng), big.mark = ",")` were provided by `r n_distinct(dataset_clean$cat_eng$participant_id)` unique participants who listened to Catalan words, and `r format(nrow(dataset_clean$spa_eng), big.mark = ",")` trials were provided by `r n_distinct(dataset_clean$spa_eng$participant_id)` unique participants who listened to Spanish words. Responses given by English participants to Catalan presented words were `r round(lengths$cat_eng$len_mean, 2)` characters long on average (*Median* = `r round(lengths$cat_eng$len_median, 2)`, *SD* = `r round(lengths$cat_eng$len_sd, 2)`, Range = `r round(lengths$cat_eng$len_min, 2)`-`r round(lengths$cat_eng$len_max, 2)`), while their translations to Spanish responses were `r round(lengths$spa_eng$len_mean, 2)` characters long on average (*Median* = `r round(lengths$spa_eng$len_median, 2)`, *SD* = `r round(lengths$spa_eng$len_sd, 2)`, Range = `r round(lengths$spa_eng$len_min, 2)`-`r round(lengths$spa_eng$len_max, 2)`).

```{r tbl-results}
#| label: tbl-results
tbl_data <- list("Experiment 1" = dataset_1,
                 "Experiment 2" = dataset_2,
                 "Experiment 3" = dataset_3) |> 
    bind_rows(.id = "exp") |> 
    count(exp, group, response_type) |> 
    pivot_wider(names_from = response_type,
                values_from = n,
                names_repair = janitor::make_clean_names) |>
    relocate(exp, group, correct, typo, wrong, false_friend) |> 
    mutate(n_correct = rowSums(cbind(correct, typo)),
           n_incorrect = rowSums(cbind(wrong, false_friend)),
           n_total = rowSums(cbind(n_correct, n_incorrect)),
           across(correct:false_friend, 
                  \(x) x / n_total,
                  .names = "{col}_prop")) |> 
    select(exp, group, correct, typo, wrong, false_friend,
           matches("prop")) |> 
    relocate(correct, correct_prop,
             typo, typo_prop,
             wrong, wrong_prop,
             false_friend, false_friend_prop)


tbl_data |> 
    gt(rowname_col = "group", groupname_col = "exp") |> 
    fmt_missing(everything(), everything(), "--") |> 
    tab_spanner("Correct responses", 
                matches("correct|typo")) |> 
    tab_spanner("Incorrect responses", 
                matches("wrong|false")) |> 
    fmt_integer(c(correct, typo, wrong, false_friend)) |> 
    fmt_number(is.double, pattern = "({x})", scale_by = 100) |> 
    cols_label(correct = "Correct",
               typo = "Typo",
               wrong = "Wrong",
               false_friend = "False friend",
               correct_prop = "(%)",
               typo_prop = "(%)",
               wrong_prop = "(%)",
               false_friend_prop = "(%)") |> 
    tab_style(cell_text(weight = "bold"), 
              cells_column_spanners()) |> 
    tab_style(cell_text(style = "italic"), 
              cells_column_labels()) |> 
    summary_rows(columns = is.integer,
                 fns = list(label = md("*Sum*"),
                            id = "totals",
                            fn = "sum"),
                 fmt = ~fmt_integer(.)) |>
    summary_rows(columns = matches("prop"),
                 fns = list(label = md("*Mean*"), 
                            id = "means",
                            fn = "mean"),
                 fmt = ~fmt_number(., scale_by = 100)) |> 
    tab_style(cell_text(align = "center"),
              cells_column_labels())

```


```{r coefs-1}
c1 <- gather_draws(exp_1_m0, `b_.*`, `sd_.*`, regex = TRUE) |> 
    summarise(across(.value, lst(.median = median, 
                                 .lower = \(x) hdi(x)[1],
                                 .upper = \(x) hdi(x)[2],
                                 .rope = \(x) mean(between(x, -0.1, 0.1))),
                     .names = "{fn}"))
c1 <- split(c1, c1$.variable)
names(c1) <- janitor::make_clean_names(names(c1)) 
```

@tbl-dataset shows a summary of participants' accuracy across Experiments 1, 2, and 3. MCMC chains in the model showed strong evidence of convergence ($\hat{R}<1.01$) (see Appendix 2 for more detailed model diagnostics). Participants translating Catalan words and participants translating Spanish words performed equivalently, as indicated by the regression coefficient of *Group* ($\beta$ = `r round(c1$b_group1$.median, 3)`, 95% CrI = [`r round(c1$b_group1$.lower, 3)`, `r round(c1$b_group1$.upper, 3)`], *p*(ROPE) = `r round(c1$b_group1$.rope, 3)`). Overall, participants responded less accurately to words with more CLPNs than to words with fewer CLPNs, regardless of the amount of phonological similarity between the presented word and its translation. This is indicated by the size of the regression coefficient of the two-way interaction between *Similarity* and *CLPN* ($\beta$ = `r round(c1$b_neigh_n_h_lv$.median, 3)`, 95% CrI = [`r round(c1$b_neigh_n_h_lv$.lower, 3)`, `r round(c1$b_neigh_n_h_lv$.upper, 3)`], *p*(ROPE) = `r round(c1$b_neigh_n_h_lv$.rope, 3)`). As anticipated, participants' performance benefited from an increase in *Similarity* ($\beta$ = `r round(c1$b_lv$.median, 3)`, 95% CrI = [`r round(c1$b_lv$.lower, 3)`, `r round(c1$b_lv$.upper, 3)`], *p*(ROPE) = `r round(c1$b_lv$.rope, 3)`), while the number of *CLND* had the opposite effect ($\beta$ = `r round(c1$b_neigh_n_h$.median, 3)`, 95% CrI = [`r round(c1$b_neigh_n_h$.lower, 3)`, `r round(c1$b_neigh_n_h$.upper, 3)`], *p*(ROPE) = `r round(c1$b_neigh_n_h$.rope, 3)`). @fig-epreds-1 illustrates the posterior of the average predictions of the model for words with different values of *Similarity* and *CLPN*. @fig-coefs shows a graphic summary of the posterior distribution of the regression coefficients of interest.

```{r tbl-dataset}
#| label: tbl-dataset
sem <- function(x) mean(x) / (sqrt(length(x)))

#' Proportion adjusted from boundary values (Gelman, Hill & Vehtari, 2020)
#' 
prop_adj <- function(x, n){
    e <- (x+2)/(n+4)
    return(e)
}

#' Adjusted standard error of proportion (Gelman, Hill & Vehtari, 2020)
#' 
prop_adj_se <- function(x, n) {
    e <- (x+2)/(n+4)
    se <- sqrt(e*(1-e)/(n+4))
    return(se)
}

#' Adjusted standard error of proportion (Gelman, Hill & Vehtari, 2020)
#' 
prop_adj_ci <- function(x, n, .width = 0.95) {
    e <- (x+2)/(n+4)
    se <- sqrt(e*(1-e)/(n+4))
    ci <-  e + qnorm(c((1-.width)/2, (1-(1-.width)/2)))*se
    ci[1] <- ifelse(ci[1]<0, 0, ci[1]) # truncate at 0
    ci[2] <- ifelse(ci[2]>1, 1, ci[2]) # truncate at 1
    return(ci)
}

tbl_data <- list("Experiment 1" = dataset_1,
                 "Experiment 2" = dataset_2,
                 "Experiment 3" = dataset_3) |> 
    bind_rows(.id = "exp") |> 
    add_count(exp, group, participant_id, name = "trials") |> 
    summarise(n_trials = n(),
              correct = sum(correct),
              .by = c(group, exp, participant_id)) |> 
    mutate(correct = prop_adj(correct, n_trials)) |> 
    summarise(across(correct, lst(mean, sd, sem, min, max)),
              across(n_trials, lst(mean, sum, sd, min, max)),
              n_participants = n_distinct(participant_id),
              .by = c(group, exp)) |> 
    relocate(n_participants, matches("correct"))

tbl_data |> 
    gt(rowname_col = "group",
       groupname_col = "exp") |> 
    fmt_number(is.numeric, decimals = 2) |> 
    #gtExtras::gt_plt_dist(correct_list, type = "density") |> 
    fmt_integer(c(matches("sum|min|max"), n_participants)) |> 
    fmt_number(matches("correct"), scale_by = 100) |> 
    tab_spanner("Accuracy (%)", matches("correct")) |> 
    tab_spanner("Valid trials", matches("n_trials")) |> 
    cols_merge_range(n_trials_min, n_trials_max) |> 
    cols_merge_range(correct_min, correct_max) |> 
    cols_label(n_trials_sum = "N trials",
               n_participants = "N",
               n_trials_mean = "Mean",
               n_trials_sd = "SD",
               n_trials_min = "Range",
               correct_mean = "Mean",
               correct_sd = "SD",
               correct_sem = "SE",
               correct_min = "Range") |> 
    summary_rows(columns = is.integer,
                 fns = list(label = md("*Sum*"),
                            id = "totals",
                            fn = "sum"),
                 fmt = ~fmt_integer(.)) |>
    summary_rows(columns = matches("mean|sd|sem"),
                 fns = list(label = md("*Mean*"), 
                            id = "means",
                            fn = "mean"),
                 fmt = ~fmt_number(., scale_by = 100)) |> 
    tab_style(cell_text(align = "center"),
              cells_column_labels()) |> 
    tab_style(cell_text(style = "italic"),
              cells_column_labels()) |> 
    tab_style(cell_text(weight = "bold"),
              cells_column_spanners()) |> 
    tab_style(cell_text(align = "left"),
              list(cells_body(), cells_column_labels()))
```


```{r fig-epreds-1}
#| label: fig-epreds-1
#| fig-height: 5
#| fig-width: 11
#| fig-cap: "Posterior model-predicted mean accuracy in Experiment 1. Predictions were generated from 4,000 posterior samples, extracted for different values of *CLPN* (0, 2, 4, 8, 12) and *Similarity* (1-100). Predictions are plotted separately for English participants translating Catalan words, and for English participants translating Spanish words. Lines indicate mean predictions, and intervals indicate 95%, 89%, 78%, 67%, and 50% credible intervals (CrI)."
get_epreds <- function(model, data, n = 1000, 
                       lv = seq(0, 1, length.out = 100), 
                       neigh_n_h = c(0, 2, 4, 8, 12),
                       ...) {
    
    knowledge <- unique(model$data$knowledge)
    confidence <- unique(model$data$confidence)
    freq_zipf_2 <- mean(data$freq_zipf_2, na.rm = TRUE)
    group <- unique(model$data$group)
    nd <- expand_grid(freq_zipf_2, neigh_n_h, lv, knowledge, confidence, group)
    epreds <- tidybayes::add_epred_draws(nd, model, re_formula = NA, ...)
    
    return(epreds)
}

get_epreds(exp_1_m0, dataset_1) |> 
    mutate(neigh_n_h = paste0(neigh_n_h, " neighbours"),
           neigh_n_h = factor(neigh_n_h,
                              levels = paste0(c(0, 2, 4, 8, 12),
                                              " neighbours"),
                              ordered = TRUE)) |> 
    ggplot(aes(lv, .epred)) +
    facet_grid(group~neigh_n_h) +
    stat_lineribbon(aes(fill_ramp = after_stat(level)),
                    linewidth = 1/2,
                    .width = c(0.95, 0.89, 0.78, 0.67, 0.50)) +
    labs(x = "Similarity\n(Levenshtein distance with correct translation)",
         y = "p(Correct)",
         colour = "Cross-language neighbourhood density",
         fill = "Cross-language neighbourhood density",
         linetype = "Cross-language neighbourhood density",
         fill_ramp = "CrI (%)") +
    scale_x_continuous(labels = scales::percent) +
    scale_y_continuous(labels = scales::percent) +
    scale_fill_brewer(palette = "Reds") +
    theme(legend.position = "none",
          legend.box = "vertical") 
```


## Discussion

In the present experiment, we investigated the extent to which the phonological similarity between translation equivalents is sufficient for successful word translation, in the absence of semantic knowledge. We tested two groups of monolingual British English natives in a translation task that involved words in Catalan or Spanish, two languages participants reported having no prior familiarity with. Participants benefited strongly from phonological similarity when the correct translation of the presented words in Catalan or Spanish had few English phonological neighbours with higher lexical frequency. This suggests that, in the absence of distractors, even naïve participants can efficiently use phonological similarity to succeed in a translation task. Our results suggest that word-to-word connections at the phonological level might play a strong role during L2 speech comprehension, specially in low-proficiency listeners.

# Experiment 2

Results in Experiment 1 suggest that English natives were able to exploit the phonological similarity between unfamiliar words in Catalan and Spanish to provide accurate translations to English. English, a Germanic language, is relatively distant from Catalan and Spanish, two Romance languages. English shares fewer cognates with Catalan and Spanish than it does with typologically closer languages, like Dutch and German. In Experiment 2, we investigated whether listeners of an unfamiliar but typologically closer language benefit more strongly from phonological similarity when performing the same task as in Experiment 1. To this aim, we presented Spanish participants, who reported little-to-no prior familiarity with Catalan, with Catalan words.

## Methods

```{r participants-numbers-2}
#| label: participants-numbers-2
participants_2 <- exp_participants |> 
    filter(group=="cat-SPA")

collection_dates <- participants_2 |> 
    pull(date) |> 
    range() |> 
    format("%B %dth, %Y")

n_participants <- n_distinct(participants_2)
n_participants_group <- table(participants_2$group)
participants_age <- participants_2 |> 
    summarise(across(age, lst(mean, sd, min, max))) 
n_female <- table(participants_2$gender)
```

Participants in Spain were contacted via announcements in Faculties, and were compensated 5€ or an Amazon voucher for the same value. Participants gave informed consent before providing any data and the study was conducted in accordance with ethical standards of the Declaration of Helsinki and the protocol was approved by the Drug Research Ethical Committee (CEIm) of the IMIM Parc de Salut Mar (2020/9080/I). Data collection took place from `r collection_dates[1]` to `r collection_dates[2]`. We collected data from `r n_participants` Spanish native participants living in Spain (*Mean* = `r round(participants_age$age_mean, 2)` years, *SD* = `r round(participants_age$age_sd, 2)`, Range = `r round(participants_age$age_min, 2)`-`r round(participants_age$age_max, 2)`, `r n_female` female). Stimuli were the same list of Catalan stimuli as in Experiment 1. Procedure and data analysis were identical as in Experiment 1.

## Results

```{r dataset-2}
#| label: dataset-2
dataset <- exp_responses |> 
    filter(group=="cat-SPA") |> 
    group_split(group) |> 
    set_names(c("cat-SPA")) |> 
    janitor::clean_names() 

dataset <- split(exp_responses, exp_responses$group)
names(dataset) <- janitor::make_clean_names(names(dataset))
r_validity <- map(dataset, 
                  \(x) janitor::clean_names(table(x$response_type)))
n_participants <- map(dataset, \(x) n_distinct(x$participant_id))

excluded_lang <- participants_2 |> 
    filter(l2_writ_prod > 3 |
               l2_oral_comp > 3 |
               cat_oral_comp > 3 | 
               cat_writ_prod > 3 |
               spa_oral_comp > 3 |
               spa_writ_prod > 3) |> 
    nrow()

excluded_impairment <- participants_1 |> 
    filter(valid_status=="Language impairment") |> 
    nrow()

excluded_ntrials <- participants_1 |> 
    filter(valid_status=="Insufficient trials") |> 
    nrow()

dataset_clean <- exp_responses |> 
    filter(group=="cat-SPA") |> 
    dplyr::filter(valid_participant, valid_response) |> 
    group_split(group) |> 
    set_names(c("cat-SPA")) |> 
    janitor::clean_names() 

lengths <- dataset_2 |> 
    mutate(len = nchar(response)) |> 
    summarise(across(len,
                     lst(mean, median, sd, min, max)),
              .by = c(group)) 
```

We collected data for a total of `r format(nrow(dataset$cat_spa) + nrow(dataset$cat_spa), big.mark = ",")` trials completed by `r n_participants$cat_spa` unique participants. We excluded trials in which participants did not enter any text (*n* = `r r_validity$cat_spa["blank"]`), in which a response in a language other than English was provided (e.g., `agua`, *n* = `r r_validity$cat_spa["language"]`), in which participants did not provide a whole word (e.g., `f`, *n* = `r r_validity$cat_spa["incomplete"]`), and in which participants added comments to the experimenter (e.g., `unsure`, (*n* = `r r_validity$cat_spa["other"]`). In addition, we excluded data from participants that self-rated their oral and/or written skills in Catalan and Spanish, or any other second language as four or higher in a five-point scale (*n* = `r excluded_lang`), were diagnosed with a language (*n* = `r excluded_impairment`), or did not contribute more than 80% of valid trials (*n* = `r excluded_ntrials`). After applying trial-level and participant-level inclusion criteria, the resulting dataset included `r format(nrow(dataset_clean$cat_spa), big.mark = ",")` trials provided by `r n_distinct(dataset_clean$cat_spa$participant_id) + n_distinct(dataset_clean$cat_spa$participant_id)` unique participants. Of those trials, `r format(nrow(dataset_clean$cat_spa), big.mark = ",")` were provided by `r n_distinct(dataset_clean$cat_spa$participant_id)` unique participants who listened to Catalan words, and `r format(nrow(dataset_clean$cat_spa), big.mark = ",")` trials were provided by `r n_distinct(dataset_clean$cat_spa$participant_id)` unique participants who listened to Spanish words. Responses given by participants were `r round(lengths$len_mean, 2)` characters long on average (*Median* = `r round(lengths$len_median, 2)`, *SD* = `r round(lengths$len_sd, 2)`, Range = `r round(lengths$len_min, 2)`-`r round(lengths$len_max, 2)`).


```{r}
c2 <- gather_draws(exp_2_m0, `b_.*`, `sd_.*`, regex = TRUE) |> 
    summarise(across(.value, lst(.median = median, 
                                 .lower = \(x) hdi(x)[1],
                                 .upper = \(x) hdi(x)[2],
                                 .rope = \(x) mean(between(x, -0.1, 0.1))),
                     .names = "{fn}"))
c2 <- split(c2, c2$.variable)
names(c2) <- janitor::make_clean_names(names(c2)) 
```

MCMC chains in the model showed strong evidence of convergence ($\hat{R}<1.01$) (see Appendix 2 for more detailed model diagnostics). Overall, participants responded less accurately to words with more CLPNs than to words with fewer CLPNs, regardless of the amount of phonological similarity between the presented word and its translation. This is indicated by the size of the regression coefficient of the two-way interaction between *Similarity* and *CLPN* ($\beta$ = `r round(c2$b_neigh_n_h_lv$.median, 3)`, 95% CrI = [`r round(c2$b_neigh_n_h_lv$.lower, 3)`, `r round(c2$b_neigh_n_h_lv$.upper, 3)`], *p*(ROPE) = `r round(c2$b_neigh_n_h_lv$.rope, 3)`). As anticipated, participants' performance benefited from an increase in *Similarity* ($\beta$ = `r round(c2$b_lv$.median, 3)`, 95% CrI = [`r round(c2$b_lv$.lower, 3)`, `r round(c2$b_lv$.upper, 3)`], *p*(ROPE) = `r round(c2$b_lv$.rope, 3)`), while the number of *CLND* had the opposite effect ($\beta$ = `r round(c2$b_neigh_n_h$.median, 3)`, 95% CrI = [`r round(c2$b_neigh_n_h$.lower, 3)`, `r round(c2$b_neigh_n_h$.upper, 3)`], *p*(ROPE) = `r round(c2$b_neigh_n_h$.rope, 3)`). @fig-epreds-1 illustrates the posterior of the average predictions of the model for words with different values of *Similarity* and *CLPN*.

```{r fig-epreds-2}
#| label: fig-epreds-2
#| fig-cap: "Posterior model-predicted mean accuracy in Experiment 1. Predictions were generated from 4,000 posterior samples, extracted for different values of *CLPN* (0, 2, 4, 8, 12) and *Similarity* (1-100). Predictions are plotted separately for English participants translating Catalan words, and for English participants translating Spanish words. Lines indicate mean predictions, and intervals indicate 95%, 89%, 78%, 67%, and 50% credible intervals (CrI)."
#| fig-height: 3.5
#| fig-width: 10
get_epreds(exp_2_m0, dataset_2) |> 
    mutate(neigh_n_h = paste0(neigh_n_h, " neighbours"),
           neigh_n_h = factor(neigh_n_h,
                              levels = paste0(c(0, 2, 4, 8, 12),
                                              " neighbours"),
                              ordered = TRUE)) |> 
    ggplot(aes(lv, .epred)) +
    facet_wrap(~neigh_n_h, nrow = 1) +
    stat_lineribbon(aes(fill_ramp = after_stat(level)),
                    linewidth = 1/2,
                    .width = c(0.99, 0.95, 0.89, 0.78, 0.67, 0.50)) +
    labs(x = "Similarity\n(Levenshtein distance with correct translation)",
         y = "p(Correct)",
         colour = "Cross-language neighbourhood density",
         fill = "Cross-language neighbourhood density",
         linetype = "Cross-language neighbourhood density",
         fill_ramp = "CrI (%)") +
    scale_x_continuous(labels = scales::percent) +
    scale_y_continuous(labels = scales::percent) +
    scale_fill_brewer(palette = "Reds") +
    theme(legend.position = "none",
          legend.box = "vertical") 
```

## Discussion

Experiment 2 was an extension of Experiment 1 to a population of monolinguals whose native language is typologically similar to the presented language. Particularly, we presented Catalan words to Spanish native speakers who were reportedly unfamiliar with Catalan. Our results indicate a similar pattern of results as those in Experiment 1: participants were able to provide correct translations of presented Catalan words, provided that the Catalan words shared some degree of phonological similarity with their Spanish translation, and that the number of phonological neighbours with higher lexical frequency was reduced. In contrasts with the results in Experiment 1, the positive impact of phonological similarity on participants' performance in Experiment 2 was more resilient to the interference of phonological neighbours. English natives in Experiment 1 provided significantly less accuracy responses when more than four phonological neighbors were present (even when translating high-similarity words), compared to when only one or neighbour were present. Spanish participants in Experiment 2 benefited from phonological similarity, even when eight neighbours were present. Spanish participants' performance declined after 8 neighbours, and was evident at 12 neighbours. Overall, this suggests that participants in Experiment 2, who were natives of a typologically similar language (Spanish) to the presented language (Catalan) benefited more strongly from phonological similarity than participants in Experiment 1, who were natives of typologically less similar language (English) to the presented language (Catalan, Spanish).

Participants from both Experiment 1 and 2 benefited strongly from phonological similarity to correctly translate words from a non-native, reportedly unfamiliar language. This pattern of results holds for most of the presented stimuli, but some low-similarity Catalan and Spanish words were responded to surprisingly accurately by English listeners. Given that participants were reportedly unfamiliar with both languages, it was expected that participants would be very unlikely to provide correct translations for words sharing little to no phonological similarity to their correct translation. @tbl-surprises a list of Catalan and Spanish words to which participants provided responses with $\geq$ 10 average accuracy.

```{r tbl-surprises}
#| label: tbl-surprises
#| tbl-cap: "List of items with unexpectedly high accuracy: the Levenshtein similarityscore betwen th presented word (in Catalan or Spanish) and their correct Enlgish or Spanish translation is zero, but participants, who are reportedly unfamiliar with the presented language, were on average >10% likely to guess the correct translation."
tbl_data <- list("Experiment 1" = dataset_1,
                 "Experiment 2" = dataset_2) |>
    bind_rows(.id = "exp") |> 
    summarise(n = n(),
              correct = sum(correct),
              .by = c(exp, group, lv, translation)) |> 
    mutate(accuracy = prop_adj(correct, n),
           accuracy_se = prop_adj_se(correct, n)) |>    
    dplyr::filter(lv==0, accuracy >= 0.1) |> 
    select(exp, translation, group, accuracy, accuracy_se) |> 
    arrange(exp, group, desc(accuracy))

tbl_data |> 
    gt(rowname_col = "translation",
       groupname_col = c("exp", "group")) |> 
    fmt_number(is.numeric, scale_by = 100) |> 
    cols_label(accuracy = "Accuracy (%)", accuracy_se = "SE")
```


It is likely that participants had prior knowledge of these words despite having reported little to no familiarity with the presented language (Catalan or Spanish). One possibility is that participants had previously encountered these words embedded in English linguistic input. Spanish words percolate English speech with relative frequency, via different sources such as popular culture, songs, TV programs, etc. In addition, words from languages other than Spanish, but with high similarity to the Spanish words (e.g., cognates from Italian or French) might appear in English speech as well. Such prior knowledge might not be specific to the low-similarity words highlighted before. Participants may also have had prior knowledge about higher-similarity words, which could have contributed to participants responding to such words more accurately than without such prior knowledge. In the case of higher-similarity words, it is more difficult to disentangle the extent to which participants' accuracy is a function of pure phonological similarity, or prior knowledge they had about the meaning of Spanish words. To investigate this issue, we run Experiment 3.

[ADD HERE LEXICAL FREQUENCY OF CATALAN AND SPANISH WORDS IN ENGLISH, AND OF CATALAN WORDS IN SPANISH]


# Experiment 3

Experiment 3 is a replication of Experiment 1, in which we collected additional data about participants' prior familiarity with the presented Catalan and Spanish words, in addition to the same translation task presented to participants in Experiment 1.

## Methods

```{r participants-numbers-3}
#| label: participants-numbers-3
participants_3 <- quest_participants |> 
    filter(group!="cat-SPA")

collection_dates <- participants_3 |> 
    pull(date) |> 
    range() |> 
    format("%B %dth, %Y")

n_participants <- n_distinct(participants_3)
n_participants_group <- table(participants_3$group)
participants_age <- participants_3 |> 
    summarise(across(age, lst(mean, sd, min, max))) 
n_female <- table(participants_3$gender)
```

Data collection took place from `r collection_dates[1]` to `r collection_dates[2]`. We collected data from `r n_participants` British English native participants living in United Kingdom (*Mean* = `r round(participants_age$age_mean, 2)` years, *SD* = `r round(participants_age$age_sd, 2)`, Range = `r round(participants_age$age_min, 2)`-`r round(participants_age$age_max, 2)`, `r n_female` female). 


```{r}
#| label: participants-excluded-3
excluded_lang <- participants_3 |> 
    filter(l2_writ_prod > 3 |
               l2_oral_comp > 3 |
               cat_oral_comp > 3 | 
               cat_writ_prod > 3 |
               spa_oral_comp > 3 |
               spa_writ_prod > 3) |> 
    nrow()

excluded_impairment <- participants_3 |> 
    filter(valid_status=="Language impairment") |> 
    nrow()

excluded_ntrials <- participants_3 |> 
    filter(valid_status=="Insufficient trials") |> 
    nrow()
```

Participants were recruited via Prolific (5£ compensation) and SONA (compensation in academic credits), and gave informed consent before providing any data and the study was conducted in accordance with ethical standards of the Declaration of Helsinki and the protocol was by the University of Oxford Medical Sciences Inter-Divisional Research Ethics Committee (IDREC) (R60939/RE007). Participants were asked to complete the experiment using a laptop in a quiet place with good internet connection. Stimuli were the same list of Catalan stimuli as in Experiment 1. 

The experiment was implemented online using Qualtrics (Qualtrics, Provo, UT). This platform was chosen to allow easier presentation of survey questions aimed to probe prior understanding of the presented words and participants’ confidence ratings of their answers. With the exception of these additional questions, we attempted to replicate the procedure of Experiment 1 as closely as possible. The Spanish and Catalan audio stimuli used were identical the materials in Experiment 1. Participants were randomly assigned to the Catalan or Spanish lists. The Catalan list had 83 trials and the Spanish list had 99 trials. Participants first completed the consent form followed by the questionnaire about demographic status, language background and set up. They then proceeded to the experimental task.

On each trial, participants listened to the audio stimulus by clicking on the `PLAY` button. For comparability to the PsychoPy version, participants were only allowed to play the audio one time. Participants were explicitly told that they would be only allowed to listen once. The `PLAY` button vanished after one playthrough. Participants then had to answer three questions based on the audio they had heard on that trial. These questions were presented on the same page, directly below the audio player. They were first asked whether or not they knew the presented word (multiple choice---*yes*/*no*). Regardless of their answer on the first question, participants were asked what they thought the translation of the word was in English (or their best guess), and instructed to type their answer in the provided text box. Finally, they were asked to rate how confident they were in their answer on a scale of 0 to 7, where 7 was "very confident" and 0 was "not confident". There was no time limit on the response phase. All questions had to be answered to proceed to the next trial.

Participants first completed 5 practice trials with English words as the audio stimulus (ambulance, cucumber, elephant, pear, turtle). The words were recorded by a female native speaker of English. These trials acted as attention checks, as participants should always answer "yes" to the first question on prior word knowledge and be able to accurately transcribe the word they heard. Following the practice phase, participants completed the test phase where they heard either Spanish words or Catalan words. 


## Results

```{r dataset-3}
#| label: dataset-3
dataset <- quest_responses |> 
    filter(group!="cat-SPA") |> 
    group_split(group) |> 
    set_names(c("spa-ENG", "cat-ENG")) |> 
    janitor::clean_names() 

dataset <- split(quest_responses, quest_responses$group)
names(dataset) <- janitor::make_clean_names(names(dataset))
r_validity <- map(dataset, 
                  \(x) janitor::clean_names(table(x$response_type)))
n_participants <- map(dataset, \(x) n_distinct(x$participant_id))

excluded_lang <- participants_3 |> 
    filter(l2_writ_prod > 3 |
               l2_oral_comp > 3 |
               cat_oral_comp > 3 | 
               cat_writ_prod > 3 |
               spa_oral_comp > 3 |
               spa_writ_prod > 3) |> 
    nrow()

excluded_impairment <- participants_3 |> 
    filter(valid_status=="Language impairment") |> 
    nrow()

excluded_ntrials <- participants_3 |> 
    filter(valid_status=="Insufficient trials") |> 
    nrow()

dataset_clean <- quest_responses |> 
    filter(group!="cat-SPA") |> 
    dplyr::filter(valid_participant, valid_response) |> 
    group_split(group) |> 
    set_names(c("spa-ENG", "cat-ENG")) |> 
    janitor::clean_names() 
```

We collected data for a total of `r format(nrow(dataset$cat_eng) + nrow(dataset$spa_eng), big.mark = ",")` trials completed by `r n_participants$cat_eng + n_participants$spa_eng` unique participants. Of those trials, `r format(nrow(dataset$cat_eng), big.mark = ",")` were provided by `r n_participants$cat_eng` unique participants who listened to Catalan words, and `r format(nrow(dataset$spa_eng), big.mark = ",")` trials were provided by `r n_participants$spa_eng` unique participants who listened to Spanish words. We excluded trials in which participants did not enter any text (*n* = `r dataset$cat_eng$blank`), in which a response in a language other than English was provided (e.g., `agua`, *n* = `r dataset$cat_eng$language`), in which participants did not provide a whole word (e.g., `f`, *n* = `r dataset$cat_eng$incomplete`), and in which participants added comments to the experimenter (e.g., `unsure`, (*n* = `r dataset$cat_eng$other`). In addition, we excluded data from participants that self-rated their oral and/or written skills in Catalan and Spanish, or any other second language as four or higher in a five-point scale (*n* = `r excluded_lang`), were diagnosed with a language (*n* = `r excluded_impairment`), or did not contribute more than 80% of valid trials (*n* = `r excluded_ntrials`).

After applying trial-level and participant-level inclusion criteria, the resulting dataset included `r format(nrow(dataset_clean$cat_eng) + nrow(dataset_clean$cat_eng), big.mark = ",")` trials provided by `r n_distinct(dataset_clean$cat_eng$participant_id) + n_distinct(dataset_clean$cat_eng$participant_id)` unique participants. Of those trials, `r format(nrow(dataset_clean$cat_eng), big.mark = ",")` were provided by `r n_distinct(dataset_clean$cat_eng$participant_id)` unique participants who listened to Catalan words, and `r format(nrow(dataset_clean$spa_eng), big.mark = ",")` trials were provided by `r n_distinct(dataset_clean$spa_eng$participant_id)` unique participants who listened to Spanish words.

INSERT PRIOR KNOWLEDGE FILTER


```{r}
lengths <- dataset_3 |> 
    mutate(len = map_int(strsplit(response, ""), length)) |> 
    summarise(across(len,
                     lst(mean, median, sd, min, max)),
              .by = c(group)) 
lengths <- split(lengths, lengths$group)

names(lengths) <- janitor::make_clean_names(names(lengths))
```

Responses given by English participants to Catalan presented words were `r round(lengths$cat_eng$len_mean, 2)` characters long on average (*Median* = `r round(lengths$cat_eng$len_median, 2)`, *SD* = `r round(lengths$cat_eng$len_sd, 2)`, Range = `r round(lengths$cat_eng$len_min, 2)`-`r round(lengths$cat_eng$len_max, 2)`), while their translations to Spanish responses were `r round(lengths$spa_eng$len_mean, 2)` characters long on average (*Median* = `r round(lengths$spa_eng$len_median, 2)`, *SD* = `r round(lengths$spa_eng$len_sd, 2)`, Range = `r round(lengths$spa_eng$len_min, 2)`-`r round(lengths$spa_eng$len_max, 2)`).

MCMC chains in the model showed strong evidence of convergence ($\hat{R}<1.01$) (see Appendix 2 for more detailed model diagnostics). Overall, participants reported prior knowledge more often for that Spanish words with unexpectedly high accuracy (see Discussion in Experiment 2) than for words with expected accuracy (see @fig-knowledge). Participants reported prior knowledge of Catalan words with unexpected accuracy as often as those with expected accuracy. This suggests that participants in Experiment 1 may have relied, to some extent, on their prior knowledge about form-meaning mappings to correctly translate some Spanish words. To isolate such an effect of prior Spanish knowledge, we run the same analysis as in Experiment 1 on the newly collected translations from Experiment 3, now excluding responses to words in which participants reported prior knowledge.

```{r fig-knowledge}
#| label: fig-knowledge
#| fig-width: 7
#| fig-height: 5
#| fig-cap: "Catalan/Spanish prior word knowledge as reported by English native participants in Experiment 3. (A) Average proportion of participants that reported prior knowledge for words with surprisingly high accuracy (no phonological similarity with the correct translation, accuracy higher than 10%), and for words with expected accuracy (low similarity-low accuracy, or high similarity-high accuracy). (B) Average accuracy for words with expected and unexpected accuracy."
dataset_3 |> 
    mutate(surprise = translation %in% tbl_data$translation) |> 
    summarise(knowledge = mean(knowledge),
              .by = c(translation, group, surprise)) |> 
    select(translation, group, surprise, knowledge) |> 
    mutate(surprise = if_else(
        surprise,
        "Unexpectedly high accuracy\n(no phonological similarity)",
        "Expected accuracy")) |> 
    ggplot(aes(knowledge, group, shape = group)) +
    facet_wrap(~surprise, ncol = 1, switch = "y") +
    stat_summary(fun = mean,
                 size = 2,
                 geom = "point") +
    stat_summary(fun.data = mean_se,
                 geom = "errorbar",
                 width = 0.1) +
    labs(x = "Prior knowledge reported",
         y = "Group") +
    scale_x_continuous(labels = scales::percent) +
    
    dataset_3 |> 
    mutate(surprise = translation %in% tbl_data$translation) |> 
    mutate(surprise = if_else(
        surprise,
        "Unexpectedly high accuracy\n(no phonological similarity)",
        "Expected accuracy")) |> 
    summarise(correct = sum(correct),
              n = n(),
              .by = c(translation, group, surprise, knowledge)) |> 
    mutate(accuracy = prop_adj(correct, n)) |> 
    select(translation, group, accuracy, surprise, knowledge) |> 
    ggplot(aes(accuracy, group, shape = group)) +
    facet_wrap(~surprise, ncol = 1, switch = "y") +
    stat_summary(fun = mean,
                 size = 2,
                 position = position_dodge(width = 0.75),
                 geom = "point") +
    stat_summary(fun.data = mean_se,
                 geom = "errorbar",
                 width = 0.1,
                 position = position_dodge(width = 0.75)) +
    labs(x = "Accuracy",
         y = "Group") +
    scale_x_continuous(labels = scales::percent,
                       limits = c(0, 1)) +
    theme(axis.title.y = element_blank(),
          axis.text.y = element_blank(),
          strip.text = element_blank()) +
    
    plot_layout(nrow = 1, guides = "collect") &
    plot_annotation(tag_levels = "A") &
    labs(colour = "Group",
         shape = "Group") &
    theme(panel.border = element_rect(fill = NA,
                                      colour = "grey",
                                      linewidth = 1),
          axis.line = element_blank(),
          axis.text.y = element_blank(),
          axis.title.y = element_blank(),
          axis.ticks.y = element_blank(),
          legend.position = "top",
          panel.grid.major.x = element_line(colour = "grey",
                                            linetype = "dotted"),
          panel.grid.major.y = element_blank()) 
```

```{r}
c3 <- gather_draws(exp_3_m1, `b_.*`, `sd_.*`, regex = TRUE) |> 
    summarise(across(.value, lst(.median = median, 
                                 .lower = \(x) hdi(x)[1],
                                 .upper = \(x) hdi(x)[2],
                                 .rope = \(x) mean(between(x, -0.1, 0.1))),
                     .names = "{fn}"))
c3 <- split(c3, c3$.variable)
names(c3) <- janitor::make_clean_names(names(c3)) 
```

Participants translating Catalan words and participants translating Spanish words, as indicated by the regression coefficient of *Group* ($\beta$ = `r round(c3$b_group1$.median, 3)`, 95% CrI = [`r round(c3$b_group1$.lower, 3)`, `r round(c3$b_neigh_n_h_lv$.upper, 3)`], *p*(ROPE) = `r round(c3$b_group1$.rope, 3)`). Overall, participants responded less accurately to words with more CLPNs than to words with fewer CLPNs, regardless of the amount of phonological similarity between the presented word and its translation. This is indicated by the size of the regression coefficient of the two-way interaction between *Similarity* and *CLPN* ($\beta$ = `r round(c3$b_neigh_n_h_lv$.median, 3)`, 95% CrI = [`r round(c3$b_neigh_n_h_lv$.lower, 3)`, `r round(c3$b_neigh_n_h_lv$.upper, 3)`], *p*(ROPE) = `r round(c3$b_neigh_n_h_lv$.rope, 3)`). As anticipated, participants' performance benefited from an increase in *Similarity* ($\beta$ = `r round(c3$b_lv$.median, 3)`, 95% CrI = [`r round(c3$b_lv$.lower, 3)`, `r round(c3$b_lv$.upper, 3)`], *p*(ROPE) = `r round(c3$b_lv$.rope, 3)`), while the number of *CLND* had the opposite effect ($\beta$ = `r round(c3$b_neigh_n_h$.median, 3)`, 95% CrI = [`r round(c3$b_neigh_n_h$.lower, 3)`, `r round(c3$b_neigh_n_h$.upper, 3)`], *p*(ROPE) = `r round(c3$b_neigh_n_h$.rope, 3)`). @fig-epreds-1 illustrates the posterior of the average predictions of the model for words with different values of *Similarity* and *CLPN*.


```{r fig-epreds-3}
#| label: fig-epreds-3
#| fig-cap: "Posterior model-predicted mean accuracy in Experiment 2. Predictions were generated from 4,000 posterior samples, extracted for different values of *CLPN* (0, 2, 4, 8, 12) and *Similarity* (1-100). Predictions are plotted separately for English participants translating Catalan words, and for English participants translating Spanish words. Lines indicate mean predictions, and intervals indicate 95%, 89%, 78%, 67%, and 50% credible intervals (CrI)."
#| fig-height: 6
#| fig-width: 10
get_epreds(exp_3_m1, dataset_3) |> 
    mutate(neigh_n_h = paste0(neigh_n_h, " neighbours"),
           neigh_n_h = factor(neigh_n_h,
                              levels = paste0(c(0, 2, 4, 8, 12),
                                              " neighbours"),
                              ordered = TRUE)) |> 
    ggplot(aes(lv, .epred)) +
    facet_grid(group~neigh_n_h) +
    stat_lineribbon(aes(fill_ramp = after_stat(level)),
                    linewidth = 1/2,
                    .width = c(0.99, 0.95, 0.89, 0.78, 0.67, 0.50)) +
    labs(x = "Similarity\n(Levenshtein distance with correct translation)",
         y = "p(Correct)",
         fill_ramp = "CrI (%)") +
    scale_x_continuous(labels = scales::percent) +
    scale_y_continuous(labels = scales::percent) +
    scale_fill_brewer(palette = "Reds") +
    theme(legend.position = "top",
          legend.box = "vertical") 
```

## Discussion

# Joint analyses

Across Experiments 1 and 3, we found strong evidence that participants efficiently exploited phonological similarity to provide accurate translations for words in an unfamiliar language, provided that few phonological neighbours of higher lexical frequency were present. @fig-coefs summarizes the posterior distribution of the regression coefficients of the models in Experiments 1 to 3.

```{r fig-coefs}
#| label: fig-coefs
#| fig-width: 9
#| fig-height: 5
var_labels <- c("b_Intercept" = "Intercept",
                "b_freq_zipf_2" = "Frequency",
                "b_lv" = "Similarity",
                "b_neigh_n_h" = "CLPN",
                "b_neigh_n_h:lv" = "Similarity × CLPN",
                "b_group1" = "cat-ENG vs. spa-ENG") 

list(exp_1_m0, exp_2_m0, exp_3_m1) |> 
    set_names(paste0("Experiment ", 1:3)) |> 
    map_dfr(gather_draws, `b_.*`, regex = TRUE, .id = "exp") |> 
    dplyr::filter(.variable != "b_Intercept") |> 
    mutate(.value = .value/4,
           .variable = factor(.variable, 
                              levels = names(var_labels),
                              labels = var_labels)) |> 
    ggplot(aes(.value, reorder(exp, desc(exp)))) +
    facet_wrap(~.variable, scales = "free_x") +
    annotate(geom = "rect",
             xmin = -0.1/4,
             xmax = 0.1/4,
             ymin = -Inf,
             ymax = Inf,
             fill = "grey",
             alpha = 3/4,
             colour = NA) +
    geom_vline(xintercept = 0, 
               colour = "grey",
               linewidth = 3/4) +
    stat_slab(aes(fill = after_stat(level)), 
              point_interval = mode_hdi, 
              .width = c(0.95, 0.89, 0.67, 1),
              outline_bars = TRUE,
              colour = "white",
              linewidth = 0.1,
              breaks = 20,
              normalize = "panels") +
    stat_spike(at = "median",
               size = 1,
               aes(linetype = "Median"),
               normalize = "panels") +
    labs(x = "Estimate", y = "Experiment",
         colour = "CrI",
         fill = "CrI",
         linetype = "") +
    scale_fill_brewer(palette = "Reds") +
    theme(legend.position = c(1, 0),
          legend.box = "horizontal",
          legend.justification = c(1, 0),
          axis.title.x = element_blank(),
          axis.title.y = element_blank())
```


# General discussion

This type of cross-linguistic similarity has also been extremely informative for the study of bilingual lexical processing: words that share high form-similarity with their translations are processed faster and more accurately than words sharing lower phonological similarity. This phenomenon has provided strong evidence in favour of the language-non selective hypothesis of bilingual lexical access, which states that bilinguals activate lexical representations in both languages, even during monolingual situations. This facilitation effect has been reported for comprehension [@dijkstra2010cross; @midgley2011effects; @thierry2007brain], production [@costa2000cognate], learning [@de2000hard; @elias2022crosslanguage; @lotto1998effects; @valente2018does], and translation [@christoffels2006memory].

# References


