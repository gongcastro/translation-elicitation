---
title: "The role of cognateness in non-native spoken word recognition"

shorttitle: Cognateness and non-native word recognition

author:
- name: Gonzalo Garcia-Castro
  affiliation: '1'
  corresponding: yes
  address: Ramon Trial Fargas, 25-27, 08005 Barcelona
  email: gonzalo.garciadecastro@upf.edu
- name: Serene Siow
  affiliation: '2'
- name: Nuria Sebastian-Galles
  affiliation: '1'
- name: Kim Plunkett
  affiliation: '2'
affiliation:
  - id            : "1"
    institution   : "Center for Brain and Cognition, Universitat Pompeu Fabra"
  - id            : "2"
    institution   : "Department of Experimental Psychology, University of Oxford"
    
authornote: |

abstract: |

  <!-- https://tinyurl.com/ybremelq -->
  
keywords: cognate, word recognition, translation, non-native, spoken word recognition, bayesian
wordcount: 4553

bibliography:
  - r-references.bib
  - references.bib
floatsintext: yes
figurelist: no
tablelist: no
footnotelist: no
linenumbers: yes
mask: no
draft: no
numbersections: yes
csl: "apa7.csl"
documentclass: apa7
classoption: man
output: papaja::apa6_docx
---


```{r setup, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}

set.seed(42)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  results = "asis", 
  dpi = 1000
)

options(
  knitr.kable.NA = '-', 
  knitr.duplicate.label = "allow",
  gt.html_tag_check = FALSE
)

```


```{r prepare, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}

# import objects
tar_load_globals()
tar_load_all()

# set custom ggplot2 theme
theme_set(
  theme(
    title = element_text(size = 8, face = "bold"),
    
    legend.background = element_rect(fill = NA, colour = NA),
    legend.key = element_rect(fill = "white", colour = NA),
    legend.justification = "right",
    legend.direction = "horizontal",
    
    axis.line = element_line(colour = "black", size = 0.5),
    axis.ticks = element_line(colour = "black", size = 0.5),
    axis.text.x = element_text(colour = "black"),
    axis.text.y = element_text(colour = "black"),
    
    plot.caption.position = "plot",
    plot.title = element_text(hjust = 0.5),
    
    
    panel.grid = element_blank(),
    panel.background = element_rect(fill = "white", colour = NA)
  )
)

```


# Introduction


Listening to non-native speech is more costly than listening to native speech, even for highly proficient bilinguals, and especially in acoustically adverse situations [@lecumberri2010non; @takata1990english]. One of the sources of this increased difficulty stems from the possible mismatch between the phonology of the native and the non-native languages: some acoustic features embedded in the non-native speech signal do not overlap perfectly with any phonemic category in the listener's native language. For example, a Spanish native listening to French may encounter the word /pɔʁt/ (*porte*), translation of *door* in French. The voiced uvular fricative consonant /ʁ/ and the open-mid back rounded vowel /ɔ/ do not exist in Spanish. These discrepancies can nonetheless be perceived as allophonic variations of native phonemes [@best2001discrimination], and the non-native speech signal engages lexical processing mechanisms [@weber2004lexical]. Although this mismatch has a noticeable toll on comprehension [@cutler2004patterns], the fact that word recognition can take place in such circumstances illustrates that non-native listeners--even low-proficiency ones--are rarely completely naïve to the language they are listening to. In the present study we investigate the extent to which adult listeners rely on the phonological similarity between the native and the non-native languages during word recognition, capitalising on the role of *cognates*.


All languages share, to some extent, similarities at the lexical level, frequently due to their typological closeness and/or socio-historical events involving the populations of speakers of each language. Cognates embody a particular case of such commonalities. Cognates are defined as cross-language synonyms whose form is similar (at the phonological level, orthographic level, signed level, etc.), frequently due to their shared etymological origin^[Some form-similar cross-language synonyms are technically not cognates. For example, *sun* and *sol* (in Spanish), share their phonological onset, but their etymology points to different origins. We will use the word *cognateness* to include all form-similar cross-language synonyms for simplicity. It is highly implausible that etymology plays a direct role on language perception if it is not via form-similarity, since it is not necessary for participants in psycholinguistic experiments to be aware of the etymology of the words they encounter in the tasks to be subject to the effect of form-similarity.]. For example, Romance languages such as Spanish and Catalan share many cognates [@schepens2012distributions], as in the case of *puerta* and *porta* (*door* in Spanish and Catalan, respectively). Cognates play a pivotal role in virtually all models of bilingual lexical processing because they provide evidence that bilinguals access the lexicon in a language non-selective way. For instance, pictures are named faster by Spanish-Catalan bilinguals when their associated labels in both languages are cognates (e.g., *puerta*-*porta*), compared to when their labels are non-cognates (*mesa*-*taula*, *table* in Spanish and Catalan), which suggests that word representations of both languages are activated during word retrieval [@costa2000cognate]. This cognate advantage is not restricted to production, but rather extends to word recognition [@thierry2007brain; @midgley2011effects], word learning [@de2000hard; @lotto1998effects; @valente2018does], and word translation [@christoffels2006memory].



Early theoretical accounts of this phenomenon pointed to the fact that the impact of cognateness on lexical processing depends on bilinguals' proficiency in their second language (L2) [@potter1984lexical; see @chen1989patterns for similar results]. The reason behind this claim was the assumption that second-language learners start acquiring words in L2 by associating them to their translation in L1 (lexical route), and not to their shared concept (conceptual route). This word-word connection would be sensitive to form-similarity (i.e., cognateness): the more similar the two word forms are, the stronger the connection). As learners become more proficient, the connection between L2 representations and their meaning grows stronger, and L2 word processing becomes less reliant on word-word connections between L1 and L2 representations, therefore to cognateness. The Revised Hierarchical Model [RHM, @kroll1994category] captured this hypothesis and predicted that translating words from L1 to L2 (*forward* translation) should take longer than translating from L2 to L1 (*backward* translation). The rationale behind this prediction was that backward translation relies more strongly on direct word-word links between L1 and L2 representations, whereas forward translation relies more strongly on the mediation between the concept and the two word forms. One of the consequences of this prediction is that backward translation should be more sensitive to the form similarity between the L1 and the L2 representations: cognate words should be retrieved faster than non-cognate words during backward translation.


To test these predictions, @degroot1994forward and @de1992determinants asked Dutch natives with high (but non-native) English proficiency to translate words from either Dutch to English (forward translation, L1 to L2) or from English to Dutch (backward translation, L2 to L1). Participants were presented with written words in English or Dutch, and were asked to speak out loud their translation in the other language as fast as possible. Results showed that translation times and accuracy were roughly equivalent in forward and backward translation. Cognates were translated equally fast in both conditions, but non-cognates were translated faster during backward than forward translation, and translation of cognates was less affected  by semantic variables than non-cognates. Moreover, forward translation was more sensitive than backward translation to the effect of semantic variables (e.g., concreteness). Overall, these findings supported a soft version of the RHM model, in which both the conceptual and the lexical translation routes are active during translation, but where backward translation relies more strongly on direct links between L1 and L2 representations, making it more sensitive to cognateness (than forward translation). Although higher-proficiency bilinguals tested in the same task showed similar results, subsequent studies did not find differences in participant's performance during forward and backward translation, or even found better performances in forward translation [@christoffels2006memory; @christoffels2013language]. These later findings put into question the RHM's predictions regarding the relationship between proficiency and cognateness, and backed a more complex interaction between the L1 and L2 lexica during word recognition.


Since the RHM model was proposed, later studies on monolingual populations brought attention to the role of the network of connections that words establish with each other at the form-level (phonological or orthographic) or at the conceptual level. For instance, the Neighbourhood Activation Model [NAM, @luce1998recognizing] highlighted the fact that lexical selection is affected by the number of phonological neighbours of the selected word. Words surrounded by a larger number of phonological neighbours (only differing in one phoneme from the target words) are responded more slowly and less accurately than words from sparser phonological neighbourhoods [@goldinger1989priming; @luce1990similarity], especially when such neighbours' lexical frequency is higher [@luce1998recognizing]. This finding posed an important question in the field of bilingualism research: do word representations in one language form part of the phonological or orthographic neighbours of the other language? The Bilingual Interactive Activation [BIA+, @van1998orthographic] addressed this issue and suggested that lexical representations from both languages establish both excitatory and inhibitory connections with other, resulting in an integrated lexicon for both languages, in line with connectionist approaches to lexical processing [@mcclelland1981interactive]. Evidence supporting such integrated lexicon is provided by the fact that bilinguals' performance in word recognition tasks is sensitive to the orthographic neighbourhood density of the translation of the presented word. When bilingual participants are presented with words in only one language (target language) in a lexical decision task, translating them to the non-target language took longer when they were part of large  neighbourhoods [@van1998orthographic]. This suggests that words presented in the target language activated orthographic neighbourhoods in the non-target language, which competed for selection with the target word during recognition.



A more recent model of bilingual lexical processing is Multilink [@dijkstra2019multilink], which integrates and formalises previous claims and predictions on how an interactive account of bilingual lexical access impacts word recognition, production, and translation. This model assumes an integrated lexicon in which word representations from the two languages establish connections in a way that is similar to what words of a single language do. Such model also assumes that during backward translation, the presented word in L2 activates form-similar words in L1, which in turn compete with the target word in L1. It also  proposes that translation equivalents are exclusively connected through their shared concept, contrary to the RHM model's specifications (which suggest that translation equivalents are connected by excitatory connections at the lexical level; e.g., translation can be done via word-word connections). In addition, this model suggests that the strength of the association between L2 representations and their concept is a function of language users' proficiency in L2. A remarkable correlation between Multilink simulations of a translation elicitation task and behavioural data on Dutch-English bilinguals [@christoffels2006memory] has been reported, showing that forward translation is faster than backward translation, contrary to the predictions made by the RHM model. Importantly, the model also generates data supporting the claim that cognates are translated faster than non-cognates, in line with previous literature on the cognate advantage during translation. Overall, results from Multilink are compatible with an integrated bilingual lexicon and with the existence of a facilitatory effect of cross-language similarity during translation.  


The fact that Multilink successfully fitted the experimental data from @christoffels2006memory, despite the absence of any word-word connections between translation equivalents, is remarkable, and questions the necessity of such connections during translation, in favour of simpler models in which backward translation relies exclusively on the association between L2 and L1 word forms via their shared concept. It should be taken into account, however, that  @christoffels2006memory only tested fairly balanced bilinguals (i.e., bilinguals with high proficiency in L2), and that the model specification under which the quantitative predictions were generated, assumes similar vocabulary sizes across languages and same strength of word-concept connections of L1 and L2 representations. The aforementioned conclusions should therefore be taken with caution when extended to unbalanced bilinguals. It is low-proficiency bilinguals for whom the RHM model predicts strong reliance on word-word connections during translation, especially during backward translation, and this prediction remains untested my Multilink.


In the present study we investigated the plausibility of the lexical route as a mechanism exploited by monolingual adults during backward translation. Monolinguals can be considered a particular (extreme) case of unbalanced bilingualism, in which their vocabulary size in L2 is null, and word-concept connections are absent. In this case, monolinguals can only use the lexical route to succeed in the translation task, and they can only do so by exploiting the cognateness of the words presented and their translation in the native language. We hypothesised that if participants are able to translate words in L2, without knowledge of the language it belongs to, and based on their form-similarity with the target word, then the lexical route should be in place for low-proficiency bilinguals. In contrast, if participants are not able to translate words from L2 to L1 regardless of their form-similarity, then should low-proficiency bilinguals rely entirely on word-concept connections for backward translation.

Following @luce1998recognizing we also hypothesised that if participants are able to correctly translate words from an unfamiliar language based on their phonological similarity to the target word, similar sounding but incorrect translations (e.g., false friends) should be activated too. If this is the case, words whose translation in the native language are part of larger phonological neighbours should be less likely to be correctly translated, especially if such neighbours have higher lexical frequency than the target word.


# Methods

All materials, data, and code used in this study are hosted in the Open Science Framework [https://osf.io/9fjxm/](https://osf.io/9fjxm/) and a GitHub repository [https://github.com/bilingual-project/translation-elicitation.git](https://github.com/bilingual-project/translation-elicitation.git), along with additional notes.


## Participants

Data collection took place from `r format(min(participants$date, na.rm = TRUE), "%B %dth, %Y")` to `r format(max(participants$date, na.rm = TRUE), "%B %dth, %Y")`. We collected data from `r length(unique(participants$participant))` participants (*Mean* = `r printnum(mean(participants$age, na.rm = TRUE))` years, *SD* = `r printnum(sd(participants$age, na.rm = TRUE))`, Range = `r printnum(min(participants$age, na.rm = TRUE))`-`r printnum(max(participants$age, na.rm = TRUE))`). `r length(unique(participants$participant[participants$group %in% c("ENG-CAT", "ENG-SPA")]))` participants were British English native speakers living in United Kingdom (`r length(unique(participants$participant[participants$group %in% c("ENG-CAT", "ENG-SPA") & participants$sex=="Female"]))` female), and `r length(unique(participants$participant[participants$group %in% c("SPA-CAT")]))` participants were Spanish native speakers living in Spain (`r length(unique(participants$participant[participants$group %in% c("SPA-CAT") & participants$sex=="Female"]))` female). Participants in UK were recruited via Prolific (5£ compensation) and SONA (compensation in academic credits). Participants in Spain were contacted via announcements in Faculties, and were compensated 5€ or an Amazon voucher for the same value. Participants gave informed consent before providing any data and the study was conducted in accordance with ethical standards of the Declaration of Helsinki and the protocol was approved by the local ethical committee (XXXXXXXXXXXX). Participants were asked to complete the experiment using a laptop in a quiet place with good internet connection. We excluded data from participants that a) self-rated their oral and/or written skills in a second or third language as higher than 4 in a 5-point scale (*n* = `r participants %>% filter(l2written > 4 | l2oral > 4) %>% nrow()`), b) were diagnosed with a language (*n* = `r participants %>% filter(impairment) %>% nrow()`) , or c) did not contribute more than 80% of valid trials (*n* = `r participants %>% filter(invalid_participant_trials) %>% nrow()`).


## Procedure

The experiment was implemented online using Psychopy/Pavlovia [@peirce2019psychopy2]. Participants accessed the study from a link provided by Prolific or SONA and completed the experiment from an internet browser (Chrome or Mozilla). After giving their consent for participating, participants answered a series of questions about their demographic status, their language background, and the set up they were using for completing the study. Then, participants completed the experimental task. Participants were informed that they would listen to a series of pre-recorded words in Catalan or Spanish (English participants) or only Catalan (Spanish participants). They were instructed to listen to each word, guess its meaning in English (English participants) or Spanish (Spanish participants), and type their answer as soon as possible. English participants were randomly assigned to the list of Catalan or Spanish trials. Participants in the Catalan list were presented with `r nrow(stimuli[stimuli$group=="ENG-CAT",])` trials, and participants in the Spanish list were presented with `r nrow(stimuli[stimuli$group=="ENG-SPA",])` trials. Each trial started with a yellow fixation point presented during one second on the centre of the screen over a black background. After one second, the audio started playing while the dot remained being displayed until the audio ended. Upon the offset of the fixation point and audio, participants were prompted to write their answer by a ">" symbol. Typed letters were displayed in the screen in real time to provide visual feed-back to participants. Participants were allowed to correct their answer. Then, participants pressed the RETURN key to start and new trial. We excluded trials where participants did not type an existing word in the correspondent language, or did not type anything at all. Trials where the response was mistyped by only one character were counted as correct, as long as the respond did not correspond to a distinct word. Trials in which participants took longer than 10 seconds to respond were also filtered out. Participants contributed a total of `r nrow(responses)` valid trials (`r nrow(responses[responses$group %in% c("ENG-CAT", "SPA-CAT"),])` in Catalan, `r nrow(responses[responses$group %in% c("ENG-SPA"),])` in Spanish). The task took approximately 15 minutes to be completed.



```{r procedurefigure, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, out.width="80%", fig.cap="Schematic representation of a trial in the experimental task."}

include_graphics(here("img", "design.png"))

```


## Stimuli

```{r stimulilengths, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
lengths <- stimuli %>% 
  select(group, ipa1, ipa2, word1, word2) %>% 
  mutate_at(vars(ipa1:word2), nchar) %>% 
  group_by(group) %>% 
  summarise_at(vars(ipa1, ipa2, word1, word2), list(mean = mean, sd = sd, min = min, max = max)) %>%
  split(.$group) %>% 
  map(select, -group) %>% 
  set_names(make_clean_names(names(.))) %>% 
  map(unlist)
```

We arranged two lists of words: one in Catalan (to be presented to English and Spanish natives) and one in Spanish (to be presented to English natives only). Words in the Catalan list (listened to by participants) were `r printnum(lengths$eng_cat["ipa1_mean"])` phonemes long on average (*SD* = `r printnum(lengths$eng_cat["ipa1_sd"])`, Range = `r printnum(lengths$eng_cat["ipa1_min"], digits = 0)`-`r printnum(lengths$eng_cat["ipa1_max"], digits = 0)`). Their translations to English (typed by participants in the keyboard) were `r printnum(lengths$eng_cat["word2_mean"])` characters long on average (*SD* = `r printnum(lengths$eng_cat["word2_sd"])`, Range = `r printnum(lengths$eng_cat["word2_min"], digits = 0)`-`r printnum(lengths$eng_cat["word2_max"], digits = 0)`), and their translations to Spanish were `r printnum(lengths$spa_cat["word2_mean"])` characters long on average (*SD* = `r printnum(lengths$spa_cat["word2_sd"])`, Range = `r printnum(lengths$eng_cat["word2_min"], digits = 0)`-`r printnum(lengths$eng_cat["word2_max"], digits = 0)`). Words in the Spanish list were `r printnum(lengths$eng_spa["ipa1_mean"])` phonemes long on average (*SD* = `r printnum(lengths$eng_spa["ipa1_sd"])`, Range = `r printnum(lengths$eng_spa["ipa1_min"], digits = 0)`-`r printnum(lengths$eng_spa["ipa1_max"], digits = 0)`). Their translations to English were `r printnum(lengths$eng_spa["word2_mean"])` characters long on average (*SD* = `r printnum(lengths$eng_spa["word2_sd"])`, Range = `r printnum(lengths$eng_spa["word2_min"], digits = 0)`-`r printnum(lengths$eng_spa["word2_max"], digits = 0)`). 


We extracted the lexical frequencies from SUBTLEX-UK for English words [@van2014subtlex], SUBTLEX-ESP for Spanish words [@cuetos2011subtlex], and SUBTLEX-CAT for Catalan words [@boada2020subtlex]. We extracted or transformed scores as/into Zipf scores [@van2014subtlex] to so correct their logarithmic distribution, limiting their range roughly between 0 to 7, allowing an easier interpretation of further analyses [@van2014subtlex]. 

We retrieved PTHN scores (Phonological Total Higher-frequency Neighbourhood size) from the CLEARPOND database [@marian2012clearpond]. PTHN scores indicate the number of phonological neighbours of the target word with higher lexical frequency, as indicated by its score in the corresponding SUBTLEX database. CLEARPOND defines a phonological neighbour as a word whose phonological transcription in International Phonetic Alphabet (IPA) format (generated from eSPEAK, [http://espeak.sourceforge.net/](http://espeak.sourceforge.net/)) differs from that of the target word in only one addition, deletion, or substitution. PTHN scores in CLEARPOND measure have been calculated using corpora of similar size across language, allowing reliable cross-language comparisons. 


```{r stimuliplot, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, out.width="100%", fig.width=7, fig.cap="Distribution and summary statistics of stimuli properties. A\\) Lexical frequency of target words, expressed in counts per million and as Zipf scores. B\\) Number of higher-frequency phonological neighbourhoods of target words (PTHN). C\\) Normalised Levenshtein phonological similarity between the presented and the target words, expressed in percentages."}

medians <- stimuli %>% 
  group_by(group) %>% 
  summarise(
    pthn_median = median(pthn),
    pthn_sd = sd(pthn),
    frequency_zipf_median = median(frequency_zipf),
    frequency_zipf_sd = sd(frequency_zipf),
    lv_median = median(lv),
    lv_sd = sd(lv)
  ) 

stimuli %>% 
  count(group, pthn) %>% 
  ggplot(aes(pthn, n, fill = group)) + 
  facet_wrap(~group, ncol = 1) + 
  geom_rect(
    data = medians, 
    aes(
      xmin = 0, xmax = pthn_median+pthn_sd, 
      ymin = -Inf, ymax = Inf, fill = group
    ), inherit.aes = FALSE, alpha = 0.15
  ) +   
  geom_col(colour = "white") + 
  geom_vline(
    data = medians, aes(xintercept = pthn_median, group = group), 
    colour = "black", size = 0.5, linetype = "dashed"
  ) + 
  geom_text(data = medians, aes(label = "Median ± SD", x = pthn_median+10, y = 40), size = 3) +
  geom_curve(
    data = medians, aes(x = pthn_median+10, xend = pthn_median+0.2, y = 35, yend = 35),
    arrow = arrow(length = unit(0.2, "cm")), curvature = -0.25, colour = "grey30") +
  labs(x = "PTHN", y = "Number of trials", title = "Higher-frequency neighbours") +
  
  ggplot(stimuli, aes(frequency_zipf, fill = group)) + 
  facet_wrap(~group, ncol = 1) + 
  geom_rect(
    data = medians, 
    aes(
      xmin = frequency_zipf_median-frequency_zipf_sd, xmax = frequency_zipf_median+frequency_zipf_sd, 
      ymin = -Inf, ymax = Inf, fill = group
    ), inherit.aes = FALSE, alpha = 0.15
  ) +   
  geom_histogram(bins = 20, colour = "white") +
  geom_vline(
    data = medians, aes(xintercept = frequency_zipf_median, group = group), 
    colour = "black", size = 0.5, linetype = "dashed"
  ) + 
  labs(x = "Lexical frequency (Zipf)", y = "Number of trials", title = "Frequency") +
  
  ggplot(stimuli, aes(lv, fill = group)) +
  facet_wrap(~group, ncol = 1) + 
  geom_rect(
    data = medians, 
    aes(
      xmin = 0, xmax = lv_median+lv_sd, 
      ymin = -Inf, ymax = Inf, fill = group
    ), inherit.aes = FALSE, alpha = 0.15
  ) +   
  geom_histogram(bins = 20, colour = "white") +
  geom_vline(
    data = medians, aes(xintercept = lv_median, group = group), 
    colour = "black", size = 0.5, linetype = "dashed"
  ) + 
  scale_x_continuous(labels = percent) +
  labs(x = "Levenshtein", y = "Number of trials", title = "Phonological similarity") +
  
  plot_layout(guides = "collect") &
  plot_annotation(tag_levels = "A") &
  scale_fill_manual(values = c("#1A85FF", "#ff2976", "#FFC20A"), na.translate = FALSE) &
  theme(legend.position = "none")
```

We measured the phonological similarity between translation pairs by computing the Levenshtein similarity between their IPA translations using the `stringsim` function of the stringdist R package [@van2014stringdist]. This function computes the inverse of the Levenshtein distance between two character strings as a proportion. First, it computes the edit distance between two character strings (in this case, phoneme symbols) by counting the number of additions, deletions, and substitutions necessary to make both strings identical [@levenshtein1966binary]. This measure is then divided by the maximum distance (according to the length of the longest string) and then subtracted from 1. The result is percentage score indicating no similarity between the two strings if 0%, and a perfect match between two strings if 100%. We computed this similarity measure (Levenshtein, from now on) for every translation pair in our stimuli lists. Figure 1 summarises the lexical frequency, phonological neighbourhood density and phonological overlap of the words included in the Catalan and the Spanish lists.



Participants listened to one audio file in each trial. This audio file corresponded to a word in Catalan (for Spanish speakers, and for English participants allocated in the Catalan condition) or Spanish (for English speakers allocated in the Spanish condition). The audio files were the same ones used in child experiments conducted in the Laboratori de Recerca en Infància of Universitat Pompeu Fabra (Barcelona, Spain). These audio files were recorded by a proficient Catalan-Spanish female bilingual from the Metropolitan Area of Barcelona in a child-directed manner. Catalan and Spanish words were recorded at 44,100 Hz in separate files in the same session, and then de-noised using Audacity and normalised at peak intensity using Praat [@broersma2021praat]. The average duration of the audios was `r printnum(mean(stimuli$duration, na.rm = TRUE))` (*SD* = `r printnum(sd(stimuli$duration, na.rm = TRUE))`, Range = `r printnum(min(stimuli$duration, na.rm = TRUE))`-`r printnum(max(stimuli$duration, na.rm = TRUE))`). The average duration of the Catalan audio files was `r printnum(mean(stimuli$duration[stimuli$group %in% c("ENG-CAT")], na.rm = TRUE))` seconds (*SD* = `r printnum(sd(stimuli$duration[stimuli$group %in% c("ENG-CAT")], na.rm = TRUE))`, Range = `r printnum(min(stimuli$duration[stimuli$group %in% c("ENG-CAT")], na.rm = TRUE))`-`r printnum(max(stimuli$duration[stimuli$group %in% c("ENG-CAT")], na.rm = TRUE))`), and the average duration of the Spanish audios was `r printnum(mean(stimuli$duration[stimuli$group %in% c("ENG-SPA")], na.rm = TRUE))` seconds (*SD* = `r printnum(sd(stimuli$duration[stimuli$group %in% c("ENG-SPA")], na.rm = TRUE))`, Range = `r printnum(min(stimuli$duration[stimuli$group %in% c("ENG-SPA")], na.rm = TRUE))`-`r printnum(max(stimuli$duration[stimuli$group %in% c("ENG-SPA")], na.rm = TRUE))`). 



## Data analysis

We modelled the probability of participants guessing the correct translation of each input word using a generalised multilevel Bayesian regression model with a Bernoulli logit link distribution. We first fitted a base model that only included the intercept as a fixed effect, and random intercepts per participant  (Model 0). Second, we added the fixed effect of lexical frequency of the target word ($Frequency$). We then extended the model sequentially by adding the main effect of higher-frequency phonological neighbourhood ($PTHN$), the main effect of Levenshtein similarity ($Levenshtein$), the $PTHN \times Levenshtein$ interaction, the group of participants ($Group$, with levels ENG-CAT, ENG-SPA, and SPA-CAT), and finally, the $Levenshtein \times Group$ interaction). We specified two sum-coded *a priori* contrasts for the $Group$ predictor: one comparing English natives (ENG-CAT = -0.25, ENG-SPA = -0.25) against Spanish natives (SPA-CAT = +0.5), and one comparing English natives translating Spanish words (ENG-SPA = -0.5) against English natives translating Catalan words (ENG-CAT = +0.5) [@schad2020capitalize]. Each model included random intercepts by item (to account for the correlation of responses from different participants to the same word), and random intercepts and slopes by participant, where our experimental designed allowed it (to account for the correlation between responses from the same participant to different words) [@barr2013random]. We will later refer to these models as Model 0 to Model 6. For interpretability, all continuous predictor variables were standardised (transformed in standard deviations from the mean) before entering the model.


$$
\begin{align}

&\textbf{Likelihood}  \\
y_{i} \sim& Bernoulli(p_{i}) && \text{[probability of correct translation]} \\ \\

&\textbf{Parameters}  \\

logit(p_{i}) = ~ &  \beta_{0[p,w]} ~ +  && \text{[linear model]}\\
& \beta_{1[p]} ~ Frequency_{i} ~ + \\
& \beta_{2[p]} ~ PTHN_i ~ + \\
& \beta_{3[p]} ~ Similarity_i ~ + \\
& \beta_{4[p]} ~ (PTHN_i \times Similarity_i) \\ \\

\beta_{0-6[p,w]} \sim& ~  \mathcal{N}(\mu_{\beta_{j}}, \sigma_{\beta_{j}}) \text{, for participant } p ~\text{in 1, ..., } P ~\text{and  word } w ~\text{in 1, ..., } W && \text{[participant- and word-level intercepts]} \\
\beta_{1-6[p]} \sim& ~  \mathcal{N}(\mu_{\beta_{j}}, \sigma_{\beta_{j}}) \text{, for participant } p ~\text{in 1, ..., } P
&& \text{[participant-level coefficients]} \\ \\

&\textbf{Prior}  \\

\mu_{\beta_{p,w}} ~ \sim& ~ \mathcal{N}(0, 0.1) && \text{[participant-level coefficients]} \\
\sigma_{\beta_{p}}, ~ \sigma_{\beta_{w}} \sim& ~ HalfCauchy(0, 0.1) && \text{[SD for population and participant]} \\
\rho_{p}, ~ \rho_{w} \sim& ~LKJ(8) && \text{[correlation between participant-level coefficients]} \\


\end{align}
$$

To test and account for cross-group differences, we included a random intercept for each group. We compared models using leave-one-out cross-validation (*LOO*) [@vehtari2017practical]. More information about the models and model comparison can be found in Appendix 2. All analyses were performed in R environment [@rcore2019r]. We used the tidyverse family of R packages [@wickham2019tidyverse] to process data and to generate figures. We used the brms R package [@burkner2017brms] using the cmdstanr backend to the Stan probabilistic language [@carpenter2017stan] to estimate and compare the models (see Appendix 1 for mode details on the models).


# Results

All models showed good out-of-sample predictive validity, as suggested by the fact that the expected log-probability density was many times larger than its associated standard error. Model 6, which included all main effects and the two-way interactions between $PTHN$ and $Levenshtein$, and between $Group$ and $Levenshtein$ and $PTHN$ and consonant similarity, showed the best performance. 


```{r comparison, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

tab_loos <- model_loos %>% 
  as.data.frame() %>% 
  rownames_to_column("model") %>% 
  arrange(desc(elpd_loo)) %>% 
  select(model, matches("elpd_loo"), matches("looic"), matches("diff")) %>%
  mutate(model = str_replace(model, "fit_", "Model ")) %>% 
  mutate_all(~ifelse(. == 0, NA, .)) %>% 
  gt(rowname_col = "model") %>% 
  tab_header(
    title = md("Table 2"),
    subtitle = md("Model performance and comparison using leave-one-out cross-validation")
  ) %>% 
  tab_source_note(
    md("*Note*. Outcomes of the leave-one-out cross-validation procedure. Each row indicates the values of the expected log-predicted density (*ELPD*), leave-one-out information creterion (*LOO-IC*), and the difference in *ELPD* (*LOO-diff*) between each model and the inmediately simpler model. Each value is accompanied by a standard error (*SE*) indicating the uncertainty of its estimate.")
  ) %>% 
  fmt_missing(everything(), missing_text = "-") %>% 
  fmt_number(2:7) %>% 
  cols_label(
    model = md("**Model**"),
    looic = md("***LOO<sub>IC</sub>***"),
    se_looic = md("***SE*<sub>IC</sub>**"),
    elpd_loo = md("***LOO<sub>ELPD</sub>***"),
    se_elpd_loo = md("***SE***"),
    elpd_diff = md("***LOO<sub>diff</sub>***"),
    se_diff = md("***SE<sub>diff</sub>***")
  ) %>% 
  tab_style(
    style = cell_text(align = "left", weight = "bold", size = "medium"),
    locations = cells_title(groups = "title")
  ) %>% 
  tab_style(
    style = cell_text(align = "left", size = "medium", style = "italic"),
    locations = cells_title(groups = "subtitle")
  ) %>% 
  tab_style(
    style = cell_text(align = "left", size = "medium"),
    locations = cells_source_notes()
  ) %>% 
  tab_style(
    style = list(cell_borders(sides = c("left", "right"), color = "white"),
                 cell_text(align = "center")),
    locations = list(
      cells_body(), cells_row_groups(), cells_column_spanners(), 
      cells_stub(), cells_source_notes(),
      cells_column_labels(), cells_stubhead()
    )
  ) %>% 
  tab_style(
    style = list(cell_text(align = "left", size = "medium"), 
                 cell_borders(sides = "all", color = "white")),
    locations = list(cells_stub(), cells_source_notes())
  ) %>% 
  tab_style(
    style = cell_text(align = "center", size = "medium", weight = "normal", style = "normal"),
    locations = list(cells_column_spanners(), cells_column_labels())
  ) %>% 
  tab_style(
    style = cell_borders(sides = "all", color = "white"),
    locations = cells_body(columns = 1:7)
  ) %>% 
  tab_style(
    style = cell_borders(sides = "top", color = "white"),
    locations = cells_body(columns = 1:7)
  ) %>% 
  gtsave(here("img", "loos.png"))

include_graphics(here("img", "loos.png"))


```

We now report the mean of the posterior distribution of each coefficient in Model 6, along with its associated measures of uncertainty. For interpretability, we transformed the estimates of the intercept using the inverse logit function so that the values are expressed in probability of correct response instead of log-odds, and we transformed the coefficients of the rest of predictors divided by four. Dividing a coefficient expressed in log-odds by four returns an approximate of the derivative of the logistic function indicating the maximum steepness of the logistic curve. This way, the coefficients are expressed as increments or decrements in probability of correct translation [@gelman2020regression].

```{r posteriorfix, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Estimated posterior distributions of coefficients in Model 4. A\\) Population-level effects. Distributions indicate the estimated posterior likelihood density of regression coefficients of fixed effects. Credible intervals (*CrI*), represented with increasingly lighter segmentents in the distribution indicate the range of values that contain the true value with 95\\%, 80\\%, and 50\\% probability. Black dots represent the mean of each distribution. B\\) Participant\\-level coefficient variability. The model estimated participant\\-level coefficients to account for the dependency between responses from the same participant. Distributions in this panel indicate the estimated variability across coefficients from different participants, expressed as standard deviations (*SD*). C\\) Correlation between participant\\-level effects. The model allowed participant-level coefficients to co\\-vary. This panel represents the Pearson correlations between each pair of coefficients, expressed as the mean of the posterior distribution of each  estimated correlation.", fig.height=7, fig.width=6.5}



coefs <- fixef(model_fits$fit_6) %>% 
  as.data.frame() %>% 
  rownames_to_column("variable") %>% 
  clean_names() %>% 
  mutate_at(vars(estimate:q97_5), ~ifelse(variable=="Intercept", inv_logit_scaled(.), ./4)) %>% 
  group_split(variable) %>% 
  set_names(make_clean_names(map(., "variable"))) %>% 
  map(select, -variable) %>% 
  map(unlist)

str_repl <- c(
  "Intercept" = "Intercept",
  "frequency_zipf" = "Frequency\n[+1 SD]",
  "pthn:lv" = "PTHN \u00d7 Levenshtein", 
  "lv:group1" = "Levenshtein \u00d7 Group 1",
  "lv:group2" = "Levenshtein \u00d7 Group 2",
  "lv" = "Levenshtein\n[+1 SD]",
  "pthn" = "PTHN\n[+1 SD]",
  "group1" =  "Group 1\n[ENG-* vs. SPA-CAT]",
  "group2" = "Group 2\n[ENG-CAT vs. ENG-SPA]"
)


# fixed effects
post_fix <- posterior_draws_fixed %>% 
  filter(str_detect(.variable, "b_")) %>% 
  mutate(
    .variable_name = str_remove(.variable, "b_") %>% 
      str_replace_all(str_repl) %>%  
      factor(
        levels = c(
          "Intercept", "Frequency\n[+1 SD]", "PTHN\n[+1 SD]", 
          "Levenshtein\n[+1 SD]", "PTHN \u00d7 Levenshtein",
          "Group 1\n[ENG-* vs. SPA-CAT]",
          "Group 2\n[ENG-CAT vs. ENG-SPA]",
          "Levenshtein \u00d7 Group 1", "Levenshtein \u00d7 Group 2"
        )
      ),
    .value = ifelse(str_detect(.variable, "Intercept"), inv_logit_scaled(.value), .value/4)
  ) %>% 
  arrange(.variable) %>% 
  ggplot(aes(.value, fct_rev(.variable_name))) +
  geom_vline(xintercept = 0) +
  stat_slab(aes(fill = stat(cut_cdf_qi(cdf, .width = c(.5, .8, .95), labels = percent_format())))) +
  stat_pointinterval(.width = 0, point_size = 1) +
  scale_fill_manual(values = c("#1A85FF", "#9ccaff", "#d2e5fc"), na.translate = FALSE) +
  scale_x_continuous(labels = function(x) percent(round(x, 1))) +
  labs(x = "P(Correct)", y = "Posterior probability density", fill = "CrI") +
  theme(
    legend.position = c(1, 0.25),
    legend.direction = "vertical",
    legend.background = element_rect(fill = "grey95"),
    legend.key = element_rect(fill = "grey95", colour = "grey95"), 
    legend.key.height = unit(0.1, "cm"),
    axis.title.y = element_blank(),
    legend.text = element_text(size = 7),
    axis.text.x = element_text(colour = "black"),
    axis.text.y = element_text(colour = "black", hjust = 1),
    axis.ticks.y = element_blank(),
    panel.grid.major.y = element_line(colour = "grey", size = 0.5)
  ) 



# SD of random effects
post_sd_data <- posterior_draws_fixed %>% 
  filter(str_detect(.variable, "sd_")) %>% 
  mutate(
    .variable_name = str_remove(.variable, "sd_participant__|sd_word__") %>% 
      str_replace_all(str_repl) %>% 
      factor(
        levels = c(
          "Intercept", "Frequency\n[+1 SD]", "PTHN\n[+1 SD]", 
          "Levenshtein\n[+1 SD]", "PTHN \u00d7 Levenshtein",
          "Group 1\n[ENG-* vs. SPA-CAT]",
          "Group 2\n[ENG-CAT vs. ENG-SPA]",
          "Levenshtein \u00d7 Group 1", "Levenshtein \u00d7 Group 2"
        )
      ),
    group = case_when(
      str_detect(.variable, "participant") ~ "Participants",
      str_detect(.variable, "word") ~ "Words"
    ),
    .value = ifelse(str_detect(.variable, "Intercept"), inv_logit_scaled(.value), .value/4)
  ) 

post_sd <- ggplot(filter(post_sd_data, group=="Participants"), aes(.value, fct_rev(.variable_name))) +
  stat_slab(aes(fill = stat(cut_cdf_qi(cdf, .width = c(.5, .8, .95), labels = percent_format())))) +
  stat_pointinterval(.width = 0, point_size = 1) +
  scale_fill_manual(values = c("#ff2976", "#f2a7c2", "#fcd9e6"), na.translate = FALSE) +
  scale_x_continuous(labels = percent, limits = c(0, 1)) +
  labs(x = "SDs in P(Correct)", title = "By participant", fill = "CrI") +
  theme(axis.text.x = element_blank()) +
  
  ggplot(filter(post_sd_data, group=="Words"), aes(.value, fct_rev(.variable_name))) +
  stat_slab(aes(fill = stat(cut_cdf_qi(cdf, .width = c(.5, .8, .95), labels = percent_format())))) +
  stat_pointinterval(.width = 0, point_size = 1) +
  scale_x_continuous(labels = percent, limits = c(0, 1)) +
  labs(x = "SDs in P(Correct)", title = "By item", fill = "CrI") +
  
  plot_layout(ncol = 1, heights = c(0.85, 0.15), guides = "collect") &
  theme(
    legend.position = "none",
    axis.text.y = element_text(hjust = 1),
    axis.title.x = element_blank(),
    legend.direction = "vertical",
    axis.title.y = element_blank(),
    legend.background = element_rect(fill = "grey95"),
    legend.key.height = unit(0.1, "cm"),
    legend.key = element_rect(fill = "grey95", colour = "grey95"),
    legend.text = element_text(size = 7),
    axis.ticks.y = element_blank(),
    panel.grid.major.y = element_line(colour = "grey", size = 0.5)
  ) 


# Pearson correlations between random effects
corr_mat <- VarCorr(model_fits$fit_6, summary = TRUE)$participant$cor 
corr_names <- rownames(VarCorr(model_fits$fit_6, summary = TRUE)$participant$cor)

corr_mat <- corr_mat %>% 
  as_tibble() %>% 
  select(matches("Estimate")) %>% 
  rename_all(str_remove, "Estimate.") %>% 
  mutate(term1 = corr_names)

corr_mat[lower.tri(corr_mat)] <- NA

corr_data <- corr_mat %>% 
  as.data.frame() %>% 
  pivot_longer(-term1, names_to = "term2", values_to = ".value") %>% 
  drop_na(.value)

post_cors <- corr_data %>% 
  mutate(term1 = factor(term1, levels = names(str_repl), ordered = TRUE),
         term2 = factor(term2, levels = names(str_repl), ordered = TRUE)) %>% 
  mutate_at(vars(term1, term2), str_replace_all, str_repl) %>% 
  mutate(
    .value_label = case_when(term1==term2 ~ NA_character_, TRUE ~ printnum(.value, gt1 = FALSE)),
    .value = case_when(term1==term2 ~ NA_real_, TRUE ~ .value)
  ) %>% 
  filter(term1 != "PTHN × Levenshtein", term2 != "Intercept") %>% 
  ggplot(aes(fct_inorder(term1), fct_rev(fct_inorder(term2)), fill = .value)) +
  geom_tile(na.rm = TRUE) +
  geom_text(aes(label = .value_label), size = 3, na.rm = TRUE) +
  labs(x = "Term 1", y = "Term 2", fill = "Posterior\ncorrelation") +
  scale_fill_gradient2(
    low = "#04bf78", mid = "white", high = "#FFC20A", 
    na.value = "white", limits = c(-0.4, 0.4)
  ) +
  scale_x_discrete(position = "top") +
  coord_equal() +
  theme(
    legend.position = c(1, 1),
    legend.direction = "horizontal",
    legend.key.height = unit(0.1, "cm"),
    legend.key.width = unit(0.3, "cm"),
    legend.justification = c(1, 1),
    legend.title = element_blank(),
    legend.text = element_text(size = 5),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    axis.ticks = element_blank(),
    axis.text.x = element_text(size = 6, angle = 90, hjust = 0, vjust = 1),
    axis.text.y = element_text(size = 6, vjust = 1, hjust = 1)
  )

post_fix / ((post_sd | post_cors) + plot_layout(widths = c(0.6, 0.4))) +
  plot_layout() +
  plot_annotation(tag_levels = "A")



```

```{r bayesfactors}
h_frequency <- abs(hypothesis(model_fits$fit_6, hypothesis = "frequency_zipf = 0")$hypothesis$Evid.Ratio)
h_pthn <- abs(hypothesis(model_fits$fit_6, hypothesis = "pthn = 0")$hypothesis$Evid.Ratio)
h_lv <- abs(hypothesis(model_fits$fit_6, hypothesis = "lv = 0")$hypothesis$Evid.Ratio)
h_pthn_lv <- abs(hypothesis(model_fits$fit_6, hypothesis = "pthn:lv = 0")$hypothesis$Evid.Ratio)
h_group1 <- abs(hypothesis(model_fits$fit_6, hypothesis = "group1 = 0")$hypothesis$Evid.Ratio)
h_group2 <- abs(hypothesis(model_fits$fit_6, hypothesis = "group1 = 0")$hypothesis$Evid.Ratio)
h_lv_group1 <- abs(hypothesis(model_fits$fit_6, hypothesis = "lv:group1 = 0")$hypothesis$Evid.Ratio)
h_lv_group2 <- abs(hypothesis(model_fits$fit_6, hypothesis = "lv:group2 = 0")$hypothesis$Evid.Ratio)

icc_conditional <- icc(model_fits$fit_6, by_group = TRUE)$ICC[2]

sds <- gather_draws(model_fits$fit_6, `sd_.*`, regex = TRUE) %>% 
  mean_qi() %>% 
  mutate(
    .value = ifelse(grepl("Intercept", .variable), inv_logit_scaled(.value), .value/4),
    .lower = ifelse(grepl("Intercept", .variable), inv_logit_scaled(.lower), .lower/4),
    .upper = ifelse(grepl("Intercept", .variable), inv_logit_scaled(.upper), .upper/4)
  ) %>% 
  split(.$.variable)

names(sds) <- make_clean_names(names(sds))

word_example <- ranef(model_fits$fit_6)$word[,,"Intercept"] %>% 
  as.data.frame() %>% 
  rownames_to_column("word") %>% 
  arrange(-Estimate) %>% 
  clean_names() %>% 
  mutate_if(is.numeric, inv_logit_scaled) %>% 
  slice(1, n()) %>% 
  left_join(distinct(stimuli, word1, ipa1), by = c("word" = "word1")) %>% 
  split(.$word)

participant_ranef_range <- sds %>% 
  bind_rows() %>% 
  filter(!grepl("Intercept|word|group1", .variable)) %>% 
  summarise(.value = range(.value)) %>% 
  unlist()

cors_range <- gather_draws(model_fits$fit_6, `cor_.*`, regex = TRUE) %>% 
  median_qi() %>% 
  summarise(.value = range(.value))

corr_mat <- VarCorr(model_fits$fit_6, summary = TRUE)$participant$cor 
corr_names <- rownames(VarCorr(model_fits$fit_6, summary = TRUE)$participant$cor)

corr_data <- corr_mat %>% 
  as_tibble() %>% 
  clean_names() %>% 
  mutate(term1 = corr_names) %>% 
  relocate(term1) %>% 
  split(.$term1)

names(corr_data) <- make_clean_names(names(corr_data))

h_cor_intercept_frequency <- hypothesis(model_fits$fit_6, "participant__Intercept__frequency_zipf = 0", class = "cor")$hypothesis$Evid.Ratio
h_cor_frequency_zipf_lv_group1 <- hypothesis(model_fits$fit_6, "participant__frequency_zipf__lv:group1 = 0", class = "cor")$hypothesis$Evid.Ratio
h_cor_intercept_pthn <- hypothesis(model_fits$fit_6, "participant__Intercept__pthn = 0", class = "cor")$hypothesis$Evid.Ratio
```

Overall, participants were `r percent(coefs$intercept["estimate"], accuracy = 0.01)`, (95% *CrI* = [`r percent(coefs$intercept["q2_5"], accuracy = 0.01)`, `r percent(coefs$intercept["q97_5"], accuracy = 0.01)`]) likely to produce correct translations. The lexical frequency of the correct translation had little impact in accuracy: every standard deviation increment (*SD* = `r printnum(sd(stimuli$frequency_zipf, na.rm = TRUE))` Zipf points) decreased the probability of correct translation in `r percent(coefs$frequency_zipf["estimate"], accuracy = 0.01)` (95% *CrI* = [`r percent(coefs$frequency_zipf["estimate"], accuracy = 0.01)`, `r percent(coefs$frequency_zipf["q2_5"], accuracy = 0.01)`], $BF_{\beta\neq< 0}$ = `r printnum(h_frequency)`). The number of phonological neighbours of the correct translation with higher lexical frequency (*PTHN*) slightly decreased the probability of a correct responses in `r percent(coefs$pthn["estimate"], accuracy = 0.01)` (95% *CrI* = [`r percent(coefs$pthn["q2_5"], accuracy = 0.01)`, `r percent(coefs$pthn["q97_5"], accuracy = 0.01)`], $BF_{\beta \neq 0}$ = `r printnum(h_pthn)`) for every increase standard deviation increment  (1 *SD* = `r printnum(sd(stimuli$pthn, na.rm = TRUE))` neighbours). Finally, phonological similarity between translations had a strong effect on participants' accuracy: for every *SD* increment in Levenshtein similarity (1 *SD* = `r printnum(percent(sd(stimuli$lv, na.rm = TRUE)), digits = 2)` similarity), translation accuracy increased in `r percent(coefs$lv["estimate"], accuracy = 0.01)` (95% *CrI* = [`r percent(coefs$lv["q2_5"], accuracy = 0.01)`, `r percent(coefs$lv["q97_5"], accuracy = 0.01)`], $BF_{\beta \neq 0}$ = $>3.09 \times 10^{18}$). This difference increased in `r percent(coefs$pthn_lv["estimate"], accuracy = 0.01)` for every *SD* increment in *PTHN* (95% *CrI* = [`r percent(coefs$pthn_lv["q2_5"], accuracy = 0.01)`, `r percent(coefs$pthn_lv["q97_5"], accuracy = 0.01)`], $BF_{\beta \neq 0}$ = `r printnum(h_pthn_lv)`)

Spanish natives translating Catalan words were `r percent(coefs$group1["estimate"], accuracy = 0.01)` more accurate than English natives translating Spanish and Catalan words (95% *CrI* = [`r percent(coefs$group1["q2_5"], accuracy = 0.01)`, `r percent(coefs$group1["q97_5"], accuracy = 0.01)`], $BF_{\beta \neq 0}$ = `r printnum(h_group1)`). This difference increased in `r percent(coefs$lv_group1["estimate"], accuracy = 0.01)` for every one *SD* increment in Levenshtein similarity (95% *CrI* = [`r percent(coefs$lv_group1["q2_5"], accuracy = 0.01)`, `r percent(coefs$lv_group1["q97_5"], accuracy = 0.01)`], $BF_{\beta \neq 0}$ = `r printnum(h_lv_group1)`). English natives translating Catalan words were `r percent(abs(coefs$group2["estimate"]), accuracy = 0.01)` more accurate than those translating Spanish words (95% *CrI* = [`r percent(coefs$group2["q2_5"], accuracy = 0.01)`, `r percent(coefs$group2["q97_5"], accuracy = 0.01)`], $BF_{\beta \neq 0}$ = `r printnum(h_group2)`). For every one *SD* increment in Levenshtein similarity, the effect increased in `r percent(abs(coefs$lv_group2["estimate"]), accuracy = 0.01)` (95% *CrI* = [`r percent(coefs$lv_group2["q2_5"], accuracy = 0.01)`, `r percent(coefs$lv_group2["q97_5"], accuracy = 0.01)`], $BF_{\beta \neq 0}$ = `r printnum(h_lv_group1)`).


```{r marginaleffects, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.width=7, fig.height=3.5, fig.cap="Expected mean posterior predictions from Model 4. A) Population-level predictions: the X-axis and the Y-axis represent the Levenshtein distance (in standard deviations from the mean) and the probability of correct translation, respectively. We simulated 200 observations from our model: 100 simulations for words with low *PTHN* (-1 *SD*) and 100 simulations for high *PTHN* (+1 *SD*) words. We did this across the range of values of the Levenshtein scores. For each simulation, we drew a single sample from the posterior distribution of each coefficient. Each simulation is depicted in the graph as a line: pink in the case of low PTHN words, and blue in the case of high *PTHN* words. We also plotted the mean of the high *PTHN* (black solid line) and low *PTHN* (black dashed line) simulations to indicate the expected mean value of the posterior predictions of the model. The dispersion of the lines indicates the uncertainty of our predictions. We computed these posterior predictions for each group of participants, and plot tem in separate panels."}

m <- posterior_epreds_fixed %>% 
  mutate_at(vars(pthn, frequency_zipf), function(x) paste0(x, " SD")) %>% 
  mutate(
    pthn = case_when(
      pthn=="-1 SD" ~ "Low PTHN",
      pthn=="0 SD" ~ "Mean PTHN",
      pthn=="1 SD" ~ "High PTHN"
    ),
    frequency_zipf = case_when(
      frequency_zipf=="-1 SD" ~ "Low frequency",
      frequency_zipf=="0 SD" ~ "Mean frequency",
      frequency_zipf=="1 SD" ~ "High frequency"
    )
  ) 


ggplot(m, aes(lv, .epred, colour = pthn, fill = pthn)) +
  facet_wrap(~group) +
  geom_hline(yintercept = 0.5, colour = "grey") +
  geom_line(aes(group = interaction(pthn, .draw)), alpha = 0.25, size = 0.5, show.legend = TRUE) +
  stat_summary(aes(linetype = pthn), fun = mean, geom = "line", colour = "black", size = 0.75, show.legend = TRUE) +
  annotate("segment", x = -0.5, xend = -0.9, y = 0.85, yend = 0.85, arrow = arrow(length = unit(0.1, "cm")), size = .5) +
  annotate("segment", x = 2.25, xend = 2.65, y = 0.85, yend = 0.85, arrow = arrow(length = unit(0.1, "cm")), size = .5) +
  annotate("text", x = -0.5, y = 0.95, size = 2.75, label = "Less similar") +
  annotate("text", x = 2.15, y = 0.95, size = 2.75, label = "More similar") +
  labs(x = "Phonological similarity\n(Levenshtein, SD)", y = "P(Correct)", 
       colour = "PTHN", fill = "PTHN", linetype = "PTHN",
       title = "Population-level predictions") +
  scale_color_manual(values = c("#1E88E5", "#ff2976")) +
  scale_y_continuous(labels = function(x) percent(round(x, 2)), limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  theme(
    legend.position = "top",
    plot.title = element_blank()
  )


```


The conditional intra-class correlation (*ICC*) for items was `r percent(icc_conditional, accuracy = 0.01)`, indicating that responses from different participants to the same item were, on average, more similar to each other than responses to other items [@gelman2006data]. Item-level intercepts showed remarkable variability (*SD* = `r percent(sds$sd_word_intercept$.value, accuracy = 0.01)`, 95% CrI = [`r percent(sds$sd_word_intercept$.lower, accuracy = 0.01)`, `r percent(sds$sd_word_intercept$.upper, accuracy = 0.01)`]), indicating that participants' accuracy differed substantially across words: some words were correctly translated nearly always (e.g., Spanish *tomate*-*tomato*, /tomate/-/təmɑːtoʊ/, `r percent(word_example$tomate$estimate, accuracy = 0.01)`, 95% CrI = [`r percent(word_example$tomate$q2_5, accuracy = 0.01)`, `r percent(word_example$tomate$q97_5, accuracy = 0.01)`]), while others were almost never translated correctly (e.g., Catalan *colom*-*pigeon*, /kulom/-/pɪdʒɪn/, `r percent(word_example$colom$estimate, accuracy = 0.01)`, 95% CrI = [`r percent(word_example$colom$q2_5, accuracy = 0.01)`, `r percent(word_example$colom$q97_5, accuracy = 0.01)`]). Participant-level intercepts also showed high variability (*SD* = `r percent(sds$sd_participant_intercept$.value, accuracy = 0.01)`, 95% CrI = [`r percent(sds$sd_participant_intercept$.lower, accuracy = 0.01)`, `r percent(sds$sd_participant_intercept$.upper, accuracy = 0.01)`]), suggesting that average translation accuracy also varied significantly across participants. Finally, participant-level estimated slopes showed less variability: *SD* of $Frequency$, $PTHN$, $Levenshtein$, $PTHN \times Levenshtein$, and $Levenshtein \times Group 2$ ranged between `r percent(participant_ranef_range[1], accuracy = 0.01)`, and `r percent(participant_ranef_range[2], accuracy = 0.01)`. The participant-level estimated slopes of the $Leveshtein \times Group 1$ interaction are an exception, though (*SD* = `r percent(sds$sd_participant_lv_group1$.value, accuracy = 0.01)`, 95% CrI = [`r percent(sds$sd_participant_lv_group1$.lower, accuracy = 0.01)`, `r percent(sds$sd_participant_lv_group1$.upper, accuracy = 0.01)`]). This indicates that the impact of phonological similarity ($Levenshtein$) on the difference in accuracy between English and Spanish natives differed substantially across participants. 


Finally, participant-level effects showed weak Pearson correlations overall (<10%), with three exceptions. The estimated correlation between the intercepts and the slopes of $Frequency$ was `r percent(corr_data$intercept$estimate_frequency_zipf, accuracy = 0.01)` (95% CrI = [`r percent(corr_data$intercept$q2_5_frequency_zipf, accuracy = 0.01)`, `r percent(corr_data$intercept$q97_5_frequency_zipf, accuracy = 0.01)`], $BF_{Cor \neq 0}$ = `r `round(h_cor_intercept_frequency, 2)`). The estimated correlation between the slopes of $Frequency$ and $Levenshtein \times Group 1$ was `r percent(corr_data$frequency_zipf$estimate_lv_group1, accuracy = 0.01)` (95% CrI = [`r percent(corr_data$frequency_zipf$q2_5_lv_group1, accuracy = 0.01)`, `r percent(corr_data$frequency_zipf$q97_5_lv_group1, accuracy = 0.01)`], $BF_{Cor \neq 0}$ = `r round(h_cor_frequency_zipf_lv_group1, 2)`). The estimated correlation between the intercepts and the slopes of $PTHN$ was `r percent(corr_data$intercept$estimate_pthn, accuracy = 0.01)` (95% CrI = [`r percent(corr_data$intercept$q2_5_pthn, accuracy = 0.01)`, `r percent(corr_data$intercept$q97_5_pthn, accuracy = 0.01)`], $BF_{Cor \neq 0}$ = `r round(h_cor_intercept_pthn, 2)`). The large uncertainty associated with these estimates, however, prevents us from interpreting the size of these correlations as practically meaningful.



# Discussion


\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup

\newpage


